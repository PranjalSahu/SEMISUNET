{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "#from scipy.misc import imresize\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "#import hdf5storage\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "#from sklearn.decomposition import PCA\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "\n",
    "import functools\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     11,
     28,
     39,
     66,
     74
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Pytorch Models for training\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class SUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.gn5(x)\n",
    "       \n",
    "        x = self.up2(x, x3)\n",
    "        x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "model = SUNet(1, 1)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     202,
     225,
     253,
     265,
     296,
     362,
     435,
     472,
     490
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "unlabelled_ids     = unlabelled_ids\n",
    "train_ids          = train_ids[:4]\n",
    "val_ids            = val_ids\n",
    "test_ids           = test_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "prev_max        = -1000\n",
    "model_student   = UNetDoubleSmallGroupNorm(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 100\n",
    "\n",
    "\n",
    "teacher_dice_array = []\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "\n",
    "    train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx, valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    model_save_name = \"tmi-compare-sunet\"\n",
    "    \n",
    "    if np.mean(val_dice) > prev_max:\n",
    "        print(\"Step %d  Dice %.5f > %f  Train Dice %f \" % (epoch, np.mean(val_dice), prev_max, np.mean(student_dice1)))\n",
    "        prev_max     = np.mean(val_dice)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     21,
     43,
     50,
     81,
     103,
     127,
     151
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Group Norm using un-labelled data\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array).astype('float16')\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, optimizer, criterion, trainx, trainy):\n",
    "    batch_size = 2\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "        \n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "\n",
    "train_ids           = train_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "valx_img = sitk.GetImageFromArray(valx.astype('float32')[:, 0, :, :])\n",
    "sitk.WriteImage(valx_img, basepath+'CT-img.nii.gz')\n",
    "\n",
    "\n",
    "model_teacher = UNetDoubleSmallGroupNorm(1,1)#UNetDoubleSmall(1,1)\n",
    "model_teacher.cuda()\n",
    "\n",
    "\n",
    "p1         = torch.load(basepath_models+\"tmi-f-3-93.pt\")\n",
    "model_teacher.load_state_dict(p1)\n",
    "\n",
    "\n",
    "device             = torch.device(\"cuda:0\")\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0005)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_loss_array   = []\n",
    "train_loss_array = []\n",
    "\n",
    "prev_max_teacher = -1000\n",
    "prev_max    = -1000\n",
    "model_count = 0\n",
    "step_size   = 20\n",
    "beta        = 0.9\n",
    "\n",
    "val_dice_t   = evaluate_result(model_teacher, valx, valy)\n",
    "print(\"Dice in the beginning \", np.mean(val_dice_t))\n",
    "\n",
    "val_dice_t = np.mean(val_dice_t)\n",
    "prev_max   = val_dice_t\n",
    "\n",
    "teacher_dice_array = []\n",
    "\n",
    "train_dice_array = []\n",
    "val_dice_array   = []\n",
    "test_dice_array  = []\n",
    "\n",
    "model_save_name = \"tmi-f-semi-sunet\"\n",
    "\n",
    "first_time = True\n",
    "\n",
    "for epoch in range(300):\n",
    "    temp_index                 = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "    trainx1, trainx1_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "    trainy1                    = get_prediction(model_teacher, trainx1)\n",
    "    \n",
    "    trainx, trainy = sort_data(trainx1, trainy1)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, trainx.shape, trainy.shape)\n",
    "    \n",
    "    train_loss = train_model(model_student, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    train_dice = evaluate_result(model_student, trainx_l, trainy_l)\n",
    "    val_dice   = evaluate_result(model_student, valx, valy)\n",
    "    \n",
    "    # Update teacher weights\n",
    "    if np.mean(val_dice) > val_dice_t:\n",
    "        print(epoch, ' Updating Teacher Weights')\n",
    "        torch.save(model_student.state_dict(), \"temp.pt\")\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+\".pt\")\n",
    "        p1         = torch.load('temp.pt')\n",
    "        \n",
    "        model_teacher.load_state_dict(p1)\n",
    "        val_dice_t = np.mean(val_dice)\n",
    "    \n",
    "    train_dice_array.append(np.mean(train_dice))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    print(\"Step %d  Train Dice %.5f  Val Dice %.5f \" % (epoch, np.mean(train_dice), np.mean(val_dice)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     14
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For selecting the most reliable weights\n",
    "\n",
    "s  = 300\n",
    "a1 = np.zeros([s, 236])\n",
    "b1 = np.zeros([s, 126])\n",
    "\n",
    "count = 0\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "model_save_name = 'tmi-f-single-semi'\n",
    "\n",
    "def scoring_function(val_array, test_array, epoch):\n",
    "    score_array = []\n",
    "    step_size   = 5\n",
    "    alpha       = 10\n",
    "    \n",
    "    temp_array     = val_array[epoch-step_size:epoch]\n",
    "    sum_array      = 1-np.mean(temp_array, axis=0)\n",
    "    var_array      = np.std(temp_array, axis=0)\n",
    "\n",
    "    score_temp     = sum_array + alpha*var_array\n",
    "    final_score    = score_temp*val_array[epoch]\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "for epoch in range(s):\n",
    "    a = np.load('/home/yu-hao/AttentionDeepMIL/val_dice_array-'+model_save_name+'--'+str(epoch)+'.npy')\n",
    "    a1[epoch] = a\n",
    "\n",
    "score_array = []\n",
    "for epoch in range(100, 300):\n",
    "    final_score = scoring_function(a1, b1, epoch)\n",
    "    score_array.append(np.mean(final_score))\n",
    "\n",
    "index = np.argmax(score_array) \n",
    "print('Most reliable Weights Index ', index, score_array[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
