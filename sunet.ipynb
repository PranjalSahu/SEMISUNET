{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "#from scipy.misc import imresize\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "#import hdf5storage\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "#from sklearn.decomposition import PCA\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "\n",
    "import functools\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "code_folding": [
     0,
     11,
     28,
     39,
     66,
     74,
     135,
     223,
     356,
     479
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Pytorch Models for training\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class SUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes  = n_classes\n",
    "        self.bilinear   = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        \n",
    "        self.dp1 = nn.Dropout(p=0.4)\n",
    "        self.dp2 = nn.Dropout(p=0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x1 = self.gn1(x1)\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.gn2(x2)\n",
    "        \n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.gn5(x)\n",
    "       \n",
    "        x = self.up2(x, x3)\n",
    "        x = self.gn6(x)\n",
    "            \n",
    "        x = self.up3(x, x2)\n",
    "        x = self.gn7(x)\n",
    "        \n",
    "        x  = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN_old(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=256, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(128,256)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "   #     print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "  #      print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    " #      print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "       #output = self.lsgn_a(output)\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "############### MAIN MODEL ##############\n",
    "class UNetDoubleSmallGroupNormdifferent_old(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(128,256)\n",
    "        \n",
    "        self.lsgn2 = nn.GroupNorm(64,256)\n",
    "        \n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "       # x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "       # x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "       # x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "       # x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        #print('x5 shape is :',x5.shape)\n",
    "        \n",
    "        xlst = x5.reshape([4,256,256])\n",
    "\n",
    "        lstm = nn.LSTM(256,256,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "                \n",
    "        #print('xlst',xlst.shape)    \n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        \n",
    "        \n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,256,256))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,256,16,16])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    " \n",
    "        \n",
    "        \n",
    "        x5 = self.lsgn2(x5)\n",
    "        \n",
    "        ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        ups4 = ups4.cuda()\n",
    "        \n",
    "        opt = ups4(x5)\n",
    "        \n",
    "        x5 = opt\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "       # x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class UNetDoubleSmallGroupNormdifferent(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc     = DoubleConv(n_channels, 16)\n",
    "        self.down1   = Down(16, 32)\n",
    "        self.downnew = Down(16,16)\n",
    "        self.down2   = Down(32, 64)\n",
    "        self.down3   = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4   = Down(128, 256 // factor) \n",
    "        self.upsam   = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        self.ups3  = nn.ConvTranspose2d(1 , 1, kernel_size=2, stride=2)\n",
    "        self.ups4  = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(64,128)\n",
    "        self.lsgn2 = nn.GroupNorm(64,1024)\n",
    "        self.lsgn3 = nn.GroupNorm(64,1024)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        self.gn8 = nn.GroupNorm(4,8)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        #x = self.upsam()\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        #x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "        #x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "        #x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        #x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        #x5 = self.gn\n",
    "        #x5 = torch.squeeze(x5)\n",
    "        #x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        #print('x5:',x5.shape)\n",
    "        \n",
    "        xlst = x5.reshape([4,128,1024])\n",
    "        \n",
    "\n",
    "        lstm = nn.LSTM(1024,1024,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,128,1024))\n",
    "        #a = ndarray((4,128,1024))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo   = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,128,32,32])\n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        #x5 = self.lsgn3(x5)\n",
    "        \n",
    "        #x5 = self.ups4(x5)\n",
    "    \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "        #x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        #x = self.gn7(x)\n",
    "\n",
    "        #x = self.downnew(x)\n",
    "        \n",
    "        #out    = self.out_softmax(logits)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class UNetDoubleSmallWithoutGN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallWithoutGN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc   = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "       # x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "       # x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "       # x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "       # x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        ups4     = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        upsconv4 = DoubleConv(256,128)\n",
    "        ups4 = ups4.cuda()\n",
    "        \n",
    "        opt = ups4(x5)\n",
    "        \n",
    "        x5 = opt\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "       # x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=128, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p   = dropout_p\n",
    "        self.max_length  = max_length\n",
    "        self.bilinear    = bilinear\n",
    "        self.n_classes   = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn      = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.attn2   = nn.Linear(1024, 128)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(3072, 1024)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru     = nn.GRU(1024, 1024)\n",
    "        self.out     = nn.Linear(1024, 1024)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        #self.lsgn_a = nn.GroupNorm(512,1024)\n",
    "        self.lsbn_a1 = nn.BatchNorm1d(1024)\n",
    "        #self.lsgn_a2 = nn.GroupNorm(512,1024)\n",
    "        \n",
    "        #self.lsgn_in = nn.GroupNorm(64,128)\n",
    "        self.lsbn_in1 = nn.BatchNorm1d(2048)\n",
    "        self.lsbn_in2 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        \n",
    "        self.lsbn_in3 = nn.BatchNorm1d(128)#nn.GroupNorm(64,   128)\n",
    "        self.lsbn_in4 = nn.BatchNorm1d(128)#nn.GroupNorm(64,   128)\n",
    "        self.lsbn_in5 = nn.BatchNorm1d(1024)#nn.GroupNorm(512,  1024)\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4     = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h        = torch.unsqueeze(hidden, 0)\n",
    "        embedded = input\n",
    "        #embedded = self.lsgn_in1(embedded)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        hidden_bilstm = h[0]\n",
    "        hidden_bilinn = hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "        hidden_bilinn = self.lsbn_a1(hidden_bilinn)\n",
    "        \n",
    "        hidden_bi     = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        #print(hidden_bilinn.shape)\n",
    "        \n",
    "        attn_weights  = torch.cat((embedded[0], hidden_bilinn), 1)\n",
    "        attn_weights  = self.lsbn_in1(attn_weights)\n",
    "        \n",
    "        attn_weights  = self.attn(attn_weights)\n",
    "        attn_weights  = self.lsbn_in2(attn_weights)\n",
    "        \n",
    "        attn_weights  = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        attn_weights  = self.attn2(attn_weights)\n",
    "        attn_weights  = self.lsbn_in3(attn_weights)\n",
    "        \n",
    "        #print(attn_weights.unsqueeze(0).shape,encoder_outputs.unsqueeze(0).shape)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        #print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output = self.lsbn_in4(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = self.out(output[0])\n",
    "        output = self.lsbn_in5(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "#model = SUNet(1, 1)\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     202,
     225,
     253,
     265,
     296,
     362,
     435,
     472,
     490
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on MOSMEDDATA dataset\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "#nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "unlabelled_ids     = unlabelled_ids\n",
    "train_ids          = train_ids#[:4]\n",
    "val_ids            = val_ids\n",
    "test_ids           = test_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 100\n",
    "\n",
    "\n",
    "teacher_dice_array = []\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "\n",
    "    train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx, valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    model_save_name = \"tmi-compare-sunet\"\n",
    "    \n",
    "    if np.mean(val_dice) > prev_max:\n",
    "        print(\"Step %d  Dice %.5f > %f  Train Dice %f \" % (epoch, np.mean(val_dice), prev_max, np.mean(student_dice1)))\n",
    "        prev_max     = np.mean(val_dice)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/trainx_mosmed.npy', trainx_l)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/trainy_mosmed.npy', trainy_l)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/valx_mosmed.npy', valx)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/valy_mosmed.npy', valy)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/testx_mosmed.npy', testx)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/testy_mosmed.npy', testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     203,
     226,
     254,
     266,
     297,
     363,
     436,
     473,
     491
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For training LSTM model on MOSMEDDATA dataset\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 1000:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array[x_array < 0] = 0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "#nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "unlabelled_ids     = unlabelled_ids\n",
    "train_ids          = train_ids#[:4]\n",
    "val_ids            = val_ids\n",
    "test_ids           = test_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "prev_max        = -1000\n",
    "model_save_name = \"tmi-compare-lstm\"\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256, dropout_p=0.05)\n",
    "attn_decoder1.cuda()\n",
    "\n",
    "model_student   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "optimizer_attn_w  = optim.Adam(attn_decoder1.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "#trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "teacher_dice_array = []\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    index  = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[index]\n",
    "    trainy = trainy[index]\n",
    "    \n",
    "    train_loss    = train_model(model_student, 4, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx, valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    student_dice2 = evaluate_result(model_student, testx, testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    model_save_name = \"tmi-compare-lstm-mosmed\"\n",
    "    \n",
    "    #if np.mean(val_dice) > prev_max:\n",
    "    print(\"Step %d  Train Dice %f Val Dice %.5f, Test Dice %.5f\" % (epoch, np.mean(student_dice1), np.mean(val_dice), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     199,
     232,
     255,
     283,
     295,
     326,
     392,
     465,
     502,
     520,
     562,
     616,
     684,
     728
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 1, 512, 512) (179, 1, 512, 512) (126, 1, 512, 512)\n",
      "Step 0  Val Dice 0.00000, Train Dice 0.000000, Test Dice 0.000000\n",
      "1\n",
      "Step 1  Val Dice 0.03130, Train Dice 0.055555, Test Dice 0.076240\n",
      "Step 2  Val Dice 0.17153, Train Dice 0.190977, Test Dice 0.280173\n",
      "Step 3  Val Dice 0.21350, Train Dice 0.253893, Test Dice 0.323412\n",
      "Step 4  Val Dice 0.41975, Train Dice 0.368689, Test Dice 0.430583\n",
      "Step 5  Val Dice 0.51642, Train Dice 0.509652, Test Dice 0.552892\n",
      "Step 6  Val Dice 0.45213, Train Dice 0.413167, Test Dice 0.457101\n",
      "Step 7  Val Dice 0.51544, Train Dice 0.500628, Test Dice 0.542900\n",
      "Step 8  Val Dice 0.42377, Train Dice 0.431474, Test Dice 0.474605\n",
      "Step 9  Val Dice 0.47723, Train Dice 0.514814, Test Dice 0.559402\n",
      "Step 10  Val Dice 0.52624, Train Dice 0.562126, Test Dice 0.598172\n",
      "11\n",
      "Step 11  Val Dice 0.52085, Train Dice 0.515840, Test Dice 0.552679\n",
      "Step 12  Val Dice 0.32870, Train Dice 0.343167, Test Dice 0.372356\n",
      "Step 13  Val Dice 0.47750, Train Dice 0.481559, Test Dice 0.525449\n",
      "Step 14  Val Dice 0.51736, Train Dice 0.504383, Test Dice 0.545919\n",
      "Step 15  Val Dice 0.48479, Train Dice 0.487939, Test Dice 0.528930\n",
      "Step 16  Val Dice 0.52137, Train Dice 0.530305, Test Dice 0.588010\n",
      "Step 17  Val Dice 0.49283, Train Dice 0.552340, Test Dice 0.596330\n",
      "Step 18  Val Dice 0.44548, Train Dice 0.473654, Test Dice 0.531378\n",
      "Step 19  Val Dice 0.42250, Train Dice 0.470721, Test Dice 0.542647\n",
      "Step 20  Val Dice 0.51448, Train Dice 0.543176, Test Dice 0.578908\n",
      "21\n",
      "Step 21  Val Dice 0.52599, Train Dice 0.583362, Test Dice 0.606367\n",
      "Step 22  Val Dice 0.51785, Train Dice 0.565803, Test Dice 0.602469\n",
      "Step 23  Val Dice 0.53625, Train Dice 0.575016, Test Dice 0.595225\n",
      "Step 24  Val Dice 0.44125, Train Dice 0.516666, Test Dice 0.562579\n",
      "Step 25  Val Dice 0.53367, Train Dice 0.580189, Test Dice 0.589754\n",
      "Step 26  Val Dice 0.57454, Train Dice 0.631276, Test Dice 0.637715\n",
      "Step 27  Val Dice 0.54995, Train Dice 0.608530, Test Dice 0.627517\n",
      "Step 28  Val Dice 0.54097, Train Dice 0.601224, Test Dice 0.608710\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using LSTM Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "\n",
    "covid19 = False\n",
    "mosmed  = True\n",
    "\n",
    "# For COVID-19 dataset\n",
    "if covid19:\n",
    "    trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "    trainy_l = np.load(basepath+'train_y.npy')\n",
    "    trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "    train_size    = 45\n",
    "    valx          = trainx_l[train_size:]\n",
    "    valy          = trainy_l[train_size:]\n",
    "\n",
    "    trainx_l = trainx_l[:train_size]\n",
    "    trainy_l = trainy_l[:train_size]\n",
    "\n",
    "    testx = np.load(basepath+'test_x.npy')/255.0\n",
    "    testy = np.load(basepath+'test_y.npy')\n",
    "    testy[testy > 0] = 1\n",
    "\n",
    "    trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "    trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    for i in range(trainx_l.shape[0]):\n",
    "        trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "        trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "    for i in range(valx.shape[0]):\n",
    "        valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "        valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "    for i in range(testx.shape[0]):\n",
    "        testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "        testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "    \n",
    "    trainx_l = trainx_l1\n",
    "    trainy_l = trainy_l1\n",
    "    valx     = valx1\n",
    "    valy     = valy1\n",
    "    testx    = testx1\n",
    "    testy    = testy1\n",
    "\n",
    "# For Mosmed Dataset\n",
    "if mosmed:\n",
    "    basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "    basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "    train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "    val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "    test_ids       = np.load(basepath+'TEST.npy')\n",
    "    \n",
    "    train_ids          = train_ids\n",
    "    val_ids            = val_ids\n",
    "    test_ids           = test_ids\n",
    "    \n",
    "    trainx_l, trainy_l = read_training_data(train_ids)\n",
    "    valx, valy         = read_training_data(val_ids)\n",
    "    testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "    \n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256, dropout_p=0.5)\n",
    "attn_decoder1.cuda()\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "optimizer_attn_w  = optim.Adam(attn_decoder1.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# Good setting\n",
    "# 0.5, 0.001, 0.001\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 200\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    index  = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[index]\n",
    "    trainy = trainy[index]\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model2(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    #print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    #print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    #model_save_name = \"ipmi-attentionlstm-covid19\"\n",
    "    model_save_name = \"tmi-compare-lstm\"\n",
    "    \n",
    "    #if np.mean(val_dice1) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice1)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-studentmodel.pt')\n",
    "    torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-attention.pt')\n",
    "    #attn_decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     190,
     281,
     472,
     529
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Double Attention Model\n",
    "\n",
    "class UNetDoubleSmallGroupNormdifferent(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.downnew =Down(16,16)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        \n",
    "        self.upsam = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        self.ups3 = nn.ConvTranspose2d(1 , 1, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.ups4  = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(64,128)\n",
    "        \n",
    "        self.lsgn2 = nn.GroupNorm(64,1024)\n",
    "        \n",
    "        self.lsgnp1 = nn.GroupNorm(128,256)\n",
    "        \n",
    "        self.lsgnp2 = nn.GroupNorm(64,256)\n",
    "        \n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        self.gn8 = nn.GroupNorm(4,8)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        #x = self.upsam()\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        #x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "        #x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "        #x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        #x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        #x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        #print('x5:',x5.shape)\n",
    "        \n",
    "        ############### first attention layer for size 128,32,32 ---part 1 ######################\n",
    "        xlst = x5.reshape([4,128,1024])\n",
    "        \n",
    "\n",
    "        lstm = nn.LSTM(1024,1024,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "                \n",
    "        #print('xlst',xlst.shape)    \n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        #print(f.shape)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,128,1024))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,128,32,32])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        x5 = self.lsgn1(x5)\n",
    "        ############### second attention layer for size 256,16,16 ---part 2 ######################\n",
    "        x5 = self.down5(x5)\n",
    "        xlst = x5.reshape([4,256,256])\n",
    "        lstm = nn.LSTM(256,256,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "        \n",
    "        xlst = self.lsgnp1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "    \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,256,256))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder2.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgnp2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,256,16,16])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        x5 = self.lsgnp2(x5)\n",
    "        \n",
    "        x5 = self.ups4(x5)\n",
    "        \n",
    "        #x5 = self.ups4(x5)\n",
    "    \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "        #x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        #x = self.gn7(x)\n",
    "\n",
    "        #x = self.downnew(x)\n",
    "        \n",
    "        #out    = self.out_softmax(logits)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN_2(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=256, bilinear=True):\n",
    "        super(AttnDecoderRNN_2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(128,256)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        embedded =self.lsgn_a(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "   #     print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "  #      print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    " #      print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output = self.lsgn_a(output)\n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "       #output = self.lsgn_a(output)\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=128, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.attn2 = nn.Linear(1024,128)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(3072, 1024)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(1024, 1024)\n",
    "        self.out = nn.Linear(1024, 1024)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(512,1024)\n",
    "        \n",
    "        self.lsgn_in = nn.GroupNorm(64,128)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        embedded = self.lsgn_in(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        \n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "        \n",
    "        #print('hidden bilinn shape:',hidden_bilinn.shape)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "        #print(hidden_bilinn.shape)\n",
    "        \n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "        \n",
    "        attn_weights  = self.attn2(attn_weights)\n",
    "        \n",
    "        attn_weights = self.lsgn_in(attn_weights)\n",
    "        \n",
    "        #print(attn_weights.unsqueeze(0).shape,encoder_outputs.unsqueeze(0).shape)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        #print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        #print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    "        \n",
    "        #print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output = self.lsgn_in(output)\n",
    "        \n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx          = trainx_l[train_size:]\n",
    "valy          = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx     = valx1\n",
    "valy     = valy1\n",
    "testx    = testx1\n",
    "testy    = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256,dropout_p=0.85)\n",
    "attn_decoder1.cuda()\n",
    "\n",
    "attn_decoder2 = AttnDecoderRNN_2(256, 256,dropout_p=0.85)\n",
    "attn_decoder2.cuda()\n",
    "\n",
    "model_student = UNetDoubleSmallGroupNormdifferent(1,1)\n",
    "model_student.cuda()\n",
    "\n",
    "\n",
    "optimizer_student    = optim.Adam(model_student.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "optimizer_attn_w    = optim.Adam(attn_decoder1.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "optimizer_attn_w_2  = optim.Adam(attn_decoder2.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_loss_array   = []\n",
    "train_loss_array = []\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "              \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        optimizer_attn_w_2.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "           \n",
    "        loss_array.append(loss.item())\n",
    "\n",
    "    #    torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "        \n",
    "        optimizer_attn_w_2.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "total_epochs       = 1000\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    index  = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[index]\n",
    "    trainy = trainy[index]\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    #print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    #print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    #model_save_name = \"ipmi-attentionlstm-covid19\"\n",
    "    model_save_name = \"tmi-compare-lstm\"\n",
    "    \n",
    "    #if np.mean(val_dice1) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice1)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-double-studentmodel.pt')\n",
    "    torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-double-attention.pt')\n",
    "    #attn_decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     200,
     233,
     256,
     284,
     296,
     327,
     393,
     466,
     503,
     521,
     563,
     617
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using SU-Net Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "#index    = np.random.permutation(trainx_l.shape[0])\n",
    "#trainx_l = trainx_l[index]\n",
    "#trainy_l = trainy_l[index]\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = trainx_l, trainy_l\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 30\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    model_save_name = \"ipmi-sunet-covid19\"\n",
    "    \n",
    "    if np.mean(val_dice1) > prev_max:\n",
    "        print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "        prev_max     = np.mean(val_dice1)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-6.pt')\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     213,
     236,
     264,
     276,
     307,
     373,
     446,
     483,
     501,
     543,
     597
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Semi-supervised training SU-Net Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, datax):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(datax)//batch_size):\n",
    "        x = datax[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "unlabelledx_l = np.load(basepath+'unlabelled_x.npy')/255.0\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "unlabelledx1    = np.zeros([unlabelledx_l.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "for i in range(unlabelledx1.shape[0]):\n",
    "    unlabelledx1[i, 0] = scipy.ndimage.zoom(unlabelledx_l[i], 2, order=3)\n",
    "    #testy1[i, 0] = scipy.ndimage.zoom(unlabelledy1[i], 2, order=0)\n",
    "\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "model_student.load_state_dict(p1)\n",
    "\n",
    "\n",
    "unlabelledy1 = get_prediction(model_student, unlabelledx1)\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape, unlabelledy1.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "model_student.load_state_dict(p1)\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "#criterion          = nn.MSELoss()\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "#trainy[trainy > 0.5] = 1\n",
    "#trainy[trainy < 0.5] = 0\n",
    "total_epochs     = 1000\n",
    "#trainx, trainy   = sort_data(trainx, trainy)\n",
    "\n",
    "trainx = trainx#[800:]\n",
    "trainy = trainy#[800:]\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "index        = np.random.permutation(np.arange(len(unlabelledx1)))\n",
    "unlabelledx1 = unlabelledx1[index]\n",
    "unlabelledy1 = unlabelledy1[index]\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    # Re-generate pseudo labels again\n",
    "    if epoch%5 == 0:\n",
    "        step_size        = len(unlabelledx1)//20-1\n",
    "        inputx           = unlabelledx1#[0:(1+epoch%20)*step_size]\n",
    "        unlabelledy1     = get_prediction(model_student, inputx)\n",
    "        #unlabelledy1[unlabelledy1 < 0.5] = 0\n",
    "        #unlabelledy1[unlabelledy1 > 0.5] = 1\n",
    "        trainx_l1        = inputx#np.concatenate([trainx_l, inputx], axis=0)\n",
    "        trainy_l1        = unlabelledy1#np.concatenate([trainy_l, unlabelledy1], axis=0)\n",
    "        trainx, trainy   = sort_data(trainx_l1, trainy_l1)\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    \n",
    "    model_save_name = \"tmi-compare-sunet-covid19-semi\"\n",
    "    \n",
    "    #if np.mean(val_dice) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     199,
     232,
     255,
     283,
     295,
     326,
     392,
     465,
     502,
     520,
     562,
     616
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Semi-supervised training LSTM Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, datax):\n",
    "    output_array   = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(datax)//batch_size):\n",
    "        x = datax[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "unlabelledx_l = np.load(basepath+'unlabelled_x.npy')/255.0\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "unlabelledx1    = np.zeros([unlabelledx_l.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "for i in range(unlabelledx1.shape[0]):\n",
    "    unlabelledx1[i, 0] = scipy.ndimage.zoom(unlabelledx_l[i], 2, order=3)\n",
    "    #testy1[i, 0] = scipy.ndimage.zoom(unlabelledy1[i], 2, order=0)\n",
    "\n",
    "model_save_name = \"tmi-compare-lstm\"\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256, dropout_p=0.45)\n",
    "attn_decoder1.cuda()\n",
    "p1         = torch.load(basepath_models+model_save_name+'-attention.pt')\n",
    "attn_decoder1.load_state_dict(p1)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_teacher   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_teacher.cuda()\n",
    "p1         = torch.load(basepath_models+model_save_name+'-studentmodel.pt')\n",
    "model_teacher.load_state_dict(p1)\n",
    "\n",
    "\n",
    "model_student   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "#model_student   = SUNet(1, 1)\n",
    "#model_student.cuda()\n",
    "#p1         = torch.load(basepath_models+model_save_name+'-studentmodel.pt')\n",
    "#model_student.load_state_dict(p1)\n",
    "\n",
    "\n",
    "unlabelledy1 = get_prediction(model_student, unlabelledx1)\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape, unlabelledy1.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "#model_student   = SUNet(1, 1)\n",
    "#model_student.cuda()\n",
    "#p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "#model_student.load_state_dict(p1)\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.00001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "#criterion          = nn.MSELoss()\n",
    "\n",
    "optimizer_attn_w  = optim.Adam(attn_decoder1.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "#trainy[trainy > 0.5] = 1\n",
    "#trainy[trainy < 0.5] = 0\n",
    "total_epochs     = 1000\n",
    "#trainx, trainy   = sort_data(trainx, trainy)\n",
    "\n",
    "trainx = trainx#[800:]\n",
    "trainy = trainy#[800:]\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "index        = np.random.permutation(np.arange(len(unlabelledx1)))\n",
    "unlabelledx1 = unlabelledx1[index]\n",
    "unlabelledy1 = unlabelledy1[index]\n",
    "\n",
    "pred          = get_prediction(model_teacher, testx)\n",
    "student_dice2 = evaluate_result_new(pred, testy)\n",
    "\n",
    "print('Student Dice in beginning is ', np.mean(student_dice2))\n",
    "    \n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    # Re-generate pseudo labels again\n",
    "    #if epoch%5 == 0:\n",
    "    step_size        = len(unlabelledx1)//20-1\n",
    "    inputx           = unlabelledx1#[0:(1+epoch%20)*step_size]\n",
    "    unlabelledy1     = get_prediction(model_teacher, inputx)\n",
    "    unlabelledy1[unlabelledy1 < 0.5] = 0\n",
    "    unlabelledy1[unlabelledy1 > 0.5] = 1\n",
    "    trainx_l1        = inputx#np.concatenate([trainx_l, inputx], axis=0)\n",
    "    trainy_l1        = unlabelledy1#np.concatenate([trainy_l, unlabelledy1], axis=0)\n",
    "    index = np.random.permutation(np.arange(len(trainx_l1)))\n",
    "    trainx_l1 = trainx_l1[index]\n",
    "    trainy_l1 = trainy_l1[index]\n",
    "    #trainx, trainy   = sort_data(trainx_l1, trainy_l1)\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model2(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    \n",
    "    print('Student length is ', len(student_dice2))\n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    \n",
    "    model_save_name = \"tmi-compare-lstm-semi\"\n",
    "    \n",
    "    if np.mean(val_dice) > prev_max:\n",
    "        print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "        prev_max     = np.mean(val_dice)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-modelstudent.pt')\n",
    "        torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-attention.pt')\n",
    "    \n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     213,
     236,
     264,
     276,
     307,
     373,
     446,
     483,
     501,
     543,
     597
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using SU-Net  Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 40\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = UNetDoubleSmallWithoutGN(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 1000\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    model_save_name = \"tmi-compare-unet-covid19\"\n",
    "    \n",
    "    #if np.mean(val_dice) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     81,
     103,
     127,
     151
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Group Norm using un-labelled data\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array).astype('float16')\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, optimizer, criterion, trainx, trainy):\n",
    "    batch_size = 2\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "        \n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "\n",
    "train_ids           = train_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "valx_img = sitk.GetImageFromArray(valx.astype('float32')[:, 0, :, :])\n",
    "sitk.WriteImage(valx_img, basepath+'CT-img.nii.gz')\n",
    "\n",
    "\n",
    "model_teacher = SUNet(1,1)#UNetDoubleSmall(1,1)\n",
    "model_teacher.cuda()\n",
    "\n",
    "\n",
    "p1         = torch.load(basepath_models+\"tmi-f-3-93.pt\")\n",
    "model_teacher.load_state_dict(p1)\n",
    "\n",
    "\n",
    "device             = torch.device(\"cuda:0\")\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0005)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_loss_array   = []\n",
    "train_loss_array = []\n",
    "\n",
    "prev_max_teacher = -1000\n",
    "prev_max    = -1000\n",
    "model_count = 0\n",
    "step_size   = 20\n",
    "beta        = 0.9\n",
    "\n",
    "val_dice_t   = evaluate_result(model_teacher, valx, valy)\n",
    "print(\"Dice in the beginning \", np.mean(val_dice_t))\n",
    "\n",
    "val_dice_t = np.mean(val_dice_t)\n",
    "prev_max   = val_dice_t\n",
    "\n",
    "teacher_dice_array = []\n",
    "\n",
    "train_dice_array = []\n",
    "val_dice_array   = []\n",
    "test_dice_array  = []\n",
    "\n",
    "model_save_name = \"tmi-f-semi-sunet\"\n",
    "\n",
    "first_time = True\n",
    "\n",
    "for epoch in range(300):\n",
    "    temp_index                 = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "    trainx1, trainx1_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "    trainy1                    = get_prediction(model_teacher, trainx1)\n",
    "    \n",
    "    trainx, trainy = sort_data(trainx1, trainy1)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, trainx.shape, trainy.shape)\n",
    "    \n",
    "    train_loss = train_model(model_student, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    train_dice = evaluate_result(model_student, trainx_l, trainy_l)\n",
    "    val_dice   = evaluate_result(model_student, valx, valy)\n",
    "    \n",
    "    # Update teacher weights\n",
    "    if np.mean(val_dice) > val_dice_t:\n",
    "        print(epoch, ' Updating Teacher Weights')\n",
    "        torch.save(model_student.state_dict(), \"temp.pt\")\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+\".pt\")\n",
    "        p1         = torch.load('temp.pt')\n",
    "        \n",
    "        model_teacher.load_state_dict(p1)\n",
    "        val_dice_t = np.mean(val_dice)\n",
    "    \n",
    "    train_dice_array.append(np.mean(train_dice))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    print(\"Step %d  Train Dice %.5f  Val Dice %.5f \" % (epoch, np.mean(train_dice), np.mean(val_dice)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For selecting the most reliable weights\n",
    "\n",
    "s  = 300\n",
    "a1 = np.zeros([s, 236])\n",
    "b1 = np.zeros([s, 126])\n",
    "\n",
    "count = 0\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "model_save_name = 'tmi-f-single-semi'\n",
    "\n",
    "def scoring_function(val_array, test_array, epoch):\n",
    "    score_array = []\n",
    "    step_size   = 5\n",
    "    alpha       = 10\n",
    "    \n",
    "    temp_array     = val_array[epoch-step_size:epoch]\n",
    "    sum_array      = 1-np.mean(temp_array, axis=0)\n",
    "    var_array      = np.std(temp_array, axis=0)\n",
    "\n",
    "    score_temp     = sum_array + alpha*var_array\n",
    "    final_score    = score_temp*val_array[epoch]\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "for epoch in range(s):\n",
    "    a = np.load('/home/yu-hao/AttentionDeepMIL/val_dice_array-'+model_save_name+'--'+str(epoch)+'.npy')\n",
    "    a1[epoch] = a\n",
    "\n",
    "score_array = []\n",
    "for epoch in range(100, 300):\n",
    "    final_score = scoring_function(a1, b1, epoch)\n",
    "    score_array.append(np.mean(final_score))\n",
    "\n",
    "index = np.argmax(score_array) \n",
    "print('Most reliable Weights Index ', index, score_array[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
