{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "#!pip install monai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numba\n",
    "from numba import njit, prange\n",
    "\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from skimage.measure import label\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import zoom\n",
    "#from scipy.misc import imresize\n",
    "import pywt\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "import pywt\n",
    "#import hdf5storage\n",
    "\n",
    "import scipy.io as sio\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "import pywt\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import skimage.io as io\n",
    "#from sklearn.decomposition import PCA\n",
    "import collections, numpy\n",
    "import warnings\n",
    "from scipy import ndimage, misc\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy\n",
    "import warnings\n",
    "\n",
    "import functools\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import monai\n",
    "from monai.handlers import CheckpointSaver, MeanDice, StatsHandler, ValidationHandler\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscreted,\n",
    "    CastToTyped,\n",
    "    LoadNiftid,\n",
    "    Orientationd,\n",
    "    RandAffined,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    ")\n",
    "\n",
    "np.random.seed(0)\n",
    "#torch.manual_seed(0)!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     11,
     28,
     48,
     82,
     99,
     114,
     125,
     136,
     163,
     190,
     198,
     206,
     269,
     332,
     393,
     429,
     517,
     650,
     773,
     836,
     934,
     939,
     1021
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Pytorch Models for training\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class DoubleConv_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Down_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv_3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class Up_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up   = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "            self.conv = DoubleConv_3D(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv_3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes  = n_classes\n",
    "        self.bilinear   = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        \n",
    "        self.dp1 = nn.Dropout(p=0.2)\n",
    "        self.dp2 = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x1 = self.gn1(x1)\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.gn2(x2)\n",
    "        \n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.gn3(x3)\n",
    "        #x3 = self.dp1(x3)\n",
    "        \n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.gn4(x4)\n",
    "        #x4 = self.dp2(x4)\n",
    "        \n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.gn5(x)\n",
    "       \n",
    "        x = self.up2(x, x3)\n",
    "        x = self.gn6(x)\n",
    "            \n",
    "        x = self.up3(x, x2)\n",
    "        x = self.gn7(x)\n",
    "        \n",
    "        x  = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class SUNet_3D(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet_3D, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes  = n_classes\n",
    "        self.bilinear   = bilinear\n",
    "\n",
    "        self.inc   = DoubleConv_3D(n_channels, 16)\n",
    "        self.down1 = Down_3D(16, 32)\n",
    "        self.down2 = Down_3D(32, 64)\n",
    "        self.down3 = Down_3D(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down_3D(128, 256 // factor)\n",
    "        self.up1 = Up_3D(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up_3D(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up_3D(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up_3D(32, 16, bilinear)\n",
    "        self.outc = OutConv_3D(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        \n",
    "        self.dp1 = nn.Dropout(p=0.2)\n",
    "        self.dp2 = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x1 = self.gn1(x1)\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.gn2(x2)\n",
    "        \n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.gn3(x3)\n",
    "        #x3 = self.dp1(x3)\n",
    "        \n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.gn4(x4)\n",
    "        #x4 = self.dp2(x4)\n",
    "        \n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.gn5(x)\n",
    "       \n",
    "        x = self.up2(x, x3)\n",
    "        x = self.gn6(x)\n",
    "            \n",
    "        x = self.up3(x, x2)\n",
    "        x = self.gn7(x)\n",
    "        \n",
    "        x  = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class SUNet_with_BN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet_with_BN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes  = n_classes\n",
    "        self.bilinear   = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.gn1 = nn.BatchNorm2d(16)\n",
    "        self.gn2 = nn.BatchNorm2d(32)\n",
    "        self.gn3 = nn.BatchNorm2d(64)\n",
    "        self.gn4 = nn.BatchNorm2d(128)\n",
    "        self.gn5 = nn.BatchNorm2d(64)\n",
    "        self.gn6 = nn.BatchNorm2d(32)\n",
    "        self.gn7 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.dp1 = nn.Dropout(p=0.4)\n",
    "        self.dp2 = nn.Dropout(p=0.4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x1 = self.gn1(x1)\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.gn2(x2)\n",
    "        \n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.gn5(x)\n",
    "       \n",
    "        x = self.up2(x, x3)\n",
    "        x = self.gn6(x)\n",
    "            \n",
    "        x = self.up3(x, x2)\n",
    "        x = self.gn7(x)\n",
    "        \n",
    "        x  = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class SUNet_without_GN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(SUNet_without_GN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes  = n_classes\n",
    "        self.bilinear   = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        \n",
    "        x5 = self.down4(x4)\n",
    "        x  = self.up1(x5, x4)\n",
    "        x  = self.up2(x, x3)\n",
    "        x  = self.up3(x, x2)\n",
    "        x  = self.up4(x, x1)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN_old(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=256, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(128,256)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "   #     print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "  #      print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    " #      print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "       #output = self.lsgn_a(output)\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "############### MAIN MODEL ##############\n",
    "class UNetDoubleSmallGroupNormdifferent_old(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(128,256)\n",
    "        \n",
    "        self.lsgn2 = nn.GroupNorm(64,256)\n",
    "        \n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "       # x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "       # x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "       # x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "       # x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        #print('x5 shape is :',x5.shape)\n",
    "        \n",
    "        xlst = x5.reshape([4,256,256])\n",
    "\n",
    "        lstm = nn.LSTM(256,256,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "                \n",
    "        #print('xlst',xlst.shape)    \n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        \n",
    "        \n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,256,256))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,256,16,16])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    " \n",
    "        \n",
    "        \n",
    "        x5 = self.lsgn2(x5)\n",
    "        \n",
    "        ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        ups4 = ups4.cuda()\n",
    "        \n",
    "        opt = ups4(x5)\n",
    "        \n",
    "        x5 = opt\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "       # x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class UNetDoubleSmallGroupNormdifferent(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc     = DoubleConv(n_channels, 16)\n",
    "        self.down1   = Down(16, 32)\n",
    "        self.downnew = Down(16,16)\n",
    "        self.down2   = Down(32, 64)\n",
    "        self.down3   = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4   = Down(128, 256 // factor) \n",
    "        self.upsam   = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        self.ups3  = nn.ConvTranspose2d(1 , 1, kernel_size=2, stride=2)\n",
    "        self.ups4  = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(64,128)\n",
    "        self.lsgn2 = nn.GroupNorm(64,1024)\n",
    "        self.lsgn3 = nn.GroupNorm(64,1024)\n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        self.gn8 = nn.GroupNorm(4,8)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        #x = self.upsam()\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        #x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "        #x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "        #x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        #x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        #x5 = self.gn\n",
    "        #x5 = torch.squeeze(x5)\n",
    "        #x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        #print('x5:',x5.shape)\n",
    "        \n",
    "        xlst = x5.reshape([4,128,1024])\n",
    "        \n",
    "\n",
    "        lstm = nn.LSTM(1024,1024,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,128,1024))\n",
    "        #a = ndarray((4,128,1024))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo   = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,128,32,32])\n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        #x5 = self.lsgn3(x5)\n",
    "        \n",
    "        #x5 = self.ups4(x5)\n",
    "    \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "        #x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        #x = self.gn7(x)\n",
    "\n",
    "        #x = self.downnew(x)\n",
    "        \n",
    "        #out    = self.out_softmax(logits)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class UNetDoubleSmallWithoutGN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallWithoutGN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc   = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "       # x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "       # x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "       # x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "       # x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        ups4     = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        upsconv4 = DoubleConv(256,128)\n",
    "        ups4 = ups4.cuda()\n",
    "        \n",
    "        opt = ups4(x5)\n",
    "        \n",
    "        x5 = opt\n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "       # x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=128, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p   = dropout_p\n",
    "        self.max_length  = max_length\n",
    "        self.bilinear    = bilinear\n",
    "        self.n_classes   = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn      = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.attn2   = nn.Linear(1024, 128)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(3072, 1024)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru     = nn.GRU(1024, 1024)\n",
    "        self.out     = nn.Linear(1024, 1024)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        #self.lsgn_a = nn.GroupNorm(512,1024)\n",
    "        self.lsbn_a1 = nn.BatchNorm1d(1024)\n",
    "        #self.lsgn_a2 = nn.GroupNorm(512,1024)\n",
    "        \n",
    "        #self.lsgn_in = nn.GroupNorm(64,128)\n",
    "        self.lsbn_in1 = nn.BatchNorm1d(2048)\n",
    "        self.lsbn_in2 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        \n",
    "        self.lsbn_in3 = nn.BatchNorm1d(128)#nn.GroupNorm(64,   128)\n",
    "        self.lsbn_in4 = nn.BatchNorm1d(128)#nn.GroupNorm(64,   128)\n",
    "        self.lsbn_in5 = nn.BatchNorm1d(1024)#nn.GroupNorm(512,  1024)\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4     = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h        = torch.unsqueeze(hidden, 0)\n",
    "        embedded = input\n",
    "        #embedded = self.lsgn_in1(embedded)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        hidden_bilstm = h[0]\n",
    "        hidden_bilinn = hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "        hidden_bilinn = self.lsbn_a1(hidden_bilinn)\n",
    "        \n",
    "        hidden_bi     = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        #print(hidden_bilinn.shape)\n",
    "        \n",
    "        attn_weights  = torch.cat((embedded[0], hidden_bilinn), 1)\n",
    "        attn_weights  = self.lsbn_in1(attn_weights)\n",
    "        \n",
    "        attn_weights  = self.attn(attn_weights)\n",
    "        attn_weights  = self.lsbn_in2(attn_weights)\n",
    "        \n",
    "        attn_weights  = F.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        attn_weights  = self.attn2(attn_weights)\n",
    "        attn_weights  = self.lsbn_in3(attn_weights)\n",
    "        \n",
    "        #print(attn_weights.unsqueeze(0).shape,encoder_outputs.unsqueeze(0).shape)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        #print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output = self.lsbn_in4(output)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = self.out(output[0])\n",
    "        output = self.lsbn_in5(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Unet implementation\n",
    "    Paper: https://arxiv.org/abs/1804.03999\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ch=1, output_ch=1):\n",
    "        super(AttU_Net, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        #self.active = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        #print(x5.shape)\n",
    "        d5 = self.Up5(e5)\n",
    "        #print(d5.shape)\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "      #  out = self.active(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class UNetNormal(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNetNormal, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        \n",
    "        my_factor = 1\n",
    "        factor    = 1\n",
    "        \n",
    "        self.inc   = DoubleConv(n_channels, 32*my_factor)\n",
    "        self.down1 = Down(32*my_factor, 64*my_factor)\n",
    "        self.down2 = Down(64*my_factor, 128*my_factor)\n",
    "        self.down3 = Down(128*my_factor, 256*my_factor)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(256*my_factor, 512*my_factor // factor)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(256,512)\n",
    "        self.lsgn2 = nn.GroupNorm(512,1024)\n",
    "        \n",
    "        self.up1 = Up(512*my_factor, 256*my_factor // factor, bilinear)\n",
    "        self.up2 = Up(256*my_factor, 128*my_factor // factor, bilinear)\n",
    "        self.up3 = Up(128*my_factor, 64*my_factor // factor, bilinear)\n",
    "        self.up4 = Up(64*my_factor, 32*my_factor, bilinear)\n",
    "        self.outc = OutConv(32*my_factor, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        #self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        \n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        #out    = self.out_softmax(logits)\n",
    "        return logits\n",
    "\n",
    "#model = SUNet_3D(1, 1)\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     202,
     225,
     253,
     265,
     296,
     362,
     435,
     472,
     490
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on MOSMEDDATA dataset\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "#nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "unlabelled_ids     = unlabelled_ids\n",
    "train_ids          = train_ids#[:4]\n",
    "val_ids            = val_ids\n",
    "test_ids           = test_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 100\n",
    "\n",
    "\n",
    "teacher_dice_array = []\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "\n",
    "    train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx, valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    model_save_name = \"tmi-compare-sunet\"\n",
    "    \n",
    "    if np.mean(val_dice) > prev_max:\n",
    "        print(\"Step %d  Dice %.5f > %f  Train Dice %f \" % (epoch, np.mean(val_dice), prev_max, np.mean(student_dice1)))\n",
    "        prev_max     = np.mean(val_dice)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     199,
     230,
     253,
     281,
     293,
     324,
     390,
     463,
     500,
     518,
     566,
     622,
     653
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 512, 512) (581, 1, 512, 512) (581, 1, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For training models on Challenge Dataset\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(len(models)):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(len(models)):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "       \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "       \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = output[0, 0].astype('uint8')#scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = y[0, 0]#scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            t1 = random.randint(0, 100)\n",
    "            if t1 > 60:\n",
    "                for k in range(x.shape[0]):\n",
    "                    rotv = random.randint(0, 3)\n",
    "                    x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                    y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "covid19 = True\n",
    "mosmed  = False\n",
    "\n",
    "# For COVID-19 dataset\n",
    "if covid19:\n",
    "    trainx_l = (np.load(basepath+'train_x_challenge.npy')+1500.0)/1500.0\n",
    "    trainx_l[trainx_l > 1] = 1\n",
    "    trainx_l[trainx_l < 0] = 0\n",
    "    \n",
    "    trainy_l = np.load(basepath+'train_y_challenge.npy')\n",
    "    trainy_l[trainy_l > 0] = 1\n",
    "    \n",
    "    #index  = np.random.permutation(trainx_l.shape[0])\n",
    "    #trainx_l = trainx_l[index]\n",
    "    #trainy_l = trainy_l[index]\n",
    "    \n",
    "    train_size    = 4000\n",
    "    val_size      = 400\n",
    "    \n",
    "    testx         = trainx_l[train_size+val_size:]\n",
    "    testy         = trainy_l[train_size+val_size:]\n",
    "    \n",
    "    valx          = testx#trainx_l[train_size: train_size+val_size]\n",
    "    valy          = testy#trainy_l[train_size: train_size+val_size]\n",
    "\n",
    "    trainx_l = trainx_l[:train_size]\n",
    "    trainy_l = trainy_l[:train_size]\n",
    "\n",
    "    #testx = #np.load(basepath+'test_x.npy')/255.0\n",
    "    #testy = #np.load(basepath+'test_y.npy')\n",
    "    #testy[testy > 0] = 1\n",
    "\n",
    "    #trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    #valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    #testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    #trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    #valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    #testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    #for i in range(trainx_l.shape[0]):\n",
    "    #    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    #    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "    #for i in range(valx.shape[0]):\n",
    "    #    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    #    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "    \n",
    "    #for i in range(testx.shape[0]):\n",
    "    #    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    #    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "    \n",
    "    #trainx_l = trainx_l1\n",
    "    #trainy_l = trainy_l1\n",
    "    #valx     = valx1\n",
    "    #valy     = valy1\n",
    "    #testx    = testx1\n",
    "    #testy    = testy1\n",
    "    \n",
    "# For Mosmed Dataset\n",
    "if mosmed:\n",
    "    basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "    basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "    train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "    val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "    test_ids       = np.load(basepath+'TEST.npy')\n",
    "    \n",
    "    train_ids          = train_ids\n",
    "    val_ids            = val_ids\n",
    "    test_ids           = test_ids\n",
    "    \n",
    "    trainx_l, trainy_l = read_training_data(train_ids)\n",
    "    valx, valy         = read_training_data(val_ids)\n",
    "    testx, testy       = read_training_data(test_ids)\n",
    "    \n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "\n",
    "#model_student   = AttU_Net(1, 1)\n",
    "\n",
    "#trainx, trainy = sort_data(trainx_l, trainy_l)\n",
    "\n",
    "for ktr in range(0, 0):\n",
    "    prev_max        = -1000\n",
    "    model_student   = SUNet(1, 1)\n",
    "    model_student.cuda()\n",
    "\n",
    "    #optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001, betas=(0.8, 0.9))\n",
    "    optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001, betas=(0.9, 0.99))\n",
    "    criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "    val_dice_array   = []\n",
    "    train_dice_array = []\n",
    "    test_dice_array  = []\n",
    "\n",
    "    \n",
    "    total_epochs = 50\n",
    "\n",
    "    teacher_dice_array = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    index  = np.random.choice(len(trainx_l), 3500)\n",
    "    trainx = trainx_l#[index]\n",
    "    trainy = trainy_l#[index]\n",
    "    \n",
    "    for epoch in range(0, total_epochs):\n",
    "        if epoch%5 ==1:\n",
    "            print(epoch)\n",
    "\n",
    "        index  = np.random.permutation(trainx.shape[0])\n",
    "        trainx = trainx[index]\n",
    "        trainy = trainy[index]\n",
    "\n",
    "        train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, True)\n",
    "\n",
    "        #pred          = get_prediction(model_student, valx)\n",
    "        #val_dice      = evaluate_result(model_student, valx, valy)\n",
    "\n",
    "        #pred          = get_prediction(model_student, trainx)\n",
    "        student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "\n",
    "        #pred          = get_prediction(model_student, testx)\n",
    "        student_dice2 = evaluate_result(model_student, testx, testy)\n",
    "        val_dice      = student_dice2\n",
    "\n",
    "        train_dice_array.append(np.mean(student_dice1))\n",
    "        val_dice_array.append(np.mean(val_dice))\n",
    "        test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "        model_save_name = \"tmi-covid19-challenge19-\"+str(ktr)\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)), current_time-start_time)\n",
    "        if np.mean(val_dice) > prev_max:\n",
    "            prev_max     = np.mean(val_dice)\n",
    "            torch.save(model_student.state_dict(), basepath_models+model_save_name+\".pt\")\n",
    "\n",
    "        start_time   = current_time\n",
    "\n",
    "# # tmi-covid19-challenge7  -> 58 \n",
    "# # tmi-covid19-challenge9  -> 61 \n",
    "# # tmi-covid19-challenge10 -> 63 \n",
    "# # tmi-covid19-challenge12 -> 61 \n",
    "# tmi-covid19-challenge15-B-39 -> 0.595 (sunet 0.0001, with augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For creating the Data Loaders\n",
    "\n",
    "def get_xforms(mode=\"train\", keys=(\"image\", \"label\")):\n",
    "    \"\"\"returns a composed transform for train/val/infer.\"\"\"\n",
    "\n",
    "    xforms = [\n",
    "        LoadNiftid(keys),\n",
    "        AddChanneld(keys),\n",
    "        Orientationd(keys, axcodes=\"LPS\"),\n",
    "        Spacingd(keys, pixdim=(1, 1, 3.0), mode=(\"bilinear\", \"nearest\")[: len(keys)]),\n",
    "        ScaleIntensityRanged(keys[0], a_min=-1000.0, a_max=500.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    "    if mode == \"train\":\n",
    "        xforms.extend(\n",
    "            [\n",
    "                SpatialPadd(keys, spatial_size=(192, 192, -1), mode=\"reflect\"),  # ensure at least 192x192\n",
    "                RandAffined(\n",
    "                    keys,\n",
    "                    prob=0.15,\n",
    "                    rotate_range=(-0.05, 0.05),\n",
    "                    scale_range=(-0.1, 0.1),\n",
    "                    mode=(\"bilinear\", \"nearest\"),\n",
    "                    as_tensor_output=False,\n",
    "                ),\n",
    "                RandCropByPosNegLabeld(keys, label_key=keys[1], spatial_size=(192, 192, 16), num_samples=3),\n",
    "                #RandGaussianNoised(keys[0], prob=0.15, std=0.01),\n",
    "                RandFlipd(keys, spatial_axis=0, prob=0.5),\n",
    "                RandFlipd(keys, spatial_axis=1, prob=0.5),\n",
    "                RandFlipd(keys, spatial_axis=2, prob=0.5),\n",
    "            ]\n",
    "        )\n",
    "        dtype = (np.float32, np.uint8)\n",
    "    if mode == \"val\":\n",
    "        dtype = (np.float32, np.uint8)\n",
    "    if mode == \"infer\":\n",
    "        dtype = (np.float32,)\n",
    "    xforms.extend([CastToTyped(keys, dtype=dtype), ToTensord(keys)])\n",
    "    return monai.transforms.Compose(xforms)\n",
    "\n",
    "\n",
    "data_folder  = '/media/yu-hao/WindowsData/COVID-19-20_v2/Train/'\n",
    "model_folder = './runs1/'\n",
    "\n",
    "\"\"\"run a training pipeline.\"\"\"\n",
    "\n",
    "images = sorted(glob.glob(os.path.join(data_folder, \"*_ct.nii.gz\")))\n",
    "labels = sorted(glob.glob(os.path.join(data_folder, \"*_seg.nii.gz\")))\n",
    "#logging.info(f\"training: image/label ({len(images)}) folder: {data_folder}\")\n",
    "\n",
    "amp  = True  # auto. mixed precision\n",
    "keys = (\"image\", \"label\")\n",
    "train_frac, val_frac = 0.8, 0.2\n",
    "n_train = int(train_frac * len(images)) + 1\n",
    "n_val   = min(len(images) - n_train, int(val_frac * len(images)))\n",
    "#logging.info(f\"training: train {n_train} val {n_val}, folder: {data_folder}\")\n",
    "\n",
    "train_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[:n_train], labels[:n_train])]\n",
    "val_files = [{keys[0]: img, keys[1]: seg} for img, seg in zip(images[-n_val:], labels[-n_val:])]\n",
    "\n",
    "# create a training data loader\n",
    "batch_size = 2\n",
    "#logging.info(f\"batch size {batch_size}\")\n",
    "train_transforms = get_xforms(\"train\", keys)\n",
    "train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms, cache_rate=0.25)\n",
    "train_loader = monai.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# create a validation data loader\n",
    "val_transforms = get_xforms(\"val\", keys)\n",
    "val_ds = monai.data.CacheDataset(data=val_files, transform=val_transforms, cache_num=10)\n",
    "val_loader = monai.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,  # image-level batch to the sliding window method, not the window-level batch\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# create BasicUNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net    = SUNet_3D(1, 2)\n",
    "net    = net.to(device)\n",
    "max_epochs, lr, momentum = 500, 1e-3, 0.95\n",
    "#logging.info(f\"epochs {max_epochs}, lr {lr}, momentum {momentum}\")\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# create evaluator (to be used to measure model quality during training\n",
    "val_post_transform = monai.transforms.Compose(\n",
    "    [AsDiscreted(keys=(\"pred\", \"label\"), argmax=(True, False), to_onehot=True, n_classes=2)]\n",
    ")\n",
    "val_handlers = [\n",
    "    ProgressBar(),\n",
    "    CheckpointSaver(save_dir=model_folder, save_dict={\"net\": net}, save_key_metric=True, key_metric_n_saved=3),\n",
    "]\n",
    "evaluator = monai.engines.SupervisedEvaluator(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    inferer=get_inferer(),\n",
    "    post_transform=val_post_transform,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(include_background=False, output_transform=lambda x: (x[\"pred\"], x[\"label\"]))\n",
    "    },\n",
    "    val_handlers=val_handlers,\n",
    "    amp=amp,\n",
    ")\n",
    "\n",
    "# evaluator as an event handler of the trainer\n",
    "train_handlers = [\n",
    "    ValidationHandler(validator=evaluator, interval=1, epoch_level=True),\n",
    "    StatsHandler(tag_name=\"train_loss\", output_transform=lambda x: x[\"loss\"]),\n",
    "]\n",
    "trainer = monai.engines.SupervisedTrainer(\n",
    "    device=device,\n",
    "    max_epochs=max_epochs,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=opt,\n",
    "    loss_function=DiceCELoss(),\n",
    "    inferer=get_inferer(),\n",
    "    key_train_metric=None,\n",
    "    train_handlers=train_handlers,\n",
    "    amp=amp,\n",
    ")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     16
    ]
   },
   "outputs": [],
   "source": [
    "from ignite.contrib.handlers import ProgressBar\n",
    "class DiceCELoss(nn.Module):\n",
    "    \"\"\"Dice and Xentropy loss\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dice = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice = self.dice(y_pred, y_true)\n",
    "        # CrossEntropyLoss target needs to have shape (B, D, H, W)\n",
    "        # Target from pipeline has shape (B, 1, D, H, W)\n",
    "        cross_entropy = self.cross_entropy(y_pred, torch.squeeze(y_true, dim=1).long())\n",
    "        return dice + cross_entropy\n",
    "def get_inferer(_mode=None):\n",
    "    \"\"\"returns a sliding window inference instance.\"\"\"\n",
    "\n",
    "    patch_size = (192, 192, 16)\n",
    "    sw_batch_size, overlap = 2, 0.5\n",
    "    inferer = monai.inferers.SlidingWindowInferer(\n",
    "        roi_size=patch_size,\n",
    "        sw_batch_size=sw_batch_size,\n",
    "        overlap=overlap,\n",
    "        mode=\"gaussian\",\n",
    "        padding_mode=\"replicate\",\n",
    "    )\n",
    "    return inferer\n",
    "\n",
    "val_handlers = [\n",
    "    ProgressBar(),\n",
    "    CheckpointSaver(save_dir='./runs1/', save_dict={\"net\": net}, save_key_metric=True, key_metric_n_saved=3),\n",
    "]\n",
    "evaluator = monai.engines.SupervisedEvaluator(\n",
    "    device=device,\n",
    "    val_data_loader=val_loader,\n",
    "    network=net,\n",
    "    inferer=get_inferer(),\n",
    "    post_transform=val_post_transform,\n",
    "    key_val_metric={\n",
    "        \"val_mean_dice\": MeanDice(include_background=False, output_transform=lambda x: (x[\"pred\"], x[\"label\"]))\n",
    "    },\n",
    "    val_handlers=val_handlers,\n",
    "    amp=amp,\n",
    ")\n",
    "\n",
    "# evaluator as an event handler of the trainer\n",
    "train_handlers = [\n",
    "    ValidationHandler(validator=evaluator, interval=1, epoch_level=True),\n",
    "    StatsHandler(tag_name=\"train_loss\", output_transform=lambda x: x[\"loss\"]),\n",
    "]\n",
    "trainer = monai.engines.SupervisedTrainer(\n",
    "    device=device,\n",
    "    max_epochs=max_epochs,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=opt,\n",
    "    loss_function=DiceCELoss(),\n",
    "    inferer=get_inferer(),\n",
    "    key_train_metric=None,\n",
    "    train_handlers=train_handlers,\n",
    "    amp=amp,\n",
    ")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0,
     31
    ]
   },
   "source": [
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(len(models)):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(len(models)):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum*1.0/len(models)\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "       \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "       \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = output[0, 0].astype('uint8')#scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = y[0, 0]#scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581, 1, 512, 512) (581, 1, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "all_weights = glob.glob('/home/yu-hao/SEMISUNET/Dataset/models/tmi-covid19-challenge19-*')\n",
    "#all_weights2 = glob.glob('/home/yu-hao/SEMISUNET/Dataset/models/tmi-covid19-challenge16-*')\n",
    "#all_weights = np.concatenate([all_weights1, all_weights2])\n",
    "\n",
    "for p in all_weights:\n",
    "    model_student   = SUNet(1, 1)\n",
    "    model_student.cuda()\n",
    "    model_student.load_state_dict(torch.load(p))\n",
    "    models.append(model_student)\n",
    "\n",
    "result = get_predictions(models, testx)\n",
    "print(result.shape, testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n"
     ]
    }
   ],
   "source": [
    "dice_values = evaluate_result_new(result, testy)\n",
    "print(len(dice_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6543761573626149\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(dice_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512) (512, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2e89adb38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.ndimage import rotate\n",
    "index = random.randint(0, testx.shape[0]-1)\n",
    "img        = testx[index, 0]\n",
    "img_mask   = result[index, 0]\n",
    "img        = img.astype('float32')\n",
    "\n",
    "rotate_img      = img#rotate(img,      -35, reshape=False)\n",
    "rotate_img_mask = testy[index, 0]#rotate(img_mask, -35, reshape=False, order=1)\n",
    "print(img.shape, rotate_img.shape)\n",
    "\n",
    "plt.imshow(np.concatenate([img, rotate_img], axis=-1), cmap='gray')\n",
    "plt.imshow(np.concatenate([img_mask, rotate_img_mask], axis=-1), cmap='jet', alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6336016197230773\n"
     ]
    }
   ],
   "source": [
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "model_student.load_state_dict(torch.load(basepath_models+'tmi-covid19-challenge10-66.pt'))\n",
    "\n",
    "val_dice      = evaluate_result(model_student, valx, valy)\n",
    "print(np.mean(val_dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20,
     42,
     49,
     58,
     89,
     113,
     144,
     154,
     178,
     198,
     227,
     250,
     278,
     290,
     321,
     387,
     460,
     497,
     515
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For getting result for submission to the challenge website ENSEMBLE model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(len(models)):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(len(models)):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum*1.0/len(models)\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "       \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "       \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "models      = []\n",
    "all_weights = glob.glob('/home/yu-hao/SEMISUNET/Dataset/models/tmi-covid19-challenge16-*')\n",
    "for p in all_weights:\n",
    "    model_student   = SUNet(1, 1)\n",
    "    model_student.cuda()\n",
    "    model_student.load_state_dict(torch.load(p))\n",
    "    model_student.eval()\n",
    "    models.append(model_student)\n",
    "\n",
    "\n",
    "write_path = '/media/yu-hao/WindowsData/COVID-19-20_v2/Result/'\n",
    "all_vols   = glob.glob('/media/yu-hao/WindowsData/COVID-19-20_v2/Validation/*')\n",
    "sample_submission_path = '/home/yu-hao/Downloads/garbage1/SampleSubmission/SampleSubmission/'\n",
    "\n",
    "for p in all_vols:\n",
    "    name = p.split('/')[-1].split('-')[-1][1:-10]#split('_')[0][1:]\n",
    "    \n",
    "    temp = sitk.ReadImage(p)\n",
    "    v = (sitk.GetArrayFromImage(temp)+1500.0)/1500.0\n",
    "    v[v > 1] = 1\n",
    "    v[v < 0] = 0\n",
    "    v = np.expand_dims(v, axis=1)\n",
    "    \n",
    "    #print(v.shape, v.dtype)\n",
    "    r =  get_predictions(models, v)\n",
    "    r[r < 0.5] = 0\n",
    "    r[r > 0.5] = 1\n",
    "    \n",
    "    r  = r.astype('uint8')\n",
    "    \n",
    "    rs = r[:, 0]\n",
    "    rs =  sitk.GetImageFromArray(rs)\n",
    "    \n",
    "    sample  = sitk.ReadImage(sample_submission_path+name+'.nii.gz')\n",
    "    sample1 = sitk.GetArrayFromImage(sample)\n",
    "    rs.CopyInformation(sample)\n",
    "    \n",
    "    print(name, sample1.shape, r.shape, rs.GetSize())\n",
    "    sitk.WriteImage(rs, write_path+name+'.nii.gz')\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20,
     42,
     49,
     58,
     89,
     113,
     144,
     154,
     178,
     198,
     227,
     250,
     278,
     290,
     321,
     387,
     460,
     497,
     515
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For getting result for submission to the challenge website\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "       \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "       \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "#model_load_name = \"tmi-covid19-challenge4-15\" -> 60\n",
    "model_load_name = \"tmi-covid19-challenge10-66\"\n",
    "\n",
    "model_student.load_state_dict(torch.load(basepath_models+model_load_name+'.pt'))\n",
    "model_student.eval()\n",
    "\n",
    "\n",
    "write_path = '/media/yu-hao/WindowsData/COVID-19-20_v2/Result/'\n",
    "all_vols   = glob.glob('/media/yu-hao/WindowsData/COVID-19-20_v2/Validation/*')\n",
    "sample_submission_path = '/home/yu-hao/Downloads/garbage1/SampleSubmission/SampleSubmission/'\n",
    "\n",
    "for p in all_vols:\n",
    "    name = p.split('/')[-1].split('-')[-1][1:-10]#split('_')[0][1:]\n",
    "    \n",
    "    temp = sitk.ReadImage(p)\n",
    "    v = (sitk.GetArrayFromImage(temp)+1024.0)/1024.0\n",
    "    v[v > 1] = 1\n",
    "    v[v < 0] = 0\n",
    "    v = np.expand_dims(v, axis=1)\n",
    "    \n",
    "    #print(v.shape, v.dtype)\n",
    "    r =  get_prediction(model_student, v)\n",
    "    r[r < 0.5] = 0\n",
    "    r[r > 0.5] = 1\n",
    "    \n",
    "    r  = r.astype('uint8')\n",
    "    \n",
    "    rs = r[:, 0]\n",
    "    rs =  sitk.GetImageFromArray(rs)\n",
    "    \n",
    "    sample  = sitk.ReadImage(sample_submission_path+name+'.nii.gz')\n",
    "    sample1 = sitk.GetArrayFromImage(sample)\n",
    "    rs.CopyInformation(sample)\n",
    "    \n",
    "    print(name, sample1.shape, r.shape, rs.GetSize())\n",
    "    sitk.WriteImage(rs, write_path+name+'.nii.gz')\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20,
     42,
     49,
     58,
     89,
     113,
     144,
     154,
     178,
     207,
     230,
     258,
     270,
     301,
     367,
     440,
     477,
     495,
     537,
     567,
     628
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Active Learning\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_next_slice(model_student, remx, remy):\n",
    "    noise       = np.random.normal(0, .1, remx.shape)\n",
    "    new_signal1 = remx + noise\n",
    "    \n",
    "    pred  = get_prediction(model_student, remx)\n",
    "    pred1 = get_prediction(model_student, new_signal1)\n",
    "    \n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "\n",
    "    pred1[pred1 > 0.5] = 1\n",
    "    pred1[pred1 < 0.5] = 0\n",
    "\n",
    "    arr = []\n",
    "    for index in range(pred.shape[0]):\n",
    "        arr.append(np.sum(np.abs(pred[index, 0]-pred1[index, 0]))*1.0/den)\n",
    "    \n",
    "    arr1  = np.argsort(arr)    \n",
    "    temp1 = np.expand_dims(remx[arr1[-1]], 0)\n",
    "    temp2 = np.expand_dims(remy[arr1[-1]], 0)\n",
    "    \n",
    "    return temp1, temp2, np.delete(remx, arr1[-1], 0), np.delete(remy, arr1[-1], 0)\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "covid19 = True\n",
    "mosmed  = False\n",
    "\n",
    "# For COVID-19 dataset\n",
    "if covid19:\n",
    "    trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "    trainy_l = np.load(basepath+'train_y.npy')\n",
    "    trainy_l[trainy_l > 0] = 1\n",
    "    \n",
    "    index  = np.random.permutation(trainx_l.shape[0])\n",
    "    trainx_l = trainx_l[index]\n",
    "    trainy_l = trainy_l[index]\n",
    "    \n",
    "    train_size    = 5\n",
    "    val_size      = 5\n",
    "    \n",
    "    remaining_x   = trainx_l[train_size:-val_size]\n",
    "    remaining_y   = trainy_l[train_size:-val_size]\n",
    "    \n",
    "    valx          = trainx_l[-val_size:]\n",
    "    valy          = trainy_l[-val_size:]\n",
    "\n",
    "    trainx_l = trainx_l[:train_size]\n",
    "    trainy_l = trainy_l[:train_size]\n",
    "\n",
    "    testx = np.load(basepath+'test_x.npy')/255.0\n",
    "    testy = np.load(basepath+'test_y.npy')\n",
    "    testy[testy > 0] = 1\n",
    "\n",
    "    trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    remx1     = np.zeros([remaining_x.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    remy1     = np.zeros([remaining_y.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    for i in range(trainx_l.shape[0]):\n",
    "        trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "        trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "    for i in range(valx.shape[0]):\n",
    "        valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "        valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "    \n",
    "    for i in range(remaining_x.shape[0]):\n",
    "        remx1[i, 0] = scipy.ndimage.zoom(remaining_x[i], 2, order=3)\n",
    "        remy1[i, 0] = scipy.ndimage.zoom(remaining_y[i], 2, order=0)\n",
    "    \n",
    "    for i in range(testx.shape[0]):\n",
    "        testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "        testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "    \n",
    "    trainx_l = trainx_l1\n",
    "    trainy_l = trainy_l1\n",
    "    valx     = valx1\n",
    "    valy     = valy1\n",
    "    testx    = testx1\n",
    "    testy    = testy1\n",
    "    remx     = remx1\n",
    "    remy     = remy1\n",
    "\n",
    "# For Mosmed Dataset\n",
    "if mosmed:\n",
    "    basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "    basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "    train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "    val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "    test_ids       = np.load(basepath+'TEST.npy')\n",
    "    \n",
    "    train_ids          = train_ids\n",
    "    val_ids            = val_ids\n",
    "    test_ids           = test_ids\n",
    "    \n",
    "    trainx_l, trainy_l = read_training_data(train_ids)\n",
    "    valx, valy         = read_training_data(val_ids)\n",
    "    testx, testy       = read_training_data(test_ids)\n",
    "    \n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape, remx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "prev_max        = -1000\n",
    "#model_student   = AttU_Net(1, 1)\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx = trainx_l\n",
    "trainy = trainy_l\n",
    "#trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 100\n",
    "\n",
    "\n",
    "\n",
    "#new_slice, remx = get_next_slice(model_student, remx)\n",
    "\n",
    "teacher_dice_array = []\n",
    "total_slices       = 40\n",
    "\n",
    "torch.save(model_student.state_dict(), basepath_models+model_save_name+'-initweight.pt')\n",
    "\n",
    "iteration_array = []\n",
    "\n",
    "for kt in range(total_slices):\n",
    "    for epoch in range(total_epochs):\n",
    "        if epoch%10 ==1:\n",
    "            print(epoch)\n",
    "\n",
    "        index  = np.random.permutation(trainx.shape[0])\n",
    "        trainx = trainx[index]\n",
    "        trainy = trainy[index]\n",
    "\n",
    "        train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, True)\n",
    "\n",
    "        pred          = get_prediction(model_student, valx)\n",
    "        val_dice      = evaluate_result_new(pred, valy)\n",
    "\n",
    "        pred          = get_prediction(model_student, trainx)\n",
    "        student_dice1 = evaluate_result_new(pred, trainy)\n",
    "\n",
    "        pred          = get_prediction(model_student, testx)\n",
    "        student_dice2 = evaluate_result_new(pred, testy)\n",
    "\n",
    "        train_dice_array.append(np.mean(student_dice1))\n",
    "        val_dice_array.append(np.mean(val_dice))\n",
    "        test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "        model_save_name = \"tmi-covid19-noise1\"\n",
    "\n",
    "        if np.mean(val_dice) > prev_max:\n",
    "            print(\"Iteration %d Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (kt, epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "            prev_max     = np.mean(val_dice)\n",
    "            torch.save(model_student.state_dict(), basepath_models+model_save_name+\"-.pt\")\n",
    "        \n",
    "    model_student.load_state_dict(torch.load(basepath_models+model_save_name+\"-.pt\"))\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred,     testy)\n",
    "    iteration_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    new_slice_x, new_slice_y, remx, remy = get_next_slice(model_student, remx, remy)\n",
    "    \n",
    "    model_student.load_state_dict(torch.load(basepath_models+model_save_name+'-initweight.pt'))\n",
    "    \n",
    "    prev_max = -1\n",
    "    print('Before ', trainx.shape, trainy.shape)\n",
    "    trainx = np.concatenate([trainx, new_slice_x])\n",
    "    trainy = np.concatenate([trainy, new_slice_y])\n",
    "    print('After ', trainx.shape, trainy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise       = np.random.normal(0, .1, remx.shape)\n",
    "new_signal1 = remx + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = random.randint(0, remx.shape[0]-1)\n",
    "arr   = []\n",
    "\n",
    "for index in range(remx.shape[0]-1):\n",
    "    den = np.sum(pred[index, 0])\n",
    "    arr.append([np.sum(np.abs(pred[index, 0]-pred1[index, 0]))*1.0/den, np.sum(np.abs(pred1[index, 0]-pred2[index, 0]))*1.0/den, np.sum(np.abs(pred[index, 0]-pred2[index, 0]))*1.0/den])\n",
    "    print(index, np.sum(np.abs(pred[index, 0]-pred1[index, 0]))*1.0/den, np.sum(np.abs(pred1[index, 0]-pred2[index, 0]))*1.0/den, np.sum(np.abs(pred[index, 0]-pred2[index, 0]))*1.0/den)\n",
    "\n",
    "arr = np.array(arr)\n",
    "#plt.imshow(np.concatenate([remx[index, 0], remx[index, 0], new_signal1[index, 0], new_signal2[index, 0]], axis=-1), cmap='gray')\n",
    "#plt.imshow(np.concatenate([pred[index, 0], remy[index, 0], pred1[index, 0], pred2[index, 0]], axis=-1), cmap='jet', alpha=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/trainx_mosmed.npy', trainx_l)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/trainy_mosmed.npy', trainy_l)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/valx_mosmed.npy', valx)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/valy_mosmed.npy', valy)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/testx_mosmed.npy', testx)\n",
    "# np.save('/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/testy_mosmed.npy', testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     199,
     232,
     255,
     283,
     295,
     326,
     392,
     465,
     502,
     520,
     562,
     616,
     684,
     728
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using LSTM Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "\n",
    "covid19 = True\n",
    "mosmed  = False\n",
    "\n",
    "# For COVID-19 dataset\n",
    "if covid19:\n",
    "    trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "    trainy_l = np.load(basepath+'train_y.npy')\n",
    "    trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "    train_size    = 45\n",
    "    valx          = trainx_l[train_size:]\n",
    "    valy          = trainy_l[train_size:]\n",
    "\n",
    "    trainx_l = trainx_l[:train_size]\n",
    "    trainy_l = trainy_l[:train_size]\n",
    "\n",
    "    testx = np.load(basepath+'test_x.npy')/255.0\n",
    "    testy = np.load(basepath+'test_y.npy')\n",
    "    testy[testy > 0] = 1\n",
    "\n",
    "    trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "    trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "    valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "    testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "    \n",
    "    for i in range(trainx_l.shape[0]):\n",
    "        trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "        trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "    for i in range(valx.shape[0]):\n",
    "        valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "        valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "    for i in range(testx.shape[0]):\n",
    "        testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "        testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "    \n",
    "    trainx_l = trainx_l1\n",
    "    trainy_l = trainy_l1\n",
    "    valx     = valx1\n",
    "    valy     = valy1\n",
    "    testx    = testx1\n",
    "    testy    = testy1\n",
    "\n",
    "# For Mosmed Dataset\n",
    "if mosmed:\n",
    "    basepath         = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "    basepath_models  = '/media/yu-hao/WindowsData/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "    train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "    val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "    test_ids       = np.load(basepath+'TEST.npy')\n",
    "    \n",
    "    train_ids          = train_ids\n",
    "    val_ids            = val_ids\n",
    "    test_ids           = test_ids\n",
    "    \n",
    "    trainx_l, trainy_l = read_training_data(train_ids)\n",
    "    valx, valy         = read_training_data(val_ids)\n",
    "    testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "    \n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256, dropout_p=0.5)\n",
    "attn_decoder1.cuda()\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "optimizer_attn_w  = optim.Adam(attn_decoder1.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# Good setting\n",
    "# 0.5, 0.001, 0.001\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 200\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    index  = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[index]\n",
    "    trainy = trainy[index]\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model2(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    #print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    #print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    #model_save_name = \"ipmi-attentionlstm-covid19\"\n",
    "    model_save_name = \"tmi-compare-lstm\"\n",
    "    \n",
    "    #if np.mean(val_dice1) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice1)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-studentmodel.pt')\n",
    "    torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-attention.pt')\n",
    "    #attn_decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     190,
     281,
     472,
     529
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Double Attention Model\n",
    "\n",
    "class UNetDoubleSmallGroupNormdifferent(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,bilinear=True):\n",
    "        \n",
    "        super(UNetDoubleSmallGroupNormdifferent, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.downnew =Down(16,16)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(128, 256 // factor)\n",
    "        \n",
    "        self.upsam = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        self.ups3 = nn.ConvTranspose2d(1 , 1, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.ups4  = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        \n",
    "        \n",
    "        self.up1 = Up(256, 128 // factor, bilinear)\n",
    "        self.up2 = Up(128, 64 // factor, bilinear)\n",
    "        self.up3 = Up(64, 32 // factor, bilinear)\n",
    "        self.up4 = Up(32, 16, bilinear)\n",
    "        self.outc = OutConv(16, n_classes)\n",
    "        #self.out_sigmoid = nn.Sigmoid()\n",
    "        self.out_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lsgn1 = nn.GroupNorm(64,128)\n",
    "        \n",
    "        self.lsgn2 = nn.GroupNorm(64,1024)\n",
    "        \n",
    "        self.lsgnp1 = nn.GroupNorm(128,256)\n",
    "        \n",
    "        self.lsgnp2 = nn.GroupNorm(64,256)\n",
    "        \n",
    "        \n",
    "        self.gn1 = nn.GroupNorm(8, 16)\n",
    "        self.gn2 = nn.GroupNorm(16, 32)\n",
    "        self.gn3 = nn.GroupNorm(32, 64)\n",
    "        self.gn4 = nn.GroupNorm(64, 128)\n",
    "        self.gn5 = nn.GroupNorm(32, 64)\n",
    "        self.gn6 = nn.GroupNorm(16, 32)\n",
    "        self.gn7 = nn.GroupNorm(8, 16)\n",
    "        self.gn8 = nn.GroupNorm(4,8)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        #x = self.upsam()\n",
    "        \n",
    "        x1 = self.inc(x)\n",
    "        #x1 = self.gn1(x1)\n",
    "       \n",
    "        x2 = self.down1(x1)\n",
    "        #x2 = self.gn2(x2)\n",
    "       \n",
    "        x3 = self.down2(x2)\n",
    "        #x3 = self.gn3(x3)\n",
    "       \n",
    "        x4 = self.down3(x3)\n",
    "        #x4 = self.gn4(x4)\n",
    "       \n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        #x5 = torch.squeeze(x5)\n",
    "        #x5 = self.down5(x5)\n",
    "        #x5 = self.down6(x5)\n",
    "        \n",
    "        #print('x5:',x5.shape)\n",
    "        \n",
    "        ############### first attention layer for size 128,32,32 ---part 1 ######################\n",
    "        xlst = x5.reshape([4,128,1024])\n",
    "        \n",
    "\n",
    "        lstm = nn.LSTM(1024,1024,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "                \n",
    "        #print('xlst',xlst.shape)    \n",
    "        \n",
    "        xlst = self.lsgn1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "        \n",
    "        #print(f.shape)\n",
    "        \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,128,1024))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder1.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgn2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,128,32,32])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        x5 = self.lsgn1(x5)\n",
    "        ############### second attention layer for size 256,16,16 ---part 2 ######################\n",
    "        x5 = self.down5(x5)\n",
    "        xlst = x5.reshape([4,256,256])\n",
    "        lstm = nn.LSTM(256,256,batch_first= True,bidirectional=True,num_layers=1).cuda()\n",
    "        \n",
    "        xlst = self.lsgnp1(xlst)\n",
    "        \n",
    "        ylst = lstm(xlst)\n",
    "        \n",
    "        \n",
    "        #print(hidden)\n",
    "        \n",
    "        f = np.asarray(ylst)\n",
    "    \n",
    "        h  = torch.cuda.FloatTensor(ylst[0])\n",
    "        h = torch.squeeze(h)\n",
    "        \n",
    "        encoder_o = f[0]\n",
    "        \n",
    "        a = np.zeros((4,256,256))\n",
    "\n",
    "        a = torch.from_numpy(a)\n",
    "        a.cuda()\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            oo,b = attn_decoder2.forward(xlst,h[i],encoder_o[i])\n",
    "            oo = self.lsgnp2(oo)\n",
    "            a[i] = oo\n",
    "        \n",
    "            \n",
    "        a = a.unsqueeze(0)\n",
    "        a = a.reshape([4,256,16,16])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x5 = a  \n",
    "        x5 = x5.cuda()\n",
    "        \n",
    "        \n",
    "        x5 = x5.type(torch.cuda.FloatTensor)\n",
    "        x5 = self.lsgnp2(x5)\n",
    "        \n",
    "        x5 = self.ups4(x5)\n",
    "        \n",
    "        #x5 = self.ups4(x5)\n",
    "    \n",
    "        x = self.up1(x5, x4)\n",
    "        #x = self.gn5(x)\n",
    "        \n",
    "        x = self.up2(x, x3)\n",
    "        #x = self.gn6(x)\n",
    "       \n",
    "        x = self.up3(x, x2)\n",
    "        #x = self.gn7(x)\n",
    "       \n",
    "        x = self.up4(x, x1)\n",
    "        #x = self.gn7(x)\n",
    "\n",
    "        #x = self.downnew(x)\n",
    "        \n",
    "        #out    = self.out_softmax(logits)\n",
    "        \n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class AttnDecoderRNN_2(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=256, bilinear=True):\n",
    "        super(AttnDecoderRNN_2, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size*2, self.max_length)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(128,256)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        embedded =self.lsgn_a(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "   #     print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "  #      print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    " #      print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output = self.lsgn_a(output)\n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "       #output = self.lsgn_a(output)\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=128, bilinear=True):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.bilinear = bilinear\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(2048, 1024)\n",
    "        \n",
    "        self.attn2 = nn.Linear(1024,128)\n",
    "        \n",
    "        self.attn_24 = nn.Linear(self.hidden_size*4, self.hidden_size*2)\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        \n",
    "        self.attn_combine_bilstm = nn.Linear(3072, 1024)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(1024, 1024)\n",
    "        self.out = nn.Linear(1024, 1024)\n",
    "       # self.hidden = nn.Parameter(torch.randn(4,256,256).cuda()),nn.Parameter(torch.randn(4,256,256).cuda())\n",
    "       \n",
    "        self.lsgn_a = nn.GroupNorm(512,1024)\n",
    "        \n",
    "        self.lsgn_in = nn.GroupNorm(64,128)\n",
    "    \n",
    "        self.down5 = Down(128,256)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "                \n",
    "        self.ups4 = nn.ConvTranspose2d(256 , 256 // 2, kernel_size=2, stride=2)\n",
    "        self.upsconv4 = DoubleConv(256,128)\n",
    "\n",
    "        self.lstm = nn.LSTM(256,256,batch_first=False,bidirectional=True,num_layers=1).cuda()\n",
    "    \n",
    "    def forward(self, input,hidden,encoder_outputs):\n",
    "        \n",
    "        h = torch.unsqueeze(hidden,0)\n",
    "        \n",
    "        embedded = input\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        embedded = self.lsgn_in(embedded)\n",
    "\n",
    "        hidden_bilstm = h[0]\n",
    "        \n",
    "        \n",
    "        hidden_bilinn =  hidden_bilstm\n",
    "        \n",
    "        \n",
    "        \n",
    "        hidden_bilinn = self.attn(hidden_bilinn)\n",
    "        \n",
    "        #print('hidden bilinn shape:',hidden_bilinn.shape)\n",
    "    \n",
    "        hidden_bilinn = self.lsgn_a(hidden_bilinn)\n",
    "        #print(hidden_bilinn.shape)\n",
    "        \n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden_bilinn), 1)), dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attn_weights  = self.lsgn_a(attn_weights)\n",
    "        \n",
    "        attn_weights  = self.attn2(attn_weights)\n",
    "        \n",
    "        attn_weights = self.lsgn_in(attn_weights)\n",
    "        \n",
    "        #print(attn_weights.unsqueeze(0).shape,encoder_outputs.unsqueeze(0).shape)\n",
    "    \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        #print('attn_applied: encoder outputs',attn_applied[0].shape,encoder_outputs[0].shape)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        #print('The output shape is : ',output.shape)\n",
    "        \n",
    "        output = self.attn_combine_bilstm(output).unsqueeze(0)\n",
    "        \n",
    "        #print('The output shape after is : ',output.shape)\n",
    "        \n",
    "    \n",
    "        hidden_bi = hidden_bilinn.unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        \n",
    "        output = self.lsgn_in(output)\n",
    "        \n",
    "        #print(\"output and hidden before lstm \",output.shape,hidden_bi.shape)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden_bi)\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        output = self.lsgn_a(output)\n",
    "        \n",
    "\n",
    "    \n",
    "        return output,hidden\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(4, 256, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx          = trainx_l[train_size:]\n",
    "valy          = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx     = valx1\n",
    "valy     = valy1\n",
    "testx    = testx1\n",
    "testy    = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256,dropout_p=0.85)\n",
    "attn_decoder1.cuda()\n",
    "\n",
    "attn_decoder2 = AttnDecoderRNN_2(256, 256,dropout_p=0.85)\n",
    "attn_decoder2.cuda()\n",
    "\n",
    "model_student = UNetDoubleSmallGroupNormdifferent(1,1)\n",
    "model_student.cuda()\n",
    "\n",
    "\n",
    "optimizer_student    = optim.Adam(model_student.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "optimizer_attn_w    = optim.Adam(attn_decoder1.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "optimizer_attn_w_2  = optim.Adam(attn_decoder2.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "\n",
    "val_loss_array   = []\n",
    "train_loss_array = []\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "              \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        optimizer_attn_w_2.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "           \n",
    "        loss_array.append(loss.item())\n",
    "\n",
    "    #    torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "        \n",
    "        optimizer_attn_w_2.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "total_epochs       = 1000\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    index  = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[index]\n",
    "    trainy = trainy[index]\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    #print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    #print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    #model_save_name = \"ipmi-attentionlstm-covid19\"\n",
    "    model_save_name = \"tmi-compare-lstm\"\n",
    "    \n",
    "    #if np.mean(val_dice1) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice1)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-double-studentmodel.pt')\n",
    "    torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-double-attention.pt')\n",
    "    #attn_decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     200,
     233,
     256,
     284,
     296,
     327,
     393,
     466,
     503,
     521,
     563,
     617
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using SU-Net Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "#index    = np.random.permutation(trainx_l.shape[0])\n",
    "#trainx_l = trainx_l[index]\n",
    "#trainy_l = trainy_l[index]\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = trainx_l, trainy_l\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 30\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    pred      = get_prediction(model_student, valx)\n",
    "    val_dice1 = evaluate_result_new(pred, valy)\n",
    "    print(pred.shape, len(val_dice1), valy.shape)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    print(pred.shape, len(student_dice2), testy.shape)\n",
    "    \n",
    "    #val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    #student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice1))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    model_save_name = \"ipmi-sunet-covid19\"\n",
    "    \n",
    "    if np.mean(val_dice1) > prev_max:\n",
    "        print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice1), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "        prev_max     = np.mean(val_dice1)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-6.pt')\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     213,
     236,
     264,
     276,
     307,
     373,
     446,
     483,
     501,
     543,
     597
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Semi-supervised training SU-Net Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, datax):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(datax)//batch_size):\n",
    "        x = datax[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "unlabelledx_l = np.load(basepath+'unlabelled_x.npy')/255.0\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "unlabelledx1    = np.zeros([unlabelledx_l.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "for i in range(unlabelledx1.shape[0]):\n",
    "    unlabelledx1[i, 0] = scipy.ndimage.zoom(unlabelledx_l[i], 2, order=3)\n",
    "    #testy1[i, 0] = scipy.ndimage.zoom(unlabelledy1[i], 2, order=0)\n",
    "\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "model_student.load_state_dict(p1)\n",
    "\n",
    "\n",
    "unlabelledy1 = get_prediction(model_student, unlabelledx1)\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape, unlabelledy1.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = SUNet(1, 1)\n",
    "model_student.cuda()\n",
    "p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "model_student.load_state_dict(p1)\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "#criterion          = nn.MSELoss()\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "#trainy[trainy > 0.5] = 1\n",
    "#trainy[trainy < 0.5] = 0\n",
    "total_epochs     = 1000\n",
    "#trainx, trainy   = sort_data(trainx, trainy)\n",
    "\n",
    "trainx = trainx#[800:]\n",
    "trainy = trainy#[800:]\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "index        = np.random.permutation(np.arange(len(unlabelledx1)))\n",
    "unlabelledx1 = unlabelledx1[index]\n",
    "unlabelledy1 = unlabelledy1[index]\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    # Re-generate pseudo labels again\n",
    "    if epoch%5 == 0:\n",
    "        step_size        = len(unlabelledx1)//20-1\n",
    "        inputx           = unlabelledx1#[0:(1+epoch%20)*step_size]\n",
    "        unlabelledy1     = get_prediction(model_student, inputx)\n",
    "        #unlabelledy1[unlabelledy1 < 0.5] = 0\n",
    "        #unlabelledy1[unlabelledy1 > 0.5] = 1\n",
    "        trainx_l1        = inputx#np.concatenate([trainx_l, inputx], axis=0)\n",
    "        trainy_l1        = unlabelledy1#np.concatenate([trainy_l, unlabelledy1], axis=0)\n",
    "        trainx, trainy   = sort_data(trainx_l1, trainy_l1)\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    \n",
    "    model_save_name = \"tmi-compare-sunet-covid19-semi\"\n",
    "    \n",
    "    #if np.mean(val_dice) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     59,
     90,
     114,
     145,
     155,
     179,
     199,
     232,
     255,
     283,
     295,
     326,
     392,
     465,
     502,
     520,
     562,
     616
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Semi-supervised training LSTM Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/home/yu-hao/SEMISUNET/Dataset/'\n",
    "basepath_models  = '/home/yu-hao/SEMISUNET/Dataset/models/'\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, datax):\n",
    "    output_array   = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(datax)//batch_size):\n",
    "        x = datax[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def evaluate_result_new(pred, valy):\n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        output = pred[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y      = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        \n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            t1 = scipy.ndimage.zoom(output[0, 0].astype('uint8'), 0.6875, order=0)\n",
    "            t2 = scipy.ndimage.zoom(y[0, 0].astype('uint8'),      0.6875, order=0)\n",
    "            #print(t1.shape, t2.shape)\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    \n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 45\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "unlabelledx_l = np.load(basepath+'unlabelled_x.npy')/255.0\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "unlabelledx1    = np.zeros([unlabelledx_l.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "for i in range(unlabelledx1.shape[0]):\n",
    "    unlabelledx1[i, 0] = scipy.ndimage.zoom(unlabelledx_l[i], 2, order=3)\n",
    "    #testy1[i, 0] = scipy.ndimage.zoom(unlabelledy1[i], 2, order=0)\n",
    "\n",
    "model_save_name = \"tmi-compare-lstm\"\n",
    "\n",
    "attn_decoder1 = AttnDecoderRNN(256, 256, dropout_p=0.45)\n",
    "attn_decoder1.cuda()\n",
    "p1         = torch.load(basepath_models+model_save_name+'-attention.pt')\n",
    "attn_decoder1.load_state_dict(p1)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_teacher   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_teacher.cuda()\n",
    "p1         = torch.load(basepath_models+model_save_name+'-studentmodel.pt')\n",
    "model_teacher.load_state_dict(p1)\n",
    "\n",
    "\n",
    "model_student   = UNetDoubleSmallGroupNormdifferent(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "#model_student   = SUNet(1, 1)\n",
    "#model_student.cuda()\n",
    "#p1         = torch.load(basepath_models+model_save_name+'-studentmodel.pt')\n",
    "#model_student.load_state_dict(p1)\n",
    "\n",
    "\n",
    "unlabelledy1 = get_prediction(model_student, unlabelledx1)\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape, unlabelledy1.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "#model_student   = SUNet(1, 1)\n",
    "#model_student.cuda()\n",
    "#p1         = torch.load(basepath_models+\"tmi-compare-sunet-covid19-30.pt\")\n",
    "#model_student.load_state_dict(p1)\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.00001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "#criterion          = nn.MSELoss()\n",
    "\n",
    "optimizer_attn_w  = optim.Adam(attn_decoder1.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "#trainy[trainy > 0.5] = 1\n",
    "#trainy[trainy < 0.5] = 0\n",
    "total_epochs     = 1000\n",
    "#trainx, trainy   = sort_data(trainx, trainy)\n",
    "\n",
    "trainx = trainx#[800:]\n",
    "trainy = trainy#[800:]\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "index        = np.random.permutation(np.arange(len(unlabelledx1)))\n",
    "unlabelledx1 = unlabelledx1[index]\n",
    "unlabelledy1 = unlabelledy1[index]\n",
    "\n",
    "pred          = get_prediction(model_teacher, testx)\n",
    "student_dice2 = evaluate_result_new(pred, testy)\n",
    "\n",
    "print('Student Dice in beginning is ', np.mean(student_dice2))\n",
    "    \n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    \n",
    "    # Re-generate pseudo labels again\n",
    "    #if epoch%5 == 0:\n",
    "    step_size        = len(unlabelledx1)//20-1\n",
    "    inputx           = unlabelledx1#[0:(1+epoch%20)*step_size]\n",
    "    unlabelledy1     = get_prediction(model_teacher, inputx)\n",
    "    unlabelledy1[unlabelledy1 < 0.5] = 0\n",
    "    unlabelledy1[unlabelledy1 > 0.5] = 1\n",
    "    trainx_l1        = inputx#np.concatenate([trainx_l, inputx], axis=0)\n",
    "    trainy_l1        = unlabelledy1#np.concatenate([trainy_l, unlabelledy1], axis=0)\n",
    "    index = np.random.permutation(np.arange(len(trainx_l1)))\n",
    "    trainx_l1 = trainx_l1[index]\n",
    "    trainy_l1 = trainy_l1[index]\n",
    "    #trainx, trainy   = sort_data(trainx_l1, trainy_l1)\n",
    "    \n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model2(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    \n",
    "    pred          = get_prediction(model_student, testx)\n",
    "    student_dice2 = evaluate_result_new(pred, testy)\n",
    "    \n",
    "    print('Student length is ', len(student_dice2))\n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "    \n",
    "    \n",
    "    model_save_name = \"tmi-compare-lstm-semi\"\n",
    "    \n",
    "    if np.mean(val_dice) > prev_max:\n",
    "        print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "        prev_max     = np.mean(val_dice)\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+'-modelstudent.pt')\n",
    "        torch.save(attn_decoder1.state_dict(), basepath_models+model_save_name+'-attention.pt')\n",
    "    \n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     44,
     51,
     60,
     91,
     115,
     146,
     156,
     180,
     213,
     236,
     264,
     276,
     307,
     373,
     446,
     483,
     501,
     543,
     597
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training different models for comparison on COVID-19 dataset using SU-Net  Model\n",
    "\n",
    "import skimage\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID19/COVID-SemiSeg/Dataset/models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = F.sigmoid(pred)\n",
    "    \n",
    "    pred   = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        #output[output > 0.5]= 1\n",
    "        #output[output < 0.5]= 0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_predictions(models, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    for i in range(5):\n",
    "        models[i].eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "        outputs = []\n",
    "        for k in range(5):\n",
    "            output = models[k].forward(x)\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.data.cpu().numpy()\n",
    "            outputs.append(output)\n",
    "        \n",
    "        output_sum = np.zeros(outputs[0].shape, dtype='float16')\n",
    "        for k in range(5):\n",
    "            output_sum = output_sum+outputs[k]\n",
    "        output_sum = output_sum/5.0\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output_sum[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array)\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def get_filtered(valx, valy):\n",
    "    valxf = []\n",
    "    valyf = []\n",
    "    \n",
    "    for i in range(valx.shape[0]):\n",
    "        if np.count_nonzero(valy[i]) > 0:\n",
    "            valxf.append(valx[i])\n",
    "            valyf.append(valy[i])\n",
    "    return np.array(valxf), np.array(valyf)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 4\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "    \n",
    "    idx    = np.random.permutation(trainx.shape[0])\n",
    "    trainx = trainx[idx]\n",
    "    trainy = trainy[idx]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def prepare_batch(batch_size, k_means, trainx_l, trainy_l, h):\n",
    "    a = []\n",
    "    b = []\n",
    "    \n",
    "    for i in range(int(batch_size/2)):\n",
    "        idx = random.randint(0, trainx_l.shape[0]-1)\n",
    "        c   = k_means.predict(np.reshape(trainx_l[idx].astype('float32'), [1, 512*512]))[0]\n",
    "        \n",
    "        a.append(trainx_l[idx])\n",
    "        b.append(trainy_l[idx])\n",
    "        \n",
    "        idx = random.randint(0, len(h[c])-1)\n",
    "        t1  = np.expand_dims(np.load(h[c][idx]), 0)\n",
    "        t2  = np.expand_dims(np.load(h[c][idx].replace('-x', '-y')), 0)\n",
    "        \n",
    "        a.append(t1)\n",
    "        b.append(t2)\n",
    "   \n",
    "    a1 = np.array(a).astype('float16')\n",
    "    b1 = np.array(b).astype('float16')\n",
    "   \n",
    "    return a1, b1\n",
    "\n",
    "def store_cluster_slices(model_teacher, k_means, version):\n",
    "    epoch_array = np.arange(79)\n",
    "    all_labels  = []\n",
    "    step_size   = 10 \n",
    "    count       = 0\n",
    "    \n",
    "    for epoch in epoch_array:\n",
    "        temp_index               = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "        trainx, trainx_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "        trainy                   = get_prediction(model_teacher, trainx)\n",
    "        \n",
    "        #trainy = np.load('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/PREDICTION-NUMPY/'+str(epoch)+'.npy')\n",
    "        trainy = np.reshape(trainy, [trainy.shape[0], 512*512])\n",
    "        #print(epoch, trainy.shape, trainx.shape)\n",
    "        \n",
    "        l1     = k_means.predict(trainy)\n",
    "        \n",
    "        for jt, t in enumerate(l1):\n",
    "            temp  = np.reshape(trainy[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-y.npy', temp)\n",
    "            \n",
    "            temp  = np.reshape(trainx[jt], [512, 512]).astype('float16')\n",
    "            np.save('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/'+str(t)+'-'+str(count)+'-x.npy', temp)\n",
    "            \n",
    "            count = count+1\n",
    "    \n",
    "    return\n",
    "\n",
    "def prepare_hash(version):\n",
    "    all_cluster_files = glob.glob('/media/pranjal/BackupPlus/SIEMENS/SIEMENS/CLUSTER-NUMPY-'+str(version)+'/*.npy')\n",
    "    print('Version ', version, 'File name counts ', len(all_cluster_files))\n",
    "    filename_hash = {}\n",
    "    for i in range(50):\n",
    "        filename_hash[i] = []\n",
    "\n",
    "    for t in all_cluster_files:\n",
    "        filename_hash[int(t.split('/')[-1].split('-')[0])].append(t)\n",
    "    \n",
    "    return filename_hash\n",
    "\n",
    "def get_all_covid_lesions(valx, valy, lesion_size):\n",
    "    lesion_shapes_x = []\n",
    "    lesion_shapes_y = []\n",
    "    \n",
    "    for i in range(valy.shape[0]):\n",
    "        tx           = valx[i, 0]\n",
    "        blobs        = valy[i, 0]\n",
    "        blobs_labels = skimage.measure.label(blobs, background=0)\n",
    "        propsa       = skimage.measure.regionprops(blobs_labels)\n",
    "        \n",
    "        for k in range(len(propsa)):\n",
    "            temp = (blobs_labels == propsa[k].label).astype('uint8')\n",
    "            \n",
    "            temp_size = np.count_nonzero(temp.flatten().astype('uint8'))\n",
    "            if temp_size < lesion_size and temp_size > 5:\n",
    "                slice_x, slice_y = ndimage.find_objects(temp == 1)[0]\n",
    "                \n",
    "                roi_y = 1-temp[slice_x, slice_y]\n",
    "                roi_x = tx[slice_x, slice_y]*temp[slice_x, slice_y]\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x)\n",
    "                lesion_shapes_y.append(roi_y)\n",
    "                \n",
    "                lesion_shapes_x.append(roi_x.T)\n",
    "                lesion_shapes_y.append(roi_y.T)\n",
    "                \n",
    "                lesion_shapes_x.append(np.rot90(roi_x, 180))\n",
    "                lesion_shapes_y.append(np.rot90(roi_y, 180))\n",
    "    \n",
    "    return lesion_shapes_x, lesion_shapes_y\n",
    "\n",
    "def get_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            i,j = np.nonzero(mask[t])\n",
    "            \n",
    "            index = random.randint(0, len(i)-1)\n",
    "            \n",
    "            i = i[index]\n",
    "            j = j[index]\n",
    "            \n",
    "            lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "            \n",
    "            lesion_x     = lesion_shapes_x[lesion_index]\n",
    "            lesion_y     = lesion_shapes_y[lesion_index]\n",
    "            \n",
    "            sx     = int(lesion_x.shape[0]/2)\n",
    "            sy     = int(lesion_x.shape[1]/2)\n",
    "            \n",
    "            if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                m1         = m1*mask[t]\n",
    "                m1[m1 > 0] = 1\n",
    "\n",
    "                x_array.append(np.expand_dims(st,          axis=0))\n",
    "                x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "\n",
    "                count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_multiple_augmented_slice(batch_size, read_ids, lesion_shapes_x, lesion_shapes_y):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    index   = random.randint(0, len(read_ids)-1)\n",
    "    #print(read_ids[index])\n",
    "    \n",
    "    p       = read_ids[index].split('_')[0]\n",
    "    types   = 'CT-1'#read_ids[index].split('_')[1]\n",
    "    count   = 0\n",
    "    \n",
    "    name     = basepath+'studies/'+types+'/'\n",
    "    maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "    volname  = name+'study_'+p+'.nii.gz'\n",
    "    \n",
    "    segmentation_mask = basepath+'masks/'\n",
    "    segmentation_mask = segmentation_mask+'study_'+p+'_mask.nii.gz'\n",
    "    \n",
    "    mask     = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "    vol      = (sitk.GetArrayFromImage(sitk.ReadImage(volname))+1024.0)/1024.0\n",
    "    segmentation_mask = sitk.GetArrayFromImage(sitk.ReadImage(segmentation_mask))\n",
    "    \n",
    "    mask[mask > 0] = 1\n",
    "    count          = 0\n",
    "    \n",
    "    while(count < batch_size):\n",
    "        t     = np.random.randint(0, mask.shape[0]-1)\n",
    "        temp  = np.count_nonzero(mask[t].flatten())\n",
    "        \n",
    "        # Check if lung region is present\n",
    "        if temp > 0:\n",
    "            st  = vol[t]\n",
    "            #segmen\n",
    "            ipl, jpl = np.nonzero(mask[t])\n",
    "            \n",
    "            lesion_count = random.randint(0, 5)\n",
    "            temp_count   = 0\n",
    "            \n",
    "            while(temp_count < lesion_count):\n",
    "                index = random.randint(0, len(ipl)-1)\n",
    "\n",
    "                i = ipl[index]\n",
    "                j = jpl[index]\n",
    "\n",
    "                lesion_index = random.randint(0, len(lesion_shapes_x)-1)\n",
    "\n",
    "                lesion_x     = lesion_shapes_x[lesion_index]\n",
    "                lesion_y     = lesion_shapes_y[lesion_index]\n",
    "\n",
    "                sx     = int(lesion_x.shape[0]/2)\n",
    "                sy     = int(lesion_x.shape[1]/2)\n",
    "\n",
    "                if st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy].shape == lesion_x.shape:\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_y*st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "                    st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  =  lesion_x + st[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]\n",
    "\n",
    "                    m1 = segmentation_mask[t]#np.zeros(st.shape)\n",
    "                    m1[i-sx:i+lesion_x.shape[0]-sx, j-sy:j+lesion_x.shape[1]-sy]  += 1-lesion_y\n",
    "                    m1         = m1*mask[t]\n",
    "                    m1[m1 > 0] = 1\n",
    "                    segmentation_mask[t] = m1\n",
    "                    temp_count           = temp_count + 1\n",
    "            \n",
    "            x_array.append(np.expand_dims(st,          axis=0))\n",
    "            x_array_lungmask.append(np.expand_dims(m1, axis=0))\n",
    "            \n",
    "            count = count+1\n",
    "\n",
    "    x_array          = np.array(x_array)\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def plot_figure_slope(model_save_name):\n",
    "    N = 2\n",
    "    a = val_dice_array1#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = train_dice_array1#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = test_dice_array1#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    temp  = 0\n",
    "    slope = 0\n",
    "    #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "    for i in range(1, len(a)):\n",
    "        if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "            temp  = i#np.argmax(a)\n",
    "            slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "            #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "\n",
    "def plot_figure(model_save_name):\n",
    "    a = list(val_dice_array)#np.convolve(val_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    b = list(train_dice_array)#np.convolve(train_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    c = list(test_dice_array)#np.convolve(test_dice_array1, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "    #a.insert(0, 0)\n",
    "    #b.insert(0, 0)\n",
    "    #c.insert(0, 0)\n",
    "#     temp  = 0\n",
    "#     slope = 0\n",
    "#     #np.abs(np.abs(b[i]-b[i-1])-np.abs(a[i]-a[i-1])) < 0.1 and\n",
    "#     for i in range(1, len(a)):\n",
    "#         if b[i] >= b[i-1] and a[i] >= a[i-1]:\n",
    "#             temp  = i#np.argmax(a)\n",
    "#             slope = b[i]-b[i-1]-(a[i]-a[i-1])\n",
    "#             #print(i, slope, np.abs(b[i]-b[i-1]), np.abs(a[i]-a[i-1]), b[i], b[i-1])\n",
    "    \n",
    "    # Take arg max for semi model\n",
    "    temp = np.argmax(a)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.plot(c)\n",
    "    plt.ylabel('some numbers')\n",
    "    plt.annotate('Index '+str(temp), xy=(0.75, 0.25), xycoords='axes fraction')\n",
    "    plt.annotate('Train '+str(round(b[temp], 3)), xy=(0.75, 0.20), xycoords='axes fraction')\n",
    "    plt.annotate('Val   '+str(round(a[temp], 3)), xy=(0.75, 0.15), xycoords='axes fraction')\n",
    "    plt.annotate('Test  '+str(round(c[temp], 3)), xy=(0.75, 0.10), xycoords='axes fraction')\n",
    "    #plt.annotate('Slope '+str(round(slope, 3)),   xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "    #plt.text(6, 0, )\n",
    "    #plt.text(6, 0.1, 'Val   '+str(round(a[temp], 3)))\n",
    "    #plt.text(6, 0.2, 'Train '+str(round(b[temp], 3)))\n",
    "    #plt.text(6, 0.3, 'Test  '+str(round(c[temp], 3)))\n",
    "    \n",
    "    plt.savefig(model_save_name+\".png\")\n",
    "    \n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    \n",
    "    return\n",
    "\n",
    "def train_model1(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "        #x2 = model.forward(x)        \n",
    "        #print(x2.shape)\n",
    "        \n",
    "#         lstm = nn.LSTM(512*512,512*512,batchfirst=True)\n",
    "#         hidden = (torch.randn(1, 512, 512), torch.randn(1, 512, 512))\n",
    "#         outlstm = lstm(x, hidden)\n",
    "#         n = np.asarray(outlstm)\n",
    "  \n",
    "        print(i, x.shape[0])\n",
    "        \n",
    "        if(x.shape[0]!= 4):\n",
    "            break\n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def train_model2(model, batch_size, optimizer, criterion, trainx, trainy, augment=False):\n",
    "    #batch_size = 4\n",
    "    loss_array = []\n",
    "   \n",
    "    model.train()\n",
    "    #print(len(trainx)//batch_size)\n",
    "   \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, 0, :, :]\n",
    "        \n",
    "                \n",
    "        if augment:\n",
    "            for k in range(x.shape[0]):\n",
    "                rotv = random.randint(0, 3)\n",
    "                x[k, 0, :, :] = np.rot90(x[k, 0, :, :], rotv)\n",
    "                y[k, 0, :, :] = np.rot90(y[k, 0, :, :], rotv)\n",
    "       \n",
    "  \n",
    "        if(x.shape[0]!=4):\n",
    "            break\n",
    "            \n",
    "    \n",
    "        x = np.expand_dims(x, 1)\n",
    "        \n",
    "\n",
    "        y = np.expand_dims(y, 1)\n",
    "\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        \n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_attn_w.zero_grad()\n",
    "        \n",
    "        output = model.forward(x)\n",
    "        #print(i,attn_weights[1])\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "       \n",
    "        loss_array.append(loss.item())\n",
    "        \n",
    "       # torch.nn.utils.clip_grad_norm(attn_decoder1.parameters(),0.7)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_attn_w.step()\n",
    "   \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "# train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "# val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "# test_ids       = np.load(basepath+'TEST.npy')\n",
    "# unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "# nocovid_ids    = np.load(basepath+'NOCOVID.npy')\n",
    "\n",
    "\n",
    "# unlabelled_ids     = unlabelled_ids\n",
    "# train_ids          = train_ids[:4]\n",
    "# val_ids            = val_ids\n",
    "# test_ids           = test_ids\n",
    "\n",
    "trainx_l = np.load(basepath+'train_x.npy')/255.0\n",
    "trainy_l = np.load(basepath+'train_y.npy')\n",
    "trainy_l[trainy_l > 0] = 1\n",
    "\n",
    "train_size    = 40\n",
    "valx = trainx_l[train_size:]\n",
    "valy = trainy_l[train_size:]\n",
    "\n",
    "trainx_l = trainx_l[:train_size]\n",
    "trainy_l = trainy_l[:train_size]\n",
    "\n",
    "testx = np.load(basepath+'test_x.npy')/255.0\n",
    "testy = np.load(basepath+'test_y.npy')\n",
    "testy[testy > 0] = 1\n",
    "\n",
    "\n",
    "trainx_l1 = np.zeros([trainx_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valx1     = np.zeros([valx.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testx1    = np.zeros([testx.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "trainy_l1 = np.zeros([trainy_l.shape[0], 1, 512, 512], dtype='float16')\n",
    "valy1     = np.zeros([valy.shape[0], 1, 512, 512],     dtype='float16')\n",
    "testy1    = np.zeros([testy.shape[0], 1, 512, 512],    dtype='float16')\n",
    "\n",
    "\n",
    "for i in range(trainx_l.shape[0]):\n",
    "    trainx_l1[i, 0] = scipy.ndimage.zoom(trainx_l[i], 2, order=3)\n",
    "    trainy_l1[i, 0] = scipy.ndimage.zoom(trainy_l[i], 2, order=0)\n",
    "\n",
    "for i in range(valx.shape[0]):\n",
    "    valx1[i, 0] = scipy.ndimage.zoom(valx[i], 2, order=3)\n",
    "    valy1[i, 0] = scipy.ndimage.zoom(valy[i], 2, order=0)\n",
    "\n",
    "for i in range(testx.shape[0]):\n",
    "    testx1[i, 0] = scipy.ndimage.zoom(testx[i], 2, order=3)\n",
    "    testy1[i, 0] = scipy.ndimage.zoom(testy[i], 2, order=0)\n",
    "\n",
    "\n",
    "trainx_l = trainx_l1\n",
    "trainy_l = trainy_l1\n",
    "valx = valx1\n",
    "valy = valy1\n",
    "testx = testx1\n",
    "testy = testy1\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        #nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "prev_max        = -1000\n",
    "model_student   = UNetDoubleSmallWithoutGN(1, 1)\n",
    "model_student.cuda()\n",
    "\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0001)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_dice_array   = []\n",
    "train_dice_array = []\n",
    "test_dice_array  = []\n",
    "\n",
    "trainx, trainy   = sort_data(trainx_l, trainy_l)\n",
    "total_epochs = 1000\n",
    "\n",
    "\n",
    "# trainx = np.expand_dims(trainx, axis=1)\n",
    "# trainy = np.expand_dims(trainy, axis=1)\n",
    "\n",
    "# valx   = np.expand_dims(valx, axis=1)\n",
    "# valy   = np.expand_dims(valy, axis=1)\n",
    "\n",
    "# testx  = np.expand_dims(testx, axis=1)\n",
    "# testy  = np.expand_dims(testy, axis=1)\n",
    "\n",
    "teacher_dice_array = []\n",
    "test_dice_array    = []\n",
    "\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    if epoch%10 ==1:\n",
    "        print(epoch)\n",
    "    #train_model1(model, optimizer, criterion, trainx, trainy, augment=False)\n",
    "    train_loss    = train_model(model_student, 4, optimizer_student, criterion, trainx, trainy, False)\n",
    "    #train_loss    = train_model(model_student, 2, optimizer_student, criterion, trainx, trainy, False)\n",
    "    \n",
    "    val_dice      = evaluate_result(model_student, valx,   valy)\n",
    "    student_dice1 = evaluate_result(model_student, trainx, trainy)\n",
    "    student_dice2 = evaluate_result(model_student, testx,  testy)\n",
    "    \n",
    "    \n",
    "    train_dice_array.append(np.mean(student_dice1))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    test_dice_array.append(np.mean(student_dice2))\n",
    "\n",
    "    model_save_name = \"tmi-compare-unet-covid19\"\n",
    "    \n",
    "    #if np.mean(val_dice) > prev_max:\n",
    "    print(\"Step %d  Val Dice %.5f, Train Dice %f, Test Dice %f\" % (epoch, np.mean(val_dice), np.mean(student_dice1), np.mean(student_dice2)))\n",
    "    prev_max     = np.mean(val_dice)\n",
    "    torch.save(model_student.state_dict(), basepath_models+model_save_name+'-'+str(epoch)+\".pt\")\n",
    "\n",
    "    #np.save(model_save_name+'_train.npy',      train_dice_array)\n",
    "    #np.save(model_save_name+'_validation.npy', val_dice_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     21,
     43,
     50,
     81,
     103,
     127,
     151
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For training the Group Norm using un-labelled data\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "basepath         = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/'\n",
    "basepath_models  = '/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/COVID_MOSCOW/COVID_MOSCOW/COVID19_1110/models/single_models/'\n",
    "\n",
    "\n",
    "def read_training_data(read_ids):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    \n",
    "    for p in read_ids:\n",
    "        name = basepath+'masks/'\n",
    "        name = name+'study_'+p+'_mask.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(name))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(name.replace('_mask.nii.gz', '.nii.gz').replace('masks', 'studies/CT-1')))\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            temp  = np.count_nonzero(mask[t].flatten())\n",
    "            if temp > 0:\n",
    "                x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                y_array.append(np.expand_dims(mask[t], axis=0))\n",
    "\n",
    "    x_array = (np.array(x_array)+1024.0)/1024.0\n",
    "    y_array = np.array(y_array)\n",
    "    \n",
    "    return x_array, y_array\n",
    "\n",
    "def dice(im1, im2):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "    return 2. * intersection.sum() / (im1.sum() + im2.sum()+0.00001)\n",
    "\n",
    "def read_training_data_unlabelled(read_ids):\n",
    "    x_array          = []\n",
    "    x_array_lungmask = []\n",
    "    \n",
    "    names   = [x.split('_')[0] for x in read_ids]\n",
    "    types   = [x.split('_')[1] for x in read_ids]\n",
    "    count   = 0\n",
    "    \n",
    "    for p in names:\n",
    "        name     = basepath+'studies/'+types[count]+'/'\n",
    "        maskname = name+'study_'+p+'_mask.nii.gz'\n",
    "        volname  = name+'study_'+p+'.nii.gz'\n",
    "        \n",
    "        mask = sitk.GetArrayFromImage(sitk.ReadImage(maskname))\n",
    "        vol  = sitk.GetArrayFromImage(sitk.ReadImage(volname))\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        for t in range(mask.shape[0]):\n",
    "            if True:#t % 1 == 0:\n",
    "                temp  = np.count_nonzero(mask[t].flatten())\n",
    "                if temp > 0: # Check if lung region is present\n",
    "                    x_array.append(np.expand_dims(vol[t], axis=0))\n",
    "                    x_array_lungmask.append(np.expand_dims(mask[t], axis=0))\n",
    "        \n",
    "        count = count+1\n",
    "\n",
    "    x_array          = (np.array(x_array)+1024.0)/1024.0\n",
    "    x_array_lungmask = np.array(x_array_lungmask)\n",
    "    \n",
    "    return x_array, x_array_lungmask\n",
    "\n",
    "def get_prediction(model, valx):\n",
    "    output_array   = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.data.cpu().numpy()\n",
    "        \n",
    "        for k in range(output.shape[0]):\n",
    "            output_array.append(output[k, 0])\n",
    "    \n",
    "    output_array = np.array(output_array).astype('float16')\n",
    "    output_array = np.expand_dims(output_array, 1)\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size, :, :, :]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "\n",
    "        output = torch.sigmoid(output)        \n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        output[output < 0.5] = 0\n",
    "        output[output > 0.5] = 1\n",
    "        \n",
    "        for pk in range(output.shape[0]):\n",
    "            dt = dice(y[pk, 0, :, :], output[pk, 0, :, :])\n",
    "            val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, optimizer, criterion, trainx, trainy):\n",
    "    batch_size = 2\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)        \n",
    "        loss   = criterion(output , y)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "def sort_data(trainx1, trainy1):\n",
    "    # Sort the data\n",
    "    X = trainx1\n",
    "    Y = trainy1\n",
    "    r = [t for t in sorted(zip(Y,X), key=lambda pair: np.sum(pair[0].flatten()))]\n",
    "    \n",
    "    trainx = []\n",
    "    trainy = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        trainy.append(r[i][0])\n",
    "        trainx.append(r[i][1])\n",
    "    \n",
    "    trainx = np.array(trainx)\n",
    "    trainy = np.array(trainy)\n",
    "    \n",
    "    return trainx, trainy\n",
    "        \n",
    "train_ids      = np.load(basepath+'TRAIN.npy')\n",
    "val_ids        = np.load(basepath+'VALIDATION.npy')\n",
    "test_ids       = np.load(basepath+'TEST.npy')\n",
    "unlabelled_ids = np.load(basepath+'NOTLABELLED.npy')\n",
    "\n",
    "train_ids           = train_ids\n",
    "\n",
    "trainx_l, trainy_l = read_training_data(train_ids)\n",
    "valx, valy         = read_training_data(val_ids)\n",
    "testx, testy       = read_training_data(test_ids)\n",
    "\n",
    "print(trainx_l.shape, valx.shape, testx.shape)\n",
    "\n",
    "valx_img = sitk.GetImageFromArray(valx.astype('float32')[:, 0, :, :])\n",
    "sitk.WriteImage(valx_img, basepath+'CT-img.nii.gz')\n",
    "\n",
    "\n",
    "model_teacher = SUNet(1,1)#UNetDoubleSmall(1,1)\n",
    "model_teacher.cuda()\n",
    "\n",
    "\n",
    "p1         = torch.load(basepath_models+\"tmi-f-3-93.pt\")\n",
    "model_teacher.load_state_dict(p1)\n",
    "\n",
    "\n",
    "device             = torch.device(\"cuda:0\")\n",
    "optimizer_student  = optim.Adam(model_student.parameters(), lr=0.0005)\n",
    "criterion          = nn.BCEWithLogitsLoss(torch.ones([1]).cuda())\n",
    "\n",
    "val_loss_array   = []\n",
    "train_loss_array = []\n",
    "\n",
    "prev_max_teacher = -1000\n",
    "prev_max    = -1000\n",
    "model_count = 0\n",
    "step_size   = 20\n",
    "beta        = 0.9\n",
    "\n",
    "val_dice_t   = evaluate_result(model_teacher, valx, valy)\n",
    "print(\"Dice in the beginning \", np.mean(val_dice_t))\n",
    "\n",
    "val_dice_t = np.mean(val_dice_t)\n",
    "prev_max   = val_dice_t\n",
    "\n",
    "teacher_dice_array = []\n",
    "\n",
    "train_dice_array = []\n",
    "val_dice_array   = []\n",
    "test_dice_array  = []\n",
    "\n",
    "model_save_name = \"tmi-f-semi-sunet\"\n",
    "\n",
    "first_time = True\n",
    "\n",
    "for epoch in range(300):\n",
    "    temp_index                 = epoch%(int(len(unlabelled_ids)/step_size))\n",
    "    trainx1, trainx1_lungmask  = read_training_data_unlabelled(unlabelled_ids[temp_index*step_size:temp_index*step_size+step_size])\n",
    "    trainy1                    = get_prediction(model_teacher, trainx1)\n",
    "    \n",
    "    trainx, trainy = sort_data(trainx1, trainy1)\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, trainx.shape, trainy.shape)\n",
    "    \n",
    "    train_loss = train_model(model_student, optimizer_student, criterion, trainx, trainy)\n",
    "    \n",
    "    train_dice = evaluate_result(model_student, trainx_l, trainy_l)\n",
    "    val_dice   = evaluate_result(model_student, valx, valy)\n",
    "    \n",
    "    # Update teacher weights\n",
    "    if np.mean(val_dice) > val_dice_t:\n",
    "        print(epoch, ' Updating Teacher Weights')\n",
    "        torch.save(model_student.state_dict(), \"temp.pt\")\n",
    "        torch.save(model_student.state_dict(), basepath_models+model_save_name+\".pt\")\n",
    "        p1         = torch.load('temp.pt')\n",
    "        \n",
    "        model_teacher.load_state_dict(p1)\n",
    "        val_dice_t = np.mean(val_dice)\n",
    "    \n",
    "    train_dice_array.append(np.mean(train_dice))\n",
    "    val_dice_array.append(np.mean(val_dice))\n",
    "    \n",
    "    print(\"Step %d  Train Dice %.5f  Val Dice %.5f \" % (epoch, np.mean(train_dice), np.mean(val_dice)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For selecting the most reliable weights\n",
    "\n",
    "s  = 300\n",
    "a1 = np.zeros([s, 236])\n",
    "b1 = np.zeros([s, 126])\n",
    "\n",
    "count = 0\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "model_save_name = 'tmi-f-single-semi'\n",
    "\n",
    "def scoring_function(val_array, test_array, epoch):\n",
    "    score_array = []\n",
    "    step_size   = 5\n",
    "    alpha       = 10\n",
    "    \n",
    "    temp_array     = val_array[epoch-step_size:epoch]\n",
    "    sum_array      = 1-np.mean(temp_array, axis=0)\n",
    "    var_array      = np.std(temp_array, axis=0)\n",
    "\n",
    "    score_temp     = sum_array + alpha*var_array\n",
    "    final_score    = score_temp*val_array[epoch]\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "for epoch in range(s):\n",
    "    a = np.load('/home/yu-hao/AttentionDeepMIL/val_dice_array-'+model_save_name+'--'+str(epoch)+'.npy')\n",
    "    a1[epoch] = a\n",
    "\n",
    "score_array = []\n",
    "for epoch in range(100, 300):\n",
    "    final_score = scoring_function(a1, b1, epoch)\n",
    "    score_array.append(np.mean(final_score))\n",
    "\n",
    "index = np.argmax(score_array) \n",
    "print('Most reliable Weights Index ', index, score_array[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
