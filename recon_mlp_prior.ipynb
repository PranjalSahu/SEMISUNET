{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# All Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import time\n",
    "import pydicom\n",
    "import glob\n",
    "from numba import jit\n",
    "from skimage import filters\n",
    "import copy\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "\n",
    "#import cv2\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#from scipy.misc import imresize\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# GAN model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "#from scipy.misc import imsave\n",
    "import scipy.stats\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "import pydicom\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.filters import threshold_otsu\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "import glob\n",
    "#from scipy.misc import imread\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.measure import compare_psnr\n",
    "import numpy\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import scipy.ndimage.interpolation as ndii\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     3,
     20,
     55,
     99
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] PyTorch models for training the regularizer\n",
    "\n",
    "\n",
    "class RegCNN(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_features = 125\n",
    "        self.in_features  = 125\n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.randn(self.out_features, self.in_features))\n",
    "        self.bias   = torch.nn.Parameter(torch.randn(self.out_features, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(self.weight, x)\n",
    "        x = self.bias + x\n",
    "        x = torch.tanh(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        return(x)\n",
    "\n",
    "class RegCNNA(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features   = 125\n",
    "        self.out_features1 = 125\n",
    "        self.out_features2 = 125\n",
    "        self.out_features3 =   5\n",
    "        \n",
    "        self.weight1 = torch.nn.Parameter(torch.randn(self.out_features1, self.in_features))\n",
    "        self.bias1   = torch.nn.Parameter(torch.randn(self.out_features1, 1))\n",
    "        \n",
    "        self.weight2 = torch.nn.Parameter(torch.randn(self.out_features2, self.out_features1))\n",
    "        self.bias2   = torch.nn.Parameter(torch.randn(self.out_features2, 1))\n",
    "        \n",
    "        #self.weight3 = torch.nn.Parameter(torch.randn(self.out_features3, self.out_features2))\n",
    "        #self.bias3   = torch.nn.Parameter(torch.randn(self.out_features3, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(self.weight1, x)\n",
    "        x = self.bias1 + x\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        x = torch.matmul(self.weight2, x)\n",
    "        x = self.bias2 + x\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        #x = torch.matmul(self.weight3, x)\n",
    "        #x = self.bias3 + x\n",
    "        #x = torch.tanh(x)\n",
    "        #print(x[124])\n",
    "        \n",
    "        x = torch.mean(x, dim=0)\n",
    "        return x\n",
    "\n",
    "class RegCNNB(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features   = 125\n",
    "        self.out_features1 = 125\n",
    "        self.out_features2 = 125\n",
    "        self.out_features3 =   5\n",
    "        \n",
    "        #self.weight1 = torch.nn.Parameter(torch.randn(self.out_features1, self.in_features))\n",
    "        #self.bias1   = torch.nn.Parameter(torch.randn(self.out_features1, 1))\n",
    "        \n",
    "        #self.weight2 = torch.nn.Parameter(torch.randn(self.out_features2, self.out_features1))\n",
    "        #self.bias2   = torch.nn.Parameter(torch.randn(self.out_features2, 1))\n",
    "        \n",
    "        #self.weight3 = torch.nn.Parameter(torch.randn(self.out_features3, self.out_features2))\n",
    "        #self.bias3   = torch.nn.Parameter(torch.randn(self.out_features3, 1))\n",
    "        \n",
    "        self.linear1  = torch.nn.Linear(self.in_features,   self.out_features1, bias=True)\n",
    "        self.linear2  = torch.nn.Linear(self.out_features1, self.out_features2, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.linear1(x)\n",
    "        #x = torch.relu(x)\n",
    "        \n",
    "        #x = self.linear2(x)\n",
    "        #x = torch.relu(x)\n",
    "        \n",
    "        #x = torch.matmul(self.weight1, x)\n",
    "        #x = self.bias1 + x\n",
    "        #x = torch.tanh(x)\n",
    "        \n",
    "        #x = torch.matmul(self.weight2, x)\n",
    "        #x = self.bias2 + x\n",
    "        #x = torch.tanh(x)\n",
    "        \n",
    "        #x = torch.matmul(self.weight3, x)\n",
    "        #x = self.bias3 + x\n",
    "        #x = torch.tanh(x)\n",
    "        #print(x[124])\n",
    "        \n",
    "        #x = torch.mean(x, dim=0)\n",
    "        return x\n",
    "\n",
    "class RegCNNC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter0 = 16\n",
    "        self.filter1 = 16\n",
    "        self.filter2 = 8\n",
    "        self.filter3 = 64\n",
    "        self.filter4 = 64\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv3d(1, self.filter0, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        #self.conv_block2 = nn.Sequential(\n",
    "        #    nn.Conv3d(self.filter0, self.filter1, kernel_size=5, stride=1, padding=2),\n",
    "        #    nn.ReLU())\n",
    "        \n",
    "        #self.conv_block3 = nn.Sequential(\n",
    "        #    nn.Conv3d(self.filter1, 1, kernel_size=5, stride=1, padding=2),\n",
    "        #    nn.ReLU())\n",
    "            #nn.MaxPool3d(2, stride=2))\n",
    "        \n",
    "#         self.conv_block4 = nn.Sequential(\n",
    "#             nn.Conv3d(self.filter2, self.filter3, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool3d(2, stride=2))\n",
    "        \n",
    "#         self.conv_block5 = nn.Sequential(\n",
    "#             nn.Conv3d(self.filter3, self.filter4, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU())\n",
    "        #self.conv_block4 = nn.MaxPool3d(2, stride=2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        #print('x shape is ', x.shape)\n",
    "        #x = self.conv_block2(x)\n",
    "        #print('x shape is ', x.shape)\n",
    "        #x = self.conv_block3(x)\n",
    "        \n",
    "        out = torch.sum(x, 1)\n",
    "        out = torch.unsqueeze(out, 1)\n",
    "        return out\n",
    "\n",
    "class RegCNND(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter0 = 8\n",
    "        self.filter1 = 16\n",
    "        self.filter2 = 8\n",
    "        self.filter3 = 64\n",
    "        self.filter4 = 64\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv3d(1, self.filter0, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x   = self.conv_block1(x)\n",
    "        x   = torch.sum(x, 1)\n",
    "        \n",
    "        out = x\n",
    "        out = torch.reshape(out, [-1, 1])\n",
    "        return out\n",
    "\n",
    "# model  = RegCNND()\n",
    "# inx    = torch.tensor(np.random.rand(10, 1, 5, 5, 5).astype('float32'))\n",
    "# print(inx.shape)\n",
    "# result = model.forward(inx)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [STAR] All the paths for training the model\n",
    "\n",
    "projectionpath        = '/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/NORMAL/'\n",
    "highprojectionpath    = '/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/DOUBLE/'\n",
    "savepath              = '/media/pranjal/newdrive1/DBT-PROJ-DENOISE/'\n",
    "\n",
    "\n",
    "modelname = \"normal-to-three-huber-0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     3,
     10,
     31,
     76,
     282,
     441,
     568
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Helper functions for training the de-noising CNN\n",
    "\n",
    "\n",
    "def bbox2(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def get_crop_image(lowimg, highimg):\n",
    "    factor = 1\n",
    "    \n",
    "    #slice_index = random.randint(0, 24)\n",
    "    #lowimg  = lowimg[slice_index]\n",
    "    #highimg = highimg[slice_index]\n",
    "\n",
    "    thresh  = threshold_otsu(lowimg)\n",
    "    binary  = lowimg < thresh\n",
    "    result  = np.multiply(binary, lowimg)\n",
    "\n",
    "    rmin, rmax, cmin, cmax = bbox2(result)\n",
    "    resultlowimg = result[rmin:rmax, cmin:cmax]\n",
    "    resultlowimg = resultlowimg.astype(float)\n",
    "\n",
    "    result        = np.multiply(binary, highimg)\n",
    "    resulthighimg = result[rmin:rmax, cmin:cmax]\n",
    "    resulthighimg = resulthighimg.astype(float)\n",
    "    \n",
    "    return resultlowimg, resulthighimg\n",
    "    \n",
    "def get_train_data_proj(lowimg, highimg, epoch, batch_size):\n",
    "    highdose = []\n",
    "    lowdose  = []\n",
    "    cliplimit = 0\n",
    "    \n",
    "    imgshape = 256\n",
    "    \n",
    "    \n",
    "    # loop till samples of size batch_size with atleast 90% occupancy is not obtained\n",
    "    crop_count = 0\n",
    "    \n",
    "    rowst = lowimg.shape[0]\n",
    "    colst = lowimg.shape[1]\n",
    "    \n",
    "    while(crop_count < batch_size):\n",
    "        cropimgx = random.randint(1, rowst-imgshape)\n",
    "        cropimgy = random.randint(1, colst-imgshape)\n",
    "        \n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov = np.sum(lowimg_crop > cliplimit)\n",
    "        # atleast 90% occupancy should be there\n",
    "        if nonzerov*1.0/(imgshape*imgshape) < 0.65:\n",
    "            continue\n",
    "        \n",
    "        highimg_crop = highimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        \n",
    "        #prob = random.randint(0, 100)\n",
    "        #if prob < 50:\n",
    "        #    lowimg_crop  = np.fliplr(lowimg_crop)\n",
    "        #    highimg_crop = np.fliplr(highimg_crop)\n",
    "        \n",
    "        #lowimg_crop_mean  = np.mean(lowimg_crop.flatten())\n",
    "        #highimg_crop_mean = np.mean(highimg_crop.flatten())\n",
    "        #diff_mean         = highimg_crop_mean -  lowimg_crop_mean\n",
    "        \n",
    "        lowdose.append(lowimg_crop)\n",
    "        highdose.append(highimg_crop)\n",
    "        \n",
    "        crop_count = crop_count+1\n",
    "        \n",
    "    return np.array(lowdose), np.array(highdose)\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.linalg.norm(x - y)\n",
    "\n",
    "def test_on_whole_proj_200(lowfilename, model):\n",
    "    highdose = []\n",
    "    lowdose  = []\n",
    "    cliplimit = 0\n",
    "    imgshape  = 200\n",
    "    \n",
    "    ds             = pydicom.dcmread(lowfilename)\n",
    "    limg           = ds.pixel_array\n",
    "    original_image = copy.deepcopy(limg)\n",
    "    \n",
    "    \n",
    "    cleanimage = np.zeros(limg.shape)\n",
    "    \n",
    "    thresh      = threshold_otsu(limg)\n",
    "    binarymax   = limg > thresh\n",
    "    \n",
    "    \n",
    "    outsideimage_mask = limg > thresh\n",
    "    insideimage_mask  = limg < thresh\n",
    "    \n",
    "    restimage   = binarymax#np.multiply(binarymax, limg)\n",
    "    minvalue    = np.min(limg)\n",
    "    \n",
    "    binary  = limg <= thresh\n",
    "    limg    = limg - np.min(limg)\n",
    "    binary1 = limg >= 0\n",
    "    \n",
    "    newbinary    = np.multiply(binary, binary1)\n",
    "    result       = np.multiply(newbinary, limg)\n",
    "    resultlowimg = result\n",
    "    resultlowimg = resultlowimg.astype(float)\n",
    "    resultlowimg = resultlowimg/(thresh/2)\n",
    "    lowimg       = resultlowimg-1\n",
    "    \n",
    "    #print(np.max(lowimg), np.mean(lowimg), np.min(lowimg), thresh)\n",
    "    \n",
    "    rowst = limg.shape[0]\n",
    "    colst = limg.shape[1]\n",
    "    \n",
    "    # For top row\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(j+imgshape < colst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "#         if nonzerov*1.0/(imgshape*imgshape) < 0.9:\n",
    "#             cleanimage[i:i+imgshape, j:j+imgshape] = tempimg\n",
    "#             j = j+imgshape\n",
    "#             continue\n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = cleanimg[0, :, :, 0]\n",
    "        j = j+imgshape\n",
    "    \n",
    "    \n",
    "    # For bottom row\n",
    "    i = rowst-imgshape\n",
    "    j = 0\n",
    "    while(j+imgshape < colst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "#         if nonzerov*1.0/(imgshape*imgshape) < 0.9:\n",
    "#             cleanimage[i:i+imgshape, j:j+imgshape] = tempimg\n",
    "#             j = j+imgshape\n",
    "#             continue\n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = cleanimg[0, :, :, 0]\n",
    "        j = j+imgshape\n",
    "    \n",
    "    # For rightmost column\n",
    "    j = colst-imgshape\n",
    "    i = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "#         if nonzerov*1.0/(imgshape*imgshape) < 0.9:\n",
    "#             cleanimage[i:i+imgshape, j:j+imgshape] = tempimg\n",
    "#             i = i+imgshape\n",
    "#             continue    \n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = cleanimg[0, :, :, 0]\n",
    "        i = i+imgshape\n",
    "    \n",
    "    # For leftmost column\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "#         if nonzerov*1.0/(imgshape*imgshape) < 0.9:\n",
    "#             cleanimage[i:i+imgshape, j:j+imgshape] = tempimg\n",
    "#             i = i+imgshape\n",
    "#             continue\n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = cleanimg[0, :, :, 0]\n",
    "        i = i+imgshape\n",
    "    \n",
    "    halfvalue    = int(imgshape/2)\n",
    "    quartervalue = int(imgshape/4)#+int(imgshape/2)\n",
    "    \n",
    "    # indexes for deciding the patch location\n",
    "    i = 0 \n",
    "    j = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        # re-initialize the counter for j\n",
    "        j = 0\n",
    "        while(j+imgshape < colst):\n",
    "            cropimgx = i\n",
    "            cropimgy = j\n",
    "            \n",
    "            tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "            \n",
    "            lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "            nonzerov     = np.sum(lowimg_crop > cliplimit)\n",
    "            \n",
    "            # If it is not mostly occupied then don't process it\n",
    "#             if nonzerov*1.0/(imgshape*imgshape) < 0.5:\n",
    "#                 cleanimage[i+quartervalue:i+quartervalue+halfvalue, j+quartervalue:j+quartervalue+halfvalue] = tempimg[quartervalue:quartervalue+halfvalue, quartervalue:quartervalue+halfvalue]\n",
    "#                 j = j+halfvalue\n",
    "#                 continue\n",
    "            \n",
    "            before_mean = np.mean(tempimg)\n",
    "            cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "            after_mean  = np.mean(cleanimg)\n",
    "            cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "            \n",
    "            #print(np.mean(tempimg), np.mean(cleanimg))\n",
    "            \n",
    "            cleanimage[i+quartervalue:i+quartervalue+halfvalue, j+quartervalue:j+quartervalue+halfvalue] = cleanimg[0, :, :, 0][quartervalue:quartervalue+halfvalue, quartervalue:quartervalue+halfvalue]\n",
    "            j = j+halfvalue\n",
    "        i = i+halfvalue\n",
    "    \n",
    "    #plt.hist(cleanimage.flatten(), bins=256, range=(cliplimit, 1.0), fc='k', ec='k')\n",
    "    #plt.show()\n",
    "    \n",
    "    totalimage  = cleanimage+1\n",
    "    binarynew   = binary\n",
    "    \n",
    "    img_adapteq = np.multiply(totalimage, binarynew)\n",
    "    #print(np.max(img_adapteq), np.mean(img_adapteq), np.min(img_adapteq), thresh)\n",
    "    \n",
    "    mul_result   = img_adapteq*(thresh/2)\n",
    "    mul_result   = mul_result+minvalue\n",
    "    \n",
    "    #print('restimage max  ', np.max(restimage))\n",
    "    #print('mul_result max ', np.max(mul_result))\n",
    "    \n",
    "    tempvalue = np.min(mul_result)\n",
    "    #restimage = restimage*np.max(mul_result)\n",
    "    \n",
    "    #mul_result   = mul_result + restimage\n",
    "    #mul_result   = mul_result - tempvalue\n",
    "    \n",
    "    #print(mul_result.shape, restimage.shape)\n",
    "    \n",
    "    \n",
    "    # Adding the Code to replace the oustide breast region with original values\n",
    "    mul_result     = mul_result*insideimage_mask        # Get the inside region only\n",
    "    original_image = original_image*outsideimage_mask   # Get the outside region only\n",
    "    mul_result     = mul_result+original_image \n",
    "    \n",
    "    #print(np.max(mul_result), np.mean(mul_result), np.min(mul_result))\n",
    "    #print(np.mean(pt2), np.max(pt2), np.min(pt2))\n",
    "    mul_result   = mul_result.astype(np.uint16)\n",
    "    #print(np.mean(mul_result), np.max(mul_result), np.min(mul_result))\n",
    "    \n",
    "    ds.PixelData = mul_result.tostring()\n",
    "    \n",
    "    savename = lowfilename.split('\\\\')[-1]\n",
    "    savepath = cleanprojpath+savename\n",
    "    ds.save_as(savepath)\n",
    "    \n",
    "def test_on_whole_proj(lowfilename, model):\n",
    "    highdose = []\n",
    "    lowdose  = []\n",
    "    cliplimit = 0.\n",
    "    \n",
    "    ds     = pydicom.dcmread(lowfilename)\n",
    "    limg   = ds.pixel_array\n",
    "    \n",
    "    cleanimage = np.zeros(limg.shape)\n",
    "    \n",
    "    # get the histogram equalized image\n",
    "    thresh      = threshold_otsu(limg)\n",
    "    value_range = thresh - np.min(limg)\n",
    "    binary  = limg <= thresh\n",
    "    limg    = limg - np.min(limg)\n",
    "    binary1 = limg >= 0\n",
    "    newbinary = np.multiply(binary, binary1)\n",
    "    result    = np.multiply(newbinary, limg)\n",
    "    resultlowimg = result\n",
    "    resultlowimg = resultlowimg.astype(float)\n",
    "    resultlowimg = resultlowimg/thresh\n",
    "    resultlowimg = exposure.equalize_adapthist(resultlowimg, clip_limit=cliplimit)\n",
    "    lowimg       = resultlowimg\n",
    "    \n",
    "    rowst = limg.shape[0]\n",
    "    colst = limg.shape[1]\n",
    "    \n",
    "    # For top row\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(j+128 < colst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        nonzerov     = np.sum(lowimg_crop >= cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "        if nonzerov*1.0/(128*128) < 0.9:\n",
    "            cleanimage[i:i+128, j:j+128] = tempimg\n",
    "            j = j+128\n",
    "            continue    \n",
    "        cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        cleanimage[i:i+128, j:j+128] = cleanimg[0, :, :, 0]\n",
    "        j = j+128\n",
    "    \n",
    "    \n",
    "    # For bottom row\n",
    "    i = rowst-128\n",
    "    j = 0\n",
    "    while(j+128 < colst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        nonzerov     = np.sum(lowimg_crop >= cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "        if nonzerov*1.0/(128*128) < 0.9:\n",
    "            cleanimage[i:i+128, j:j+128] = tempimg\n",
    "            j = j+128\n",
    "            continue    \n",
    "        cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        cleanimage[i:i+128, j:j+128] = cleanimg[0, :, :, 0]\n",
    "        j = j+128\n",
    "    \n",
    "    # For rightmost column\n",
    "    j = colst-128\n",
    "    i = 0\n",
    "    while(i+128 < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        nonzerov     = np.sum(lowimg_crop >= cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "        if nonzerov*1.0/(128*128) < 0.9:\n",
    "            cleanimage[i:i+128, j:j+128] = tempimg\n",
    "            i = i+128\n",
    "            continue    \n",
    "        cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        cleanimage[i:i+128, j:j+128] = cleanimg[0, :, :, 0]\n",
    "        i = i+128\n",
    "    \n",
    "    # For leftmost column\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while(i+128 < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "        nonzerov     = np.sum(lowimg_crop >= cliplimit)    \n",
    "        # If it is not mostly occupied then don't process it\n",
    "        if nonzerov*1.0/(128*128) < 0.9:\n",
    "            cleanimage[i:i+128, j:j+128] = tempimg\n",
    "            i = i+128\n",
    "            continue    \n",
    "        cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        cleanimage[i:i+128, j:j+128] = cleanimg[0, :, :, 0]\n",
    "        i = i+128\n",
    "    \n",
    "    # indexes for deciding the patch location\n",
    "    i = 0 \n",
    "    j = 0\n",
    "    while(i+128 < rowst):\n",
    "        # re-initialize the counter for j\n",
    "        j = 0\n",
    "        while(j+128 < colst):\n",
    "            cropimgx = i\n",
    "            cropimgy = j\n",
    "                \n",
    "            tempimg      = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "            \n",
    "            lowimg_crop  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "            nonzerov     = np.sum(lowimg_crop >= cliplimit)\n",
    "            \n",
    "            # If it is not mostly occupied then don't process it\n",
    "            if nonzerov*1.0/(128*128) < 0.9:\n",
    "                cleanimage[i+64:i+128, j+64:j+128] = tempimg[32:96, 32:96]\n",
    "                j = j+64\n",
    "                continue\n",
    "            \n",
    "            cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "            cleanimage[i+64:i+128, j+64:j+128] = cleanimg[0, :, :, 0][32:96, 32:96]\n",
    "            j = j+64\n",
    "        i = i+64\n",
    "    \n",
    "    #plt.hist(cleanimage.flatten(), bins=256, range=(cliplimit, 1.0), fc='k', ec='k')\n",
    "    #plt.show()\n",
    "    \n",
    "    totalimage  = cleanimage\n",
    "    binarynew   = binary\n",
    "    \n",
    "    img_adapteq = np.multiply(totalimage, binarynew)\n",
    "    \n",
    "    print('value range is ', value_range)\n",
    "    mul_result   = img_adapteq*value_range\n",
    "    \n",
    "    print('Max is ', np.max(mul_result))\n",
    "    print('Min is ', np.min(mul_result))\n",
    "    \n",
    "    mul_result   = mul_result.astype(np.uint16)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Max is ', np.max(mul_result))\n",
    "    print('Min is ', np.min(mul_result))\n",
    "    \n",
    "    max_value = np.max(mul_result)\n",
    "    mul_result[mul_result == 0] = max_value*2\n",
    "    \n",
    "    #plt.hist(mul_result.flatten(), bins=256, range=(np.min(mul_result)+0.01, np.max(mul_result)), fc='k', ec='k')\n",
    "    #plt.show()\n",
    "    \n",
    "    ds.PixelData = mul_result.tostring()\n",
    "    \n",
    "    savename = lowfilename.split('\\\\')[-1]\n",
    "    savepath = cleanprojpath+savename\n",
    "    ds.save_as(savepath)\n",
    "\n",
    "def test_on_whole_slice(lowfilename, highfilename, model, xindex, jindex):\n",
    "    highdose = []\n",
    "    lowdose  = []\n",
    "    cliplimit = 0.03\n",
    "    \n",
    "    #projindex    = str(slicenum).zfill(2)+'.dcm'\n",
    "    #lowfilename  = projectionpath+str(filenum)+'_20/_'+projindex\n",
    "    #highfilename = highprojectionpath+str(filenum)+'_100/_'+projindex\n",
    "    \n",
    "    ds     = pydicom.dcmread(lowfilename)\n",
    "    lowimg = ds.pixel_array\n",
    "    \n",
    "    ds      = pydicom.dcmread(highfilename)\n",
    "    highimg = ds.pixel_array\n",
    "    \n",
    "    # get the cropped image\n",
    "    lowimg, highimg =  get_crop_image(lowimg, highimg, cliplimit)\n",
    "    \n",
    "    totalimage      = []\n",
    "    totalimagereal  = []\n",
    "    totalimagerealh = []\n",
    "    \n",
    "    rowst = lowimg.shape[0]\n",
    "    colst = lowimg.shape[1]\n",
    "    \n",
    "    print('Shape of Image is ', rowst, 'x', colst)\n",
    "    \n",
    "    # Change the indexing later to accomodate the full image view\n",
    "    # Or second option is to do the averaging at the corner views\n",
    "    \n",
    "    # count of processed rows in the whole image\n",
    "    processed_count_total = 0\n",
    "    \n",
    "    # indexes for deciding the patch location\n",
    "    i = 0 \n",
    "    j = 0\n",
    "    \n",
    "    while(i+128 < rowst):\n",
    "        # for clean image from model\n",
    "        newrow      = []\n",
    "        # for the real lowdose image\n",
    "        newrowreal  = []\n",
    "        # for the real highdose image\n",
    "        newrowrealh = []\n",
    "        # count of images processed in a row\n",
    "        processed_count_row = 0\n",
    "        # re-initialize the counter for j\n",
    "        j = 0\n",
    "        \n",
    "        while(j+128 < colst):\n",
    "            cropimgx = i\n",
    "            cropimgy = j\n",
    "                \n",
    "            tempimg  = lowimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "            tempimgh = highimg[cropimgx:cropimgx+128, cropimgy:cropimgy+128]\n",
    "            \n",
    "            newrowreal.append(tempimg[32:96, 32:96])\n",
    "            newrowrealh.append(tempimgh[32:96, 32:96])\n",
    "            \n",
    "#             nonzerov = np.sum(lowimg_crop >= cliplimit)\n",
    "#             # If it is not mostly occupied then don't process it\n",
    "#             if nonzerov*1.0/(128*128) < 0.9:\n",
    "#                 newrow.append(tempimg[32:96, 32:96])\n",
    "#                 processed_count_row = processed_count_row+1\n",
    "#                 continue\n",
    "            \n",
    "            cleanimg = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "            newrow.append(cleanimg[0, :, :, 0][32:96, 32:96])\n",
    "            processed_count_row = processed_count_row+1\n",
    "            \n",
    "            if i  == (128*xindex) and j == (128*jindex):\n",
    "                print('Clean Image SNR',    np.mean(cleanimg[0, :, :, 0])/np.std(cleanimg[0, :, :, 0]))\n",
    "                print('Original Image SNR', np.mean(tempimg)/np.std(tempimg))\n",
    "                print('HIGH Original Image SNR', np.mean(tempimgh)/np.std(tempimgh))\n",
    "                \n",
    "                ssim_low   = ssim(tempimgh, tempimg, data_range=tempimg.max() - tempimg.min())\n",
    "                ssim_clean = ssim(tempimgh, cleanimg[0, :, :, 0], data_range=cleanimg[0, :, :, 0].max() - cleanimg[0, :, :, 0].min())\n",
    "                \n",
    "                print('SSIM low ',   ssim_low)\n",
    "                print('SSIM clean ', ssim_clean)\n",
    "                \n",
    "                mse_clean   = mse(tempimgh, cleanimg[0, :, :, 0])\n",
    "                mse_low     = mse(tempimgh, tempimg)\n",
    "                \n",
    "                print('MSE low ',   mse_low)\n",
    "                print('MSE clean ', mse_clean)\n",
    "                \n",
    "                imsave('testimage.jpg', tempimg)\n",
    "                imsave('clean2.jpg', cleanimg[0, :, :, 0])\n",
    "                imsave('low2.jpg',   tempimg)\n",
    "                imsave('high2.jpg',  tempimgh)\n",
    "            j = j+64\n",
    "        i = i+64\n",
    "                \n",
    "        combined = np.array(newrow)\n",
    "        #print('newrow length is ', len(newrow), combined.shape)\n",
    "        \n",
    "        combined = np.hstack(combined.reshape(processed_count_row, 64, 64))\n",
    "        totalimage.append(combined)\n",
    "        \n",
    "        combinedreal = np.array(newrowreal)\n",
    "        combinedreal = np.hstack(combinedreal.reshape(processed_count_row, 64, 64))\n",
    "        totalimagereal.append(combinedreal)\n",
    "        \n",
    "        combinedrealh = np.array(newrowrealh)\n",
    "        combinedrealh = np.hstack(combinedrealh.reshape(processed_count_row, 64, 64))\n",
    "        totalimagerealh.append(combinedrealh)\n",
    "        \n",
    "        # increment the rows processed count\n",
    "        processed_count_total = processed_count_total+1\n",
    "        \n",
    "        #print('Shape of the combined image is ', combined.shape)\n",
    "    \n",
    "    totalimage = np.array(totalimage)\n",
    "    totalimage = np.vstack(totalimage.reshape(processed_count_total, 64, processed_count_row*64))\n",
    "    \n",
    "    totalimagereal = np.array(totalimagereal)\n",
    "    totalimagereal = np.vstack(totalimagereal.reshape(processed_count_total, 64, processed_count_row*64))\n",
    "    \n",
    "    totalimagerealh = np.array(totalimagerealh)\n",
    "    totalimagerealh = np.vstack(totalimagerealh.reshape(processed_count_total, 64, processed_count_row*64))\n",
    "    \n",
    "    return totalimage, totalimagereal, totalimagerealh\n",
    "\n",
    "image_indexes_result      = []\n",
    "temp_image_indexes_result = []\n",
    "\n",
    "def store_results(epoch, saveseed=5):\n",
    "    global image_indexes_result\n",
    "    global temp_image_indexes_result\n",
    "    \n",
    "    imgshape = 256\n",
    "    \n",
    "    # set the ids for saving the results\n",
    "    random.seed(saveseed)\n",
    "    \n",
    "    projindex    = 55\n",
    "    lowfilename  = projectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "    highfilename = highprojectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "    \n",
    "    lowimg  = np.fromfile(lowfilename, dtype='float32')\n",
    "    highimg = np.fromfile(highfilename, dtype='float32')\n",
    "    lowvol  = np.reshape(lowimg, [25, 1600, 3200])/1250.0\n",
    "    highvol = np.reshape(highimg, [25, 1600, 3200])/3750.0\n",
    "    \n",
    "    slice_index = 12#random.randint(0, 24)\n",
    "    lowimg      = lowvol[slice_index]\n",
    "    highimg     = highvol[slice_index]\n",
    "    \n",
    "    # get the cropped image\n",
    "    lowimg, highimg =  get_crop_image(lowimg, highimg)\n",
    "    \n",
    "    r, c = 6, 6\n",
    "    x, y = get_train_data_proj(lowimg, highimg, 1, 128)\n",
    "    x = x-0.5\n",
    "    y = y-0.5\n",
    "    \n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95)\n",
    "    plt.tight_layout()\n",
    "    fig, axs = plt.subplots(r, c, figsize=(30,10))\n",
    "    cnt      = 0\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            if len(image_indexes_result) == 0:\n",
    "                imgindex = random.randint(1, 127)\n",
    "                temp_image_indexes_result.append(imgindex)\n",
    "            else:\n",
    "                imgindex = image_indexes_result[cnt]\n",
    "            \n",
    "            cleanimg = aae.generator.predict(np.expand_dims([x[imgindex]], axis=3))\n",
    "            combined = np.array([x[imgindex]+0.5, cleanimg[0, :, :, 0]+0.5, y[imgindex]+0.5])\n",
    "            combined = np.hstack(combined.reshape(3,imgshape,imgshape))\n",
    "            #print(cleanimg[0, :, :, 0].shape, y[imgindex].shape, combined.shape)\n",
    "            axs[i,j].imshow(combined, cmap='gray')\n",
    "            axs[i, j].set_xticklabels([])\n",
    "            axs[i, j].set_yticklabels([])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    if len(image_indexes_result) == 0:\n",
    "        image_indexes_result = temp_image_indexes_result\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "    fig.savefig(savepath+modelname+\"-results/clean_%d.png\" % epoch, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     63,
     104,
     138
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Model for training the de-noising network\n",
    "\n",
    "img_shape  = (256, 256, 1)\n",
    "\n",
    "def huber_loss(y_true, y_pred, clip_delta=0.04):\n",
    "    error        = y_true - y_pred\n",
    "    cond         = tf.keras.backend.abs(error) < clip_delta\n",
    "    squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "    linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "    return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))\n",
    "\n",
    "class AdversarialAutoencoder():\n",
    "    def __init__(self):\n",
    "        self.optimizer1  = RMSprop(0.0005)\n",
    "        self.optimizer2  = RMSprop(0.00001)\n",
    "        \n",
    "        self.clip_value = 0.01\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        \n",
    "        #self.discriminator.load_weights('/media/pranjal/de24af8d-2361-4ea2-a07a-1801b54488d9/DBT_data/Results/vanilla-gan-weights-mse-0.1/discriminator_weights_8050.h5')\n",
    "        self.d_arr = []\n",
    "        self.g_arr = []\n",
    "        \n",
    "        # Build the encoder / decoder\n",
    "        self.generator = self.build_generator()\n",
    "         \n",
    "        img = Input(shape=img_shape)\n",
    "        # The generator takes the image, encodes it and reconstructs it\n",
    "        # from the encoding\n",
    "        reconstructed_img            = self.generator(img)\n",
    "        #self.adversarial_autoencoder = Model(img, reconstructed_img)\n",
    "        #self.adversarial_autoencoder.compile()\n",
    "        # For the adversarial_autoencoder model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator determines validity of the cleaned image\n",
    "        validity = self.discriminator(reconstructed_img)\n",
    "\n",
    "        # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
    "        self.adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
    "        self.adversarial_autoencoder.compile(loss=[huber_loss_mean, 'mse'],\n",
    "            loss_weights=[0.9, 0.1],\n",
    "            optimizer=self.optimizer2)\n",
    "        \n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=self.optimizer1, \n",
    "            metrics=['accuracy'])\n",
    "        #self.discriminator.load_weights('/media/pranjal/de24af8d-2361-4ea2-a07a-1801b54488d9/DBT_data/Results/gan-proj-mse-0.99-sub-weights/discriminator_weights_8400.h5')\n",
    "        #self.generator.load_weights('/media/pranjal/de24af8d-2361-4ea2-a07a-1801b54488d9/DBT_data/Results/gan-proj-mse-0.99-sub-weights/generator_weights_8400.h5')\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        self.generator.compile(loss=['mse'],optimizer=self.optimizer)\n",
    "     \n",
    "    def build_generator(self):\n",
    "        x = Input(shape=img_shape)\n",
    "        x1 = Conv2D(32, (3, 3), padding='same')(x)\n",
    "        x1 = LeakyReLU(alpha=0.2)(x1)\n",
    "        \n",
    "        x2 = Conv2D(64, (3, 3), padding='same')(x1)\n",
    "        #x2 = BatchNormalization(momentum=0.8)(x2)\n",
    "        x2 = LeakyReLU(alpha=0.2)(x2)\n",
    "        \n",
    "        x3 = Conv2D(64, (3, 3), padding='same')(x2)\n",
    "        #x3 = BatchNormalization(momentum=0.8)(x3)\n",
    "        x3 = LeakyReLU(alpha=0.2)(x3)\n",
    "        \n",
    "        #x3_m = merge([x2, x3], mode='concat', concat_axis=3)\n",
    "        x4 = Conv2D(64, (3, 3), padding='same')(x3)\n",
    "        #x4 = BatchNormalization(momentum=0.8)(x4)\n",
    "        x4 = LeakyReLU(alpha=0.2)(x4)\n",
    "        #x6_i = merge([x2, x6], mode='concat', concat_axis=3)\n",
    "        \n",
    "        \n",
    "        x5 = Conv2D(128, (3, 3), padding='same')(x4)\\\n",
    "        #x5 = BatchNormalization(momentum=0.8)(x5)\n",
    "        x5 = LeakyReLU(alpha=0.2)(x5)\n",
    "        \n",
    "        #x7_i = merge([x1, x7], mode='concat', concat_axis=3)\n",
    "        x6 = Conv2D(128, (3, 3), padding='same')(x5)\n",
    "        x6 = LeakyReLU(alpha=0.2)(x6)\n",
    "        \n",
    "        x7 = Conv2D(64, (3, 3), padding='same')(x6)\n",
    "        #x8 = BatchNormalization(momentum=0.8)(x8)\n",
    "        x7 = LeakyReLU(alpha=0.2)(x7)\n",
    "        #x7_m = merge([x1, x6], mode='concat', concat_axis=3)\n",
    "        #x8    = Conv2D(1, (3, 3), activation='relu', padding='same')(x6)\n",
    "        x8 = Conv2D(1, (3, 3), padding='same')(x7)\n",
    "        x8 = LeakyReLU(alpha=0.2)(x8)\n",
    "        \n",
    "        out = keras.layers.Subtract()([x, x8])\n",
    "        #out = ReLU()(out)\n",
    "        model = Model(x, out)\n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, (3, 3), input_shape=img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64,  (3, 3),  strides=(2, 2)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #         model.add(Conv2D(16, (6, 6),  strides=(2, 2)))\n",
    "        #         model.add(LeakyReLU(alpha=0.2))\n",
    "        #         model.add(Dropout(0.5))\n",
    "        model.add(Conv2D(64, (3, 3),  strides=(2, 2)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, (3, 3)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, (3, 3),  strides=(2, 2)))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def train_generator_autoencoder(self, epochs, batch_size=128):\n",
    "        for epoch in range(epochs):\n",
    "            # Load the dataset\n",
    "            X_train, Y_train = get_train_data(epoch, batch_size)\n",
    "            \n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "            Y_train = np.expand_dims(Y_train, axis=3)\n",
    "            \n",
    "            g_loss = self.generator.train_on_batch(X_train, Y_train)\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"Epoch \", epoch, \" G loss \", g_loss)\n",
    "            if epoch%50 == 0:\n",
    "                store_results(epoch)\n",
    "                self.generator.save_weights(savepath+modelname+'-weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                self.discriminator.save_weights(savepath+modelname+'-weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "    \n",
    "    def train(self, epochs, batch_size=128, sampling=50, saveseed=5, startepoch=0, discriminator_epochs=5):\n",
    "        for epoch in range(startepoch, epochs):\n",
    "            # Train the discriminator 5 times\n",
    "            #print('Training Discriminator ', epoch)\n",
    "            \n",
    "            projindex = str(random.randint(43, 68))#+'.dcm'\n",
    "    \n",
    "            lowfilename  = projectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "            highfilename = highprojectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "\n",
    "            lowimg  = np.fromfile(lowfilename, dtype='float32')\n",
    "            highimg = np.fromfile(highfilename, dtype='float32')\n",
    "            \n",
    "            lowvol  = np.reshape(lowimg,  [25, 1600, 3200])/1250.0\n",
    "            highvol = np.reshape(highimg, [25, 1600, 3200])/3750.0\n",
    "            \n",
    "            for itd in range(discriminator_epochs):\n",
    "                slice_index = random.randint(0, 24)\n",
    "                lowimg      = lowvol[slice_index]\n",
    "                highimg     = highvol[slice_index]\n",
    "                \n",
    "                # get the cropped image\n",
    "                lowimg, highimg =  get_crop_image(lowimg, highimg)\n",
    "                \n",
    "                # Load the dataset\n",
    "                X_train, Y_train = get_train_data_proj(lowimg, highimg, epoch, batch_size)\n",
    "                X_train = X_train - 0.5\n",
    "                Y_train = Y_train - 0.5\n",
    "                \n",
    "                #print(X_train.shape, Y_train.shape)\n",
    "                \n",
    "                X_train = np.expand_dims(X_train, axis=3)\n",
    "                Y_train = np.expand_dims(Y_train, axis=3)\n",
    "\n",
    "                # Adversarial ground truths\n",
    "                valid = np.ones((batch_size, 1))\n",
    "                fake  = np.zeros((batch_size, 1))\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                # get the noisy image and feed it into the generator\n",
    "                X_train_clean = self.generator.predict(X_train)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)\n",
    "                d_loss_real = self.discriminator.train_on_batch(Y_train, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(X_train_clean, fake)\n",
    "                d_loss      = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            #print('Training Generator ', epoch)\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            # Train the generator\n",
    "            g_loss = self.adversarial_autoencoder.train_on_batch(X_train, [Y_train, valid])\n",
    "            #print('g_loss ', g_loss)\n",
    "            self.d_arr.append(d_loss)\n",
    "            self.g_arr.append(g_loss)\n",
    "            \n",
    "            #print('Epoch ', epoch, ' Total D loss ', -d_loss, ' D loss real ', -d_loss_real, ' D loss fake ', d_loss_fake, ' G_loss ', g_loss[0], g_loss[1], g_loss[2])\n",
    "            # Plot the progress\n",
    "            #print (\"%d [D loss: %f] [G loss: %f] [G loss: %f %f]\" % (epoch, 1 - d_loss, 1 - g_loss[0], g_loss[0], g_loss[1]))\n",
    "            #print('Epoch ', epoch, ' Total D loss ', -d_loss, ' G_loss ', g_loss[0], g_loss[1], g_loss[2])\n",
    "            \n",
    "            if epoch%sampling == 0:\n",
    "                store_results(epoch, saveseed)\n",
    "                self.generator.save_weights(savepath+modelname+'-weights/generator_weights_'+str(epoch)+'.h5')\n",
    "                #self.discriminator.save_weights(savepath+modelname+'-weights/discriminator_weights_'+str(epoch)+'.h5')\n",
    "            \n",
    "            print (\"%d [D loss: %f, mean_acc: %.2f%% real_acc: %.2f%% fake_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], 100*d_loss_real[1], 100*d_loss_fake[1], g_loss[2]*0.0001, g_loss[1]*0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [STAR] Training loop of the model\n",
    "\n",
    "aae = AdversarialAutoencoder()\n",
    "# print(aae.generator.summary())\n",
    "# print(aae.discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aae.train(epochs=25000, batch_size=32, sampling=50, saveseed=7590, startepoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     36,
     57,
     76,
     95
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For denoising the projections of dicom files\n",
    "from skimage.filters import unsharp_mask, threshold_local, threshold_minimum\n",
    "\n",
    "def test_on_whole_proj_200(lowfilename, model):\n",
    "    highdose = []\n",
    "    lowdose  = []\n",
    "    cliplimit = 0\n",
    "    imgshape  = 256\n",
    "    \n",
    "    ds             = pydicom.dcmread(lowfilename)\n",
    "    limg           = ds.pixel_array\n",
    "    original_image = copy.deepcopy(limg)\n",
    "    \n",
    "    cleanimage        = np.zeros(limg.shape)\n",
    "    thresh            = threshold_otsu(limg)\n",
    "    \n",
    "    outsideimage_mask = limg > thresh\n",
    "    insideimage_mask  = limg < thresh\n",
    "    \n",
    "    binary  = limg <= thresh\n",
    "    \n",
    "    result       = np.multiply(binary, limg)\n",
    "    resultlowimg = result\n",
    "    resultlowimg = resultlowimg.astype(float)\n",
    "    lowimg       = resultlowimg\n",
    "    \n",
    "    # Perform sharpening before de-noising\n",
    "    #lowimg = unsharp_mask(lowimg, radius=3, amount=0.5, preserve_range=True)\n",
    "    \n",
    "    print(np.min(resultlowimg.flatten()), np.max(resultlowimg.flatten()))\n",
    "    rowst = limg.shape[0]\n",
    "    colst = limg.shape[1]\n",
    "    \n",
    "    # For top row\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(j+imgshape < colst):\n",
    "        cropimgx     = i\n",
    "        cropimgy     = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]/1250.0 - 0.5\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        #cleanimg    = cleanimg*3750.0\n",
    "        \n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        #cleanimg    = cleanimg + before_mean\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = (cleanimg[0, :, :, 0]+0.5)*1250\n",
    "        j = j+imgshape\n",
    "    \n",
    "    \n",
    "    # For bottom row\n",
    "    i = rowst-imgshape\n",
    "    j = 0\n",
    "    while(j+imgshape < colst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]/1250.0 - 0.5\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        #cleanimg    = cleanimg + before_mean\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = (cleanimg[0, :, :, 0]+0.5)*1250\n",
    "        j = j+imgshape\n",
    "    \n",
    "    # For rightmost column\n",
    "    j = colst-imgshape\n",
    "    i = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]/1250.0 - 0.5\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        #cleanimg    = cleanimg + before_mean\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = (cleanimg[0, :, :, 0]+0.5)*1250\n",
    "        i = i+imgshape\n",
    "    \n",
    "    # For leftmost column\n",
    "    j = 0\n",
    "    i = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        cropimgx = i\n",
    "        cropimgy = j\n",
    "        tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]/1250.0 - 0.5\n",
    "        lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "        nonzerov     = np.sum(lowimg_crop > cliplimit)    \n",
    "        \n",
    "        before_mean = np.mean(tempimg)\n",
    "        cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "        after_mean  = np.mean(cleanimg)\n",
    "        #cleanimg    = cleanimg + before_mean\n",
    "        cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "        \n",
    "        cleanimage[i:i+imgshape, j:j+imgshape] = (cleanimg[0, :, :, 0]+0.5)*1250\n",
    "        i = i+imgshape\n",
    "    \n",
    "    halfvalue    = int(imgshape/2) # replacement size\n",
    "    quartervalue = int(imgshape/4) # stride size \n",
    "    #+int(imgshape/2)\n",
    "    \n",
    "    # indexes for deciding the patch location\n",
    "    i = 0 \n",
    "    j = 0\n",
    "    while(i+imgshape < rowst):\n",
    "        # re-initialize the counter for j\n",
    "        j = 0\n",
    "        while(j+imgshape < colst):\n",
    "            cropimgx = i\n",
    "            cropimgy = j\n",
    "            \n",
    "            tempimg      = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]/1250.0 -0.5\n",
    "            lowimg_crop  = lowimg[cropimgx:cropimgx+imgshape, cropimgy:cropimgy+imgshape]\n",
    "            nonzerov     = np.sum(lowimg_crop > cliplimit)\n",
    "            \n",
    "            before_mean = np.mean(tempimg)\n",
    "            cleanimg    = model.generator.predict(np.expand_dims([tempimg], axis=3))\n",
    "            after_mean  = np.mean(cleanimg)\n",
    "            #cleanimg    = cleanimg + before_mean\n",
    "            cleanimg    = cleanimg - (after_mean-before_mean)\n",
    "            #cleanimg[cleanimg < 0] = 0\n",
    "            #print(after_mean, before_mean)\n",
    "            cleanimage[i+quartervalue:i+quartervalue+halfvalue, j+quartervalue:j+quartervalue+halfvalue] = (cleanimg[0, :, :, 0]+0.5)[quartervalue:quartervalue+halfvalue, quartervalue:quartervalue+halfvalue]*1250\n",
    "            #j = j+halfvalue\n",
    "            j = j+quartervalue\n",
    "        #i = i+halfvalue\n",
    "        i = i+quartervalue\n",
    "    \n",
    "#     savename = lowfilename.split('/')[-1]\n",
    "#     savepath = cleanprojpath+savename+'.npy'\n",
    "#     np.save(savepath, cleanimage)\n",
    "    \n",
    "    #cleanimage[cleanimage < 0] = 0\n",
    "    #cleanimage[cleanimage > 2000] = 2000\n",
    "    \n",
    "    totalimage  = cleanimage\n",
    "    binarynew   = binary\n",
    "    \n",
    "    img_adapteq  = np.multiply(totalimage, binarynew)\n",
    "    mul_result   = img_adapteq\n",
    "    \n",
    "    # Adding the Code to replace the oustide breast region with original values\n",
    "    mul_result     = mul_result*insideimage_mask        # Get the inside region only\n",
    "    mul_result[mul_result < 0] = 0\n",
    "    print(np.min(mul_result.flatten()), np.max(mul_result.flatten()), mul_result.dtype, cleanimage.dtype, cleanimg.dtype)\n",
    "    \n",
    "    original_image = original_image*outsideimage_mask   # Get the outside region only\n",
    "    mul_result     = mul_result+original_image \n",
    "    \n",
    "    \n",
    "    mul_result   = mul_result.astype(np.uint16)\n",
    "    \n",
    "    ds.PixelData = mul_result.tostring()\n",
    "    \n",
    "    savename = lowfilename.split('/')[-1]\n",
    "    savepath = cleanprojpath+savename\n",
    "    ds.save_as(savepath)\n",
    "    \n",
    "\n",
    "\n",
    "lowprojpath   = '/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-L-CC/'\n",
    "cleanprojpath = '/media/pranjal/newdrive1/REAL-DBT-PROJECTIONS/Pranjal-PMA-DATA/04140608/LE-L-CC-CLEAN1-750-HUBER/'\n",
    "\n",
    "#/media/pranjal/newdrive1/DBT-PROJ-DENOISE/normal-to-three-huber-0.9-weights\n",
    "\n",
    "aae = AdversarialAutoencoder()\n",
    "aae.generator.load_weights('/media/pranjal/newdrive1/DBT-PROJ-DENOISE/normal-to-three-huber-0.9-weights/generator_weights_750.h5')\n",
    "#aae.generator.load_weights('/media/pranjal/newdrive1/DBT-PROJ-DENOISE/normal-to-three-0.99-weights/generator_weights_400.h5')\n",
    "\n",
    "files = glob.glob(lowprojpath+'*.IMA')\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    test_on_whole_proj_200(f, aae)\n",
    "    \n",
    "    \n",
    "#tensorflow.keras.losses.Huber(delta=1.5)\n",
    "#tf.keras.losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projindex = str(random.randint(43, 68))#+'.dcm'\n",
    "epoch = 0\n",
    "batch_size = 32\n",
    "\n",
    "lowfilename  = projectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "highfilename = highprojectionpath+'3200x1600x25.'+str(projindex)+'.raw'\n",
    "\n",
    "lowimg  = np.fromfile(lowfilename, dtype='float32')\n",
    "highimg = np.fromfile(highfilename, dtype='float32')\n",
    "\n",
    "lowvol  = np.reshape(lowimg, [25, 1600, 3200])/1250.0\n",
    "highvol = np.reshape(highimg, [25, 1600, 3200])/3750.0\n",
    "\n",
    "\n",
    "slice_index = random.randint(0, 24)\n",
    "lowimg      = lowvol[slice_index]\n",
    "highimg     = highvol[slice_index]\n",
    "\n",
    "# get the cropped image\n",
    "lowimg, highimg =  get_crop_image(lowimg, highimg)\n",
    "\n",
    "# Load the dataset\n",
    "X_train, Y_train = get_train_data_proj(lowimg, highimg, epoch, batch_size)\n",
    "X_train  = X_train  - 0.5\n",
    "Y_train  = Y_train  - 0.5\n",
    "\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Recon\n",
      "Phantom  68\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For generating the projections\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def my_divide(x, y):\n",
    "    return np.divide(x, y)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def my_add(x, y):\n",
    "    return np.add(x, y)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def my_mul(x, y):\n",
    "    return np.multiply(x, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#testvol = ndimage.zoom(testvol, 0.25, order=1)\n",
    "#testvol = np.zeros([320, 448, 800], dtype='uint8')\n",
    "\n",
    "scaling_factor = 1\n",
    "\n",
    "SOD       = 65/0.025\n",
    "ODD       = 4.5/0.025\n",
    "detWidth  = 0.0085/0.025 #/(0.0255/scaling_factor)   # size of each detector pixel\n",
    "detHeight = detWidth        # size of each detector pixel\n",
    "detRows   = 3584\n",
    "detCols   = 1600\n",
    "\n",
    "\n",
    "num_angles = 25\n",
    "\n",
    "#estimate = np.fromfile(\"/media/pranjal/2d33dff3-95f7-4dc0-9842-a9b18bcf1bf9/pranjal/DBT_data/ClinicalExample/CE-05_R/ce.800x448x320.05-le.raw\", dtype='float32')\n",
    "#estimate = np.reshape(estimate, [320, 448, 800])\n",
    "\n",
    "\n",
    "a       = sio.loadmat(\"/media/pranjal/cewitdata2/DBT-PROJ-DENOISE/attenuation_values_cropped/LE/1.mat\")[\"head\"]\n",
    "testvol = np.rollaxis(a, 2, 0)\n",
    "testvol = np.moveaxis(testvol, [0, 1, 2], [0, 2, 1])\n",
    "testvol = testvol/65.0\n",
    "\n",
    "\n",
    "vol_geom = astra.create_vol_geom(int(testvol.shape[1]), int(testvol.shape[2]), int(testvol.shape[0]))\n",
    "proj_arr = np.zeros((num_angles, detCols, detRows), dtype='float16')\n",
    "testones = np.ones([int(testvol.shape[0]), int(testvol.shape[1]), int(testvol.shape[2])],  dtype='uint8')\n",
    "estimate = np.zeros([int(testvol.shape[0]), int(testvol.shape[1]), int(testvol.shape[2])], dtype='float16')\n",
    "\n",
    "print('Starting Recon')\n",
    "\n",
    "\n",
    "#proj_arr = np.array(proj_arr).astype('float16')\n",
    "#proj_arr = np.rollaxis(proj_arr, 1, 0)\n",
    "#proj_arr = proj_arr.flatten()\n",
    "\n",
    "\n",
    "# All Flags\n",
    "recon_type       = 0\n",
    "use_old_calci    = 0 # Use already generated calcifications\n",
    "save_projections = 1 # Save the projections\n",
    "insert_noise     = 1\n",
    "\n",
    "\n",
    "triple_dose  = True\n",
    "quarter_dose = False\n",
    "normal_dose  = False\n",
    "\n",
    "# SIRT iteration LOOP\n",
    "if recon_type == 0:\n",
    "    niter      = 10\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index in range(68, 69):\n",
    "        print(\"Phantom \", index)\n",
    "        a       = sio.loadmat(\"/media/pranjal/cewitdata2/DBT-PROJ-DENOISE/attenuation_values_cropped/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "        testvol = np.rollaxis(a, 2, 0)\n",
    "        testvol = np.moveaxis(testvol, [0, 1, 2], [0, 2, 1])\n",
    "        volume_mask = testvol > 0\n",
    "        \n",
    "        testvol = np.multiply(volume_mask, testvol)\n",
    "        testvol = testvol/65.0\n",
    "        \n",
    "        ang = []\n",
    "        for i in range(num_angles):\n",
    "            ang.append(1.9)\n",
    "        start_angle = -25#+np.random.randint(0, 3)\n",
    "        theta       = []\n",
    "        for i in range(num_angles):\n",
    "            theta.append(start_angle*np.pi/180.0)\n",
    "            start_angle = start_angle+ang[i]\n",
    "        \n",
    "        vectors = np.zeros((len(theta), 12))\n",
    "        \n",
    "        # For reconstructing real data\n",
    "        vectors[:, 0:3]  = np.transpose(np.array([np.sin(theta), np.zeros(len(theta)), np.cos(theta)])) * SOD        # S source to object\n",
    "        vectors[:,3:6]   = np.transpose(np.array([np.zeros(len(theta)), np.zeros(len(theta)),  -np.ones(len(theta))*ODD]))             # D object to detector\n",
    "        vectors[:,6:9]   = np.transpose(np.array([np.ones(len(theta))*detWidth, np.zeros(len(theta)), np.zeros(len(theta))]))         # U\n",
    "        vectors[:,9:12]  = np.transpose(np.array([np.zeros(len(theta)), np.ones(len(theta))*detWidth, np.zeros(len(theta))]))        # V\n",
    "\n",
    "        # Creating the projection matrix\n",
    "        proj_geom        = astra.create_proj_geom('cone_vec', detCols, detRows, vectors)\n",
    "        proj_id          = astra.create_projector('cuda3d',   proj_geom, vol_geom)\n",
    "        W                = astra.OpTomo(proj_id)\n",
    "        \n",
    "        proj_arr         = W*testvol\n",
    "        \n",
    "        temp_proj = np.reshape(proj_arr, [detCols, num_angles, detRows])\n",
    "        temp_proj = np.rollaxis(temp_proj, 0, 2)\n",
    "        \n",
    "        if triple_dose:\n",
    "            I0        = 6000\n",
    "            proj      = I0*np.exp(-temp_proj)\n",
    "            proj_noi  = np.random.poisson(proj)\n",
    "            proj_noi[proj_noi == 0] = 1\n",
    "            \n",
    "            g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "            g_noi[g_noi < 0]        = 0\n",
    "            \n",
    "            fid = open('/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/DOUBLE/'+str(temp_proj.shape[2])+'x'+str(temp_proj.shape[1])+'x'+str(temp_proj.shape[0])+'.'+str(index)+'.raw', 'w')\n",
    "            np.array(proj_noi).astype('float32').tofile(fid)\n",
    "            #np.array(g_noi).astype('float32').tofile(fid)\n",
    "        \n",
    "        if normal_dose:\n",
    "            I0        = 2000\n",
    "            proj      = I0*np.exp(-temp_proj)\n",
    "            proj_noi  = np.random.poisson(proj)\n",
    "            proj_noi[proj_noi == 0] = 1\n",
    "            \n",
    "            g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "            g_noi[g_noi < 0]        = 0\n",
    "            \n",
    "            fid = open('/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/NORMAL/'+str(temp_proj.shape[2])+'x'+str(temp_proj.shape[1])+'x'+str(temp_proj.shape[0])+'.'+str(index)+'.raw', 'w')\n",
    "            np.array(proj_noi).astype('float32').tofile(fid)\n",
    "            #np.array(g_noi).astype('float32').tofile(fid)\n",
    "        \n",
    "        if(quarter_dose):\n",
    "            I0        = 500\n",
    "            proj      = I0*np.exp(-temp_proj)\n",
    "            proj_noi  = np.random.poisson(proj)\n",
    "            proj_noi[proj_noi == 0] = 1\n",
    "            \n",
    "            g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "            g_noi[g_noi < 0]        = 0\n",
    "            \n",
    "            fid = open('/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/QUARTER/'+str(temp_proj.shape[2])+'x'+str(temp_proj.shape[1])+'x'+str(temp_proj.shape[0])+'.'+str(index)+'.raw', 'w')\n",
    "            #np.array(proj_noi).astype('float32').tofile(fid)\n",
    "            np.array(g_noi).astype('float32').tofile(fid)\n",
    "        \n",
    "            #g_noi                   = np.log(I0) - np.log(proj_noi) # convert back to line integrals \n",
    "            #g_noi[g_noi < 0]        = 0\n",
    "            #proj_arr = proj_noi#g_noi\n",
    "        \n",
    "#         temp_proj  = np.reshape(proj_arr, [detCols, num_angles, detRows])\n",
    "#         temp_proj1 = temp_proj[:, 15:-15, :]\n",
    "        \n",
    "#         # Saving the Projections\n",
    "#         if save_projections:\n",
    "#             fid = open('/media/pranjal/cewitdata2/DBT-PROJ-DENOISE/QUARTER/'+str(temp_proj.shape[2])+'x'+str(temp_proj.shape[0])+'x'+str(temp_proj.shape[1])+'.'+str(index)+'.raw', 'w')\n",
    "#             temp_proj  = np.rollaxis(temp_proj, 0, 2)\n",
    "#             np.array(temp_proj).astype('float32').tofile(fid)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1600, 3200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "index = 68\n",
    "all_proj = np.fromfile(\"/media/pranjal/cewitdata1/DBT-PROJ-DENOISE/NORMAL/3200x1600x25.\"+str(index)+\".raw\", dtype='float32')\n",
    "all_proj = np.reshape(all_proj, [25, 1600, 3200])\n",
    "                      \n",
    "print(all_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 448, 320)\n"
     ]
    }
   ],
   "source": [
    "a       = sio.loadmat(\"/media/pranjal/cewitdata2/DBT-PROJ-DENOISE/attenuation_values_cropped/LE/\"+str(index)+\".mat\")[\"head\"]\n",
    "print(a.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 2748, 1073)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Code to read the data\n",
    "\n",
    "a = np.fromfile('/media/pranjal/cewitdata1/pranjal/CT-RECON-DATA/MC_CE27_slice32_atzz666_dense_uppaddle_pc_209923395_crop_1073x2748x816.raw', dtype='uint8')\n",
    "a = np.reshape(a, [816, 2748, 1073])\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "temp = a[666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('calcification_cluster.npy', temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 2415, 1740)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81a14b9160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.fromfile('/media/pranjal/cewitdata1/pranjal/CT-RECON-DATA/pcl_22183101_crop.raw', dtype='uint8')\n",
    "a = np.reshape(a, [1140, 2415, 1740])\n",
    "#  1740   2415   1140\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "#temp = a[666]\n",
    "plt.imshow(a[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 350)\n"
     ]
    }
   ],
   "source": [
    "temp1 = np.load('calcification_cluster.npy')\n",
    "print(temp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1    = a[500, 1500:1500+temp1.shape[0], 150:150+temp1.shape[1]]\n",
    "temp1[temp1 != 250] = a1[temp1 != 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f819f79c198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#a[499, 1500:1500+temp1.shape[0], 150:150+temp1.shape[1]] =  temp1\n",
    "#a[500, 1500:1500+temp1.shape[0], 150:150+temp1.shape[1]] =  temp1\n",
    "#a[501, 1500:1500+temp1.shape[0], 150:150+temp1.shape[1]] =  temp1\n",
    "\n",
    "plt.imshow(a[501])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tofile('/media/pranjal/cewitdata1/pranjal/CT-RECON-DATA/pcl_22183101_crop_calci.raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(858161,) (858161,)\n",
      "(58, 1200, 3000) (58, 1200, 3000) (58, 1200, 3000)\n"
     ]
    }
   ],
   "source": [
    "e = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.4_4_0.0005_-0.5_1_anistropic_three_1.raw', dtype='float32')\n",
    "e = np.reshape(e, [58, 1200, 3000])\n",
    "\n",
    "temp               = e[27]#, 900:1150, 950:1350]\n",
    "temp[temp < 0.03] = 0\n",
    "temp[temp > 0.03] = 1\n",
    "#plt.imshow(temp, cmap='gray')\n",
    "\n",
    "non_zero_index = np.nonzero(temp)\n",
    "\n",
    "print(non_zero_index[0].shape, non_zero_index[1].shape)\n",
    "print(a.shape, b.shape, e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(858161,) (858161,)\n",
      "85821 85821 858161\n",
      "(85821, 1, 5, 5, 5) (85821, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] For creating the dataset to train the potential function\n",
    "\n",
    "e = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.4_4_0.0005_-0.5_1_anistropic_three_1.raw', dtype='float32')\n",
    "e = np.reshape(e, [58, 1200, 3000])\n",
    "\n",
    "a = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3200x1280x58.0_0_0.0005_0_1_anistropic_normal8_1.raw', dtype='float32')\n",
    "#a = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.0_1_0.0005_0.5_1_anistropic_normalmlp1_1.raw', dtype='float32')\n",
    "#a = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.0_0_0.0005_0.5_1_anistropic_normalmlp1_1.raw', dtype='float32')\n",
    "a = np.reshape(a, [58, 1280, 3200])\n",
    "\n",
    "#b = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.0_1_0.0005_0_1_anistropic_normal_1.raw', dtype='float32')\n",
    "#b = np.reshape(b, [58, 1200, 3000])\n",
    "\n",
    "#c = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3000x1200x58.0_1_0.0005_0_1_anistropic_three_1.raw', dtype='float32')\n",
    "#c = np.reshape(c, [58, 1200, 3000])\n",
    "\n",
    "d = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3200x1280x58.0_0_0.0005_-0.1_1_anistropic_three8_1.raw', dtype='float32')\n",
    "d = np.reshape(d, [58, 1280, 3200])\n",
    "\n",
    "#temp1 = a[:, 900:1150, 950:1350]\n",
    "#D2    = d[:, 900:1150, 950:1350]\n",
    "\n",
    "Y_array = d\n",
    "X_array = a\n",
    "\n",
    "\n",
    "# For getting the locations of the training data points\n",
    "\n",
    "temp               = e[27]#, 900:1150, 950:1350]\n",
    "temp[temp < 0.03] = 0\n",
    "temp[temp > 0.03] = 1\n",
    "\n",
    "#temp[temp < 0.045] = 0\n",
    "plt.imshow(temp, cmap='gray')\n",
    "\n",
    "non_zero_index = np.nonzero(temp)\n",
    "\n",
    "print(non_zero_index[0].shape, non_zero_index[1].shape)\n",
    "#print(a.shape, b.shape, e.shape)\n",
    "\n",
    "\n",
    "# Generate the samples for the Regression Model\n",
    "\n",
    "X      = []\n",
    "Y      = []\n",
    "voxels = 5\n",
    "\n",
    "\n",
    "for i in range(non_zero_index[0].shape[0]):\n",
    "    if np.random.rand() > 0.1:\n",
    "        continue\n",
    "    \n",
    "    ind_z = random.randint(15, 45)\n",
    "    ind_y = non_zero_index[0][i]\n",
    "    ind_x = non_zero_index[1][i]\n",
    "    \n",
    "    tx = X_array[ind_z-2:ind_z+3, non_zero_index[0][i]-2:non_zero_index[0][i]+3, non_zero_index[1][i]-2:non_zero_index[1][i]+3]\n",
    "    #ty = Y_array[ind_z-2:ind_z+3, non_zero_index[0][i]-2:non_zero_index[0][i]+3, non_zero_index[1][i]-2:non_zero_index[1][i]+3]\n",
    "    ty = Y_array[ind_z,           non_zero_index[0][i], non_zero_index[1][i]]\n",
    "    \n",
    "    if len(tx.flatten()) ==  voxels*voxels*voxels:\n",
    "        X.append([tx])# - X_array[ind_z, non_zero_index[0][i], non_zero_index[1][i]])\n",
    "        Y.append([ty])\n",
    "\n",
    "print(len(X), len(Y), non_zero_index[0].shape[0])\n",
    "\n",
    "X =  np.array(X)\n",
    "#X =  np.reshape(X, [X.shape[0], voxels*voxels*voxels])\n",
    "Y =  np.array(Y)\n",
    "#Y =  np.reshape(Y, [Y.shape[0], voxels*voxels*voxels])\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    print('classname is ', classname)\n",
    "    \n",
    "    # for every Linear layer in a model..\n",
    "    #if classname.find('Linear') != -1:\n",
    "    # apply a uniform distribution to the weights and a bias=0\n",
    "    m.weight1.data.uniform_(0.0, 0.03)\n",
    "    m.bias1.data.data.uniform_(0.0, 0.03)#.fill_(0)\n",
    "    \n",
    "    m.weight2.data.uniform_(0.0, 0.03)\n",
    "    m.bias2.data.data.uniform_(0.0, 0.03)#.fill_(0)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.001)\n",
    "\n",
    "#weights_init_uniform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     15,
     35
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation Accuracy  0.007004612\n",
      "1 Validation Accuracy  0.0005201878\n",
      "2 Validation Accuracy  0.00023408422\n",
      "4 Validation Accuracy  0.00021693288\n",
      "5 Validation Accuracy  0.000208495\n",
      "6 Validation Accuracy  0.00020386226\n",
      "7 Validation Accuracy  0.00019916958\n",
      "8 Validation Accuracy  0.00019836106\n",
      "9 Validation Accuracy  0.00018804193\n",
      "10 Validation Accuracy  0.0001854027\n",
      "13 Validation Accuracy  0.00017951554\n",
      "15 Validation Accuracy  0.00017731049\n",
      "17 Validation Accuracy  0.00017239638\n",
      "18 Validation Accuracy  0.00017114171\n",
      "19 Validation Accuracy  0.0001701624\n",
      "21 Validation Accuracy  0.00016862054\n",
      "23 Validation Accuracy  0.00016833055\n",
      "26 Validation Accuracy  0.0001661482\n",
      "30 Validation Accuracy  0.00016370388\n",
      "34 Validation Accuracy  0.00016233067\n",
      "37 Validation Accuracy  0.00016098037\n",
      "43 Validation Accuracy  0.00015899299\n",
      "48 Validation Accuracy  0.00015693196\n",
      "50 Validation Accuracy  0.00015647529\n",
      "54 Validation Accuracy  0.00015478504\n",
      "55 Validation Accuracy  0.00015462797\n",
      "57 Validation Accuracy  0.00015453687\n",
      "59 Validation Accuracy  0.00015324197\n",
      "62 Validation Accuracy  0.00015285413\n",
      "69 Validation Accuracy  0.00015193253\n",
      "71 Validation Accuracy  0.00015145286\n",
      "72 Validation Accuracy  0.00015112678\n",
      "77 Validation Accuracy  0.00014926562\n",
      "86 Validation Accuracy  0.0001481453\n",
      "87 Validation Accuracy  0.00014743647\n",
      "91 Validation Accuracy  0.0001471921\n",
      "93 Validation Accuracy  0.00014684768\n",
      "101 Validation Accuracy  0.00014560035\n",
      "110 Validation Accuracy  0.00014462511\n",
      "113 Validation Accuracy  0.0001440201\n",
      "116 Validation Accuracy  0.00014359254\n",
      "121 Validation Accuracy  0.00014302612\n",
      "127 Validation Accuracy  0.00014301945\n",
      "130 Validation Accuracy  0.00014213596\n",
      "137 Validation Accuracy  0.00014189619\n",
      "140 Validation Accuracy  0.0001414544\n",
      "141 Validation Accuracy  0.00014143421\n",
      "145 Validation Accuracy  0.00014118383\n",
      "151 Validation Accuracy  0.0001406434\n",
      "154 Validation Accuracy  0.00014002975\n",
      "163 Validation Accuracy  0.00013937475\n",
      "166 Validation Accuracy  0.00013931737\n",
      "169 Validation Accuracy  0.00013891695\n",
      "176 Validation Accuracy  0.00013863147\n",
      "179 Validation Accuracy  0.00013836437\n",
      "183 Validation Accuracy  0.0001380774\n",
      "189 Validation Accuracy  0.00013760681\n",
      "194 Validation Accuracy  0.00013729808\n",
      "197 Validation Accuracy  0.00013709889\n",
      "202 Validation Accuracy  0.00013684003\n",
      "211 Validation Accuracy  0.00013654992\n",
      "213 Validation Accuracy  0.00013634963\n",
      "215 Validation Accuracy  0.00013634149\n",
      "229 Validation Accuracy  0.00013547522\n",
      "235 Validation Accuracy  0.00013522459\n",
      "243 Validation Accuracy  0.00013489385\n",
      "246 Validation Accuracy  0.00013472138\n",
      "258 Validation Accuracy  0.00013418825\n",
      "272 Validation Accuracy  0.00013331017\n",
      "282 Validation Accuracy  0.00013318726\n",
      "286 Validation Accuracy  0.00013274646\n",
      "291 Validation Accuracy  0.00013261414\n",
      "292 Validation Accuracy  0.00013253126\n",
      "306 Validation Accuracy  0.00013239832\n",
      "307 Validation Accuracy  0.000131938\n",
      "316 Validation Accuracy  0.00013179454\n",
      "318 Validation Accuracy  0.00013165922\n",
      "322 Validation Accuracy  0.00013152587\n",
      "330 Validation Accuracy  0.00013129394\n",
      "335 Validation Accuracy  0.00013128172\n",
      "339 Validation Accuracy  0.0001311222\n",
      "347 Validation Accuracy  0.00013074286\n",
      "358 Validation Accuracy  0.00013052809\n",
      "361 Validation Accuracy  0.00013022676\n",
      "365 Validation Accuracy  0.00013010832\n",
      "390 Validation Accuracy  0.00012942511\n",
      "400 Validation Accuracy  0.00012922249\n",
      "413 Validation Accuracy  0.00012897569\n",
      "428 Validation Accuracy  0.00012877183\n",
      "436 Validation Accuracy  0.00012869823\n",
      "439 Validation Accuracy  0.00012831885\n",
      "444 Validation Accuracy  0.00012821908\n",
      "459 Validation Accuracy  0.00012818328\n",
      "461 Validation Accuracy  0.00012802878\n",
      "470 Validation Accuracy  0.00012772132\n",
      "474 Validation Accuracy  0.00012771192\n",
      "488 Validation Accuracy  0.0001276183\n",
      "495 Validation Accuracy  0.0001273943\n",
      "500 Validation Accuracy  0.00012727108\n",
      "504 Validation Accuracy  0.00012724347\n",
      "519 Validation Accuracy  0.00012706559\n",
      "526 Validation Accuracy  0.00012680872\n",
      "533 Validation Accuracy  0.00012675469\n",
      "549 Validation Accuracy  0.00012662046\n",
      "557 Validation Accuracy  0.00012652436\n",
      "561 Validation Accuracy  0.00012647988\n",
      "573 Validation Accuracy  0.00012621818\n",
      "580 Validation Accuracy  0.00012616259\n",
      "582 Validation Accuracy  0.00012610707\n",
      "591 Validation Accuracy  0.00012606633\n",
      "604 Validation Accuracy  0.00012592305\n",
      "623 Validation Accuracy  0.00012570292\n",
      "624 Validation Accuracy  0.00012566663\n",
      "648 Validation Accuracy  0.00012553655\n",
      "656 Validation Accuracy  0.00012541153\n",
      "662 Validation Accuracy  0.00012535413\n",
      "666 Validation Accuracy  0.00012533099\n",
      "668 Validation Accuracy  0.00012530979\n",
      "669 Validation Accuracy  0.00012528841\n",
      "672 Validation Accuracy  0.00012522902\n",
      "683 Validation Accuracy  0.00012522217\n",
      "689 Validation Accuracy  0.00012512553\n",
      "712 Validation Accuracy  0.00012508858\n",
      "721 Validation Accuracy  0.00012486348\n",
      "750 Validation Accuracy  0.00012474865\n",
      "782 Validation Accuracy  0.00012458612\n",
      "802 Validation Accuracy  0.00012449802\n",
      "809 Validation Accuracy  0.0001243674\n",
      "820 Validation Accuracy  0.00012430821\n",
      "842 Validation Accuracy  0.00012420409\n",
      "852 Validation Accuracy  0.00012415601\n",
      "872 Validation Accuracy  0.0001241361\n",
      "885 Validation Accuracy  0.00012405103\n",
      "901 Validation Accuracy  0.00012403121\n",
      "909 Validation Accuracy  0.00012398133\n",
      "976 Validation Accuracy  0.000123894\n",
      "983 Validation Accuracy  0.00012374805\n",
      "1018 Validation Accuracy  0.00012365966\n",
      "1067 Validation Accuracy  0.00012359153\n",
      "1082 Validation Accuracy  0.00012351108\n",
      "1115 Validation Accuracy  0.00012345488\n",
      "1120 Validation Accuracy  0.00012337681\n",
      "1138 Validation Accuracy  0.00012337333\n",
      "1192 Validation Accuracy  0.00012336065\n",
      "1239 Validation Accuracy  0.00012324739\n",
      "1245 Validation Accuracy  0.00012323269\n",
      "1273 Validation Accuracy  0.00012317522\n",
      "1288 Validation Accuracy  0.00012317415\n",
      "1353 Validation Accuracy  0.00012313349\n",
      "1360 Validation Accuracy  0.0001231099\n",
      "1431 Validation Accuracy  0.00012309548\n",
      "1440 Validation Accuracy  0.0001230904\n",
      "1463 Validation Accuracy  0.00012303563\n",
      "1511 Validation Accuracy  0.00012300238\n",
      "1538 Validation Accuracy  0.00012298673\n",
      "1615 Validation Accuracy  0.0001229391\n",
      "1777 Validation Accuracy  0.00012291323\n",
      "1787 Validation Accuracy  0.00012290826\n",
      "1808 Validation Accuracy  0.00012289651\n",
      "1881 Validation Accuracy  0.00012287925\n",
      "2035 Validation Accuracy  0.00012287768\n",
      "2102 Validation Accuracy  0.00012282653\n",
      "2277 Validation Accuracy  0.00012282185\n",
      "2290 Validation Accuracy  0.00012281614\n",
      "2425 Validation Accuracy  0.00012280753\n",
      "2586 Validation Accuracy  0.00012280127\n",
      "2765 Validation Accuracy  0.00012279746\n",
      "3042 Validation Accuracy  0.00012279728\n",
      "3196 Validation Accuracy  0.00012279117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-35ba5b92ee0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprev_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-35ba5b92ee0e>\u001b[0m in \u001b[0;36mevaluate_result\u001b[0;34m(model, valx, valy)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvaly\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mik\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mik\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For training the Regression Model using PyTorch\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "model  = RegCNND()\n",
    "init_weights(model)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "criterion          = nn.L1Loss(reduction='mean')#nn.MSELoss(reduction='mean')#nn.L1Loss(reduce=False, )#.MSELoss()\n",
    "optimizer_student  = optim.Adam(model.parameters(), lr=0.0001)#, weight_decay=0.01)\n",
    "\n",
    "def evaluate_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 100\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size]#.T\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size]\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = output.data.cpu().numpy()\n",
    "        \n",
    "        dt = np.abs(y - output)\n",
    "        dt = np.mean(dt)\n",
    "        \n",
    "        val_dice.append(dt)\n",
    "    return val_dice\n",
    "\n",
    "def train_model(model, batch_size, optimizer, criterion, trainx, trainy):\n",
    "    loss_array = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i in range(len(trainx)//batch_size):\n",
    "        x = trainx[i*batch_size:(i+1)*batch_size]#.T\n",
    "        y = trainy[i*batch_size:(i+1)*batch_size]#.T\n",
    "        \n",
    "        x = torch.tensor(x, device=device).float()\n",
    "        y = torch.tensor(y, device=device).float()\n",
    "        \n",
    "        #print(x.shape, y.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        #print(i, output.shape, y.shape)\n",
    "        \n",
    "        loss   = criterion(output , y)\n",
    "        #print(i, output.shape, loss.shape, y.shape)\n",
    "        #loss   = torch.mean(loss)\n",
    "        #print(loss.shape)\n",
    "        loss.backward()\n",
    "        \n",
    "        loss_array.append(loss.item())\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_array = np.mean(loss_array)\n",
    "    return loss_array\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)\n",
    "#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "prev_min = 1000\n",
    "\n",
    "for i in range(10000):\n",
    "    index = np.random.permutation(len(X_train))\n",
    "    \n",
    "    X_train = X_train[index]\n",
    "    y_train = y_train[index]\n",
    "    \n",
    "    train_model(model, 64, optimizer_student, criterion, X_train, y_train)\n",
    "    result = evaluate_result(model, X_test, y_test)\n",
    "    \n",
    "    if np.mean(result) < prev_min:\n",
    "        prev_min = np.mean(result)\n",
    "        print(i, 'Validation Accuracy ', np.mean(result))\n",
    "        torch.save(model.state_dict(), 'conv_prior_1-out_8-filter_loss.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 8  Filters: 944 Validation Accuracy  0.00019152784\n",
    "# 16 Filters: 191 Validation Accuracy  0.00016972028\n",
    "\n",
    "\n",
    "def predict_result(model, valx, valy):\n",
    "    model.eval()\n",
    "    \n",
    "    val_dice       = []\n",
    "    batch_size     = 1\n",
    "    \n",
    "    result       = []\n",
    "    ground_truth = []\n",
    "    inputx       = []\n",
    "    for ik in range(len(valx)//batch_size):\n",
    "        x = valx[ik*batch_size:(ik+1)*batch_size]#.T\n",
    "        y = valy[ik*batch_size:(ik+1)*batch_size]#.T\n",
    "\n",
    "        x = torch.tensor(x, device=device).float()\n",
    "\n",
    "        output = model.forward(x)\n",
    "        output = output.data.cpu().numpy()\n",
    "        \n",
    "        inputx.append(x.data.cpu().numpy())\n",
    "        result.append(output)\n",
    "        ground_truth.append(y)\n",
    "    \n",
    "    return inputx, result, ground_truth\n",
    "\n",
    "inputx, result, ground_truth = predict_result(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0274774  0.02770412 0.0278193  0.02821443 0.02832329 0.02766821\n",
      " 0.02784521 0.02813914 0.02809513 0.02810374]\n",
      "[0.0276094  0.02784416 0.02812788 0.02816381 0.02794264 0.02756655\n",
      " 0.02751802 0.02824509 0.02800724 0.02815021]\n",
      "[0.02745346 0.0275628  0.02774386 0.02795756 0.02773485 0.02730051\n",
      " 0.02720966 0.02798974 0.02773749 0.02789255]\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(ground_truth)-1)\n",
    "\n",
    "print(ground_truth[index].flatten()[:10])\n",
    "print(result[index].flatten()[:10])\n",
    "print(inputx[index].flatten()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'conv_prior_1-out_8-filter_0.0001368996_loss.pt')\n",
    "\n",
    "torch.save(model.state_dict(), 'conv_prior_1-out_32-filter_0.0001262981_loss.pt')\n",
    "\n",
    "\n",
    "# 195 -> 0.0056678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1500  Validation Accuracy  0.005796282 -> 500       (Per voxel)\n",
    "# 1500  Validation Accuracy  0.005799574 -> 125       (Per voxel)\n",
    "# 1500  Validation Accuracy  0.006036232 ->  25       (Per voxel)\n",
    "# 10000 Validation Accuracy  0.00512195  -> 500       (Per voxel) \n",
    "# 23427 Validation Accuracy  0.00366580  -> 125       (Per voxel)\n",
    "# 1639  Validation Accuracy  0.00320234  -> 125 x 125 (Per voxel)\n",
    "# 10100 Validation Accuracy  0.00266177  -> 125 x 125 (Per voxel)\n",
    "\n",
    "# conv_prior_1-out_8-filter_0.0001368996_loss.pt -> 16 filter with padding -> all out\n",
    "# 1500  Validation Accuracy  0.00013689  -> 125       (All voxels)\n",
    "\n",
    "# 2501  Validation Accuracy  0.00012629  -> 125\n",
    "\n",
    "\n",
    "# conv_prior_1-out_16-filter_loss.pt\n",
    "# 3196 Validation Accuracy  0.00012279117 -> 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64165, 125) (21389, 125) (64165,) (21389,)\n"
     ]
    }
   ],
   "source": [
    "# For training the Regression Model\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X, y = make_regression(n_samples=200, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "#X_train = X_train*100\n",
    "#X_test  = X_test*100\n",
    "#y_train = y_train*100\n",
    "#y_test  = y_test*100\n",
    "\n",
    "regr   = MLPRegressor(activation='tanh', \n",
    "                      random_state=0, \n",
    "                      solver='adam',\n",
    "                      learning_rate_init=0.0001,\n",
    "                      #learning_rate='adaptive', \n",
    "                      #hidden_layer_sizes=(125,25,),\n",
    "                      hidden_layer_sizes=(125,),\n",
    "                      #verbose=True,\n",
    "                      tol=0.00001,\n",
    "                      alpha=0.0001, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "result = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('Score is ', r2_score(result, y_test))\n",
    "#print(result.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "error = result - y_test\n",
    "error = np.square(error)\n",
    "print('Error is ', np.mean(error)*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 25) (25, 1) (25,) (1,)\n"
     ]
    }
   ],
   "source": [
    "print(regr.coefs_[0].shape, regr.coefs_[1].shape, regr.intercepts_[0].shape, regr.intercepts_[1].shape)\n",
    "\n",
    "np.save('mlp_coefs0_0.npy',      regr.coefs_[0])\n",
    "np.save('mlp_coefs0_1.npy',      regr.coefs_[1])\n",
    "np.save('mlp_intercepts0_0.npy', regr.intercepts_[0])\n",
    "np.save('mlp_intercepts0_1.npy', regr.intercepts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 125) (125,) (125, 125)\n"
     ]
    }
   ],
   "source": [
    "print(regr.coefs_[0].shape, X_test[0].shape, temp_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n",
      "[ 2.25638378e-02  5.97776647e-05  2.04450502e-03 -1.37875284e-03\n",
      "  7.12371572e-04]\n"
     ]
    }
   ],
   "source": [
    "#temp_mat = regr.coefs_[0]\n",
    "temp_ans = np.matmul(X_test[0], temp_mat)\n",
    "print(temp_ans.shape)\n",
    "print(temp_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n",
      "[ 2.25638378e-02  5.97776647e-05  2.04450502e-03 -1.37875284e-03\n",
      "  7.12371572e-04]\n"
     ]
    }
   ],
   "source": [
    "#temp_mat = regr.coefs_[0]\n",
    "temp_ans = np.dot(X_test[0], temp_mat)\n",
    "print(temp_ans.shape)\n",
    "print(temp_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) (2, 2)\n",
      "(1, 2)\n",
      "[[ 7 10]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2]])\n",
    "b = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "print(a.shape, b.shape)\n",
    "c = np.dot(a, b)\n",
    "print(c.shape)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02256383776164464, 5.9777664652542646e-05, 0.0020445050174381827, -0.0013787528351919486, 0.0007123715720690025]\n"
     ]
    }
   ],
   "source": [
    "temp_ans = [0]*125\n",
    "\n",
    "for i in range(125):\n",
    "    ans  = 0\n",
    "    for j in range(125):\n",
    "        ans = ans + X_test[0][j]*temp_mat[j][i]\n",
    "    temp_ans[i] = ans\n",
    "print(temp_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1396, 125) (125, 1) (1,)\n",
      "(1396, 1)\n",
      "[[0.03706854]\n",
      " [0.02758618]\n",
      " [0.03662279]\n",
      " [0.03742097]\n",
      " [0.03690861]]\n"
     ]
    }
   ],
   "source": [
    "#print(intercepts[0].shape)\n",
    "#print(np.dot(X_test, c[0]).shape)\n",
    "tp1 = np.dot(X_test, regr.coefs_[0])\n",
    "tp2 = tp1 + regr.intercepts_[0]\n",
    "tp3 = np.tanh(tp2)\n",
    "\n",
    "print(tp3.shape, regr.coefs_[1].shape, regr.intercepts_[1].shape)\n",
    "\n",
    "tp4 = np.dot(tp3, regr.coefs_[1])\n",
    "tp4 = tp4 + regr.intercepts_[1]\n",
    "\n",
    "#print(intercepts[0][:5])\n",
    "#print(tp1[0][:5])\n",
    "#print(tp2[0][:5])\n",
    "print(tp4.shape)\n",
    "print(tp4[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 25) (25, 1)\n",
      "(25,) (1,)\n"
     ]
    }
   ],
   "source": [
    "mlp_coefs_0_orig      = np.load('/home/pranjal/SEMISUNET/mlp_coefs0_0.npy')\n",
    "mlp_coefs_1_orig      = np.load('/home/pranjal/SEMISUNET/mlp_coefs0_1.npy')\n",
    "mlp_intercepts_0_orig = np.load('/home/pranjal/SEMISUNET/mlp_intercepts0_0.npy')\n",
    "mlp_intercepts_1_orig = np.load('/home/pranjal/SEMISUNET/mlp_intercepts0_1.npy')\n",
    "\n",
    "print(mlp_coefs_0_orig.shape, mlp_coefs_1_orig.shape)\n",
    "print(mlp_intercepts_0_orig.shape, mlp_intercepts_1_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1396, 1)\n"
     ]
    }
   ],
   "source": [
    "# Forward propagate\n",
    "\n",
    "activation = X_test\n",
    "for i in range(regr.n_layers_ - 1):\n",
    "    activation = np.dot(activation, regr.coefs_[i])\n",
    "    activation += regr.intercepts_[i]\n",
    "    if i != regr.n_layers_ - 2:\n",
    "        activation = np.tanh(activation)\n",
    "\n",
    "print(activation.shape)\n",
    "#output_activation = ACTIVATIONS[self.out_activation_]\n",
    "#output_activation(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81aab3d5c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Understanding Numba Code\n",
    "\n",
    "import numpy\n",
    "from numba import cuda\n",
    "\n",
    "def escape_time(p, maxtime):\n",
    "    \"\"\"Perform the Mandelbrot iteration until it's clear that p diverges\n",
    "    or the maximum number of iterations has been reached.\n",
    "    \"\"\"\n",
    "    z = 0j\n",
    "    for i in range(maxtime):\n",
    "        z = z ** 2 + p\n",
    "        if abs(z) > 2:\n",
    "            return i\n",
    "    return maxtime\n",
    "\n",
    "escape_time_gpu = cuda.jit(device=True)(escape_time)\n",
    "\n",
    "@cuda.jit\n",
    "def mandelbrot_gpu(M, real_min, real_max, imag_min, imag_max):\n",
    "    \"\"\"Calculate the Mandelbrot set on the GPU.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M : numpy.ndarray\n",
    "        a two-dimensional integer array that will contain the \n",
    "        escape times for each point.\n",
    "    real_min: float\n",
    "        minimum value on the real axis\n",
    "    real_max: float\n",
    "        maximum value on the real axis\n",
    "    imag_min: float\n",
    "        minimum value on the imaginary axis\n",
    "    imag_max: float\n",
    "        maximum value on the imaginary axis\n",
    "    \"\"\"\n",
    "    ny, nx = M.shape\n",
    "    i, j = cuda.grid(2)\n",
    "    \n",
    "    if i < ny and j < nx:\n",
    "        dx = (real_max - real_min) / nx\n",
    "        dy = (imag_max - imag_min) / ny\n",
    "        p = real_min + dx * i + (imag_min + dy * j) * 1j\n",
    "        M[i, j] = j#escape_time_gpu(p, 20)\n",
    "        \n",
    "        \n",
    "M = numpy.zeros((1024, 1024), dtype=numpy.int32)\n",
    "block = (32, 32)\n",
    "grid = (M.shape[0] // block[0] if M.shape[0] % block[0] == 0 \n",
    "            else M.shape[0] // block[0] + 1,\n",
    "        int(M.shape[0] // block[1] if M.shape[1] % block[1] == 0 \n",
    "            else M.shape[1] // block[1] + 1))\n",
    "\n",
    "\n",
    "mandelbrot_gpu[grid, block](M, -2.0, 2.0, -1.6, 1.6)\n",
    "\n",
    "plt.imshow(M, interpolation=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 125)\n",
      "(1, 25) (25, 1) (1,)\n",
      "Predicted Value From Model is  [[0.02117195]]\n",
      "Ground Truth Value is          0.021088189\n",
      "0.0209226\n"
     ]
    }
   ],
   "source": [
    "# Predicted Value from Regression Model is\n",
    "\n",
    "a = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3200x1280x58.0_0_0.0005_0_1_anistropic_normal8_1.raw', dtype='float32')\n",
    "a = np.reshape(a, [58, 1280, 3200])\n",
    "\n",
    "d = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3200x1280x58.0_0_0.0005_-0.1_1_anistropic_three8_1.raw', dtype='float32')\n",
    "d = np.reshape(d, [58, 1280, 3200])\n",
    "\n",
    "Y_array = d\n",
    "X_array = a\n",
    "\n",
    "ind_z = 43\n",
    "ind_y = 1000\n",
    "ind_x = 2200\n",
    "\n",
    "tx = X_array[ind_z-2:ind_z+3, ind_y-2:ind_y+3, ind_x-2:ind_x+3]\n",
    "ty = Y_array[ind_z,           ind_y, ind_x]\n",
    "\n",
    "X_test = np.array([tx.flatten()])\n",
    "print(X_test.shape)\n",
    "\n",
    "tp1 = np.dot(X_test, regr.coefs_[0])\n",
    "tp2 = tp1 + regr.intercepts_[0]\n",
    "tp3 = np.tanh(tp2)\n",
    "\n",
    "print(tp3.shape, regr.coefs_[1].shape, regr.intercepts_[1].shape)\n",
    "\n",
    "tp4 = np.dot(tp3, regr.coefs_[1])\n",
    "tp4 = tp4 + regr.intercepts_[1]\n",
    "\n",
    "print('Predicted Value From Model is ', tp4)\n",
    "print('Ground Truth Value is         ', ty)\n",
    "print(X_array[ind_z,           ind_y, ind_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.fromfile('/media/pranjal/BackupPlus/REAL-DBT-PROJECTIONS/RECONS-HUBER/xiaoyu_data-LE-L-CC_3200x1280x58.0_0_0.0005_0_1_anistropic_result8_a_1.raw', dtype='float32')\n",
    "e = np.reshape(e, [58, 1280, 3200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039948188\n",
      "0.04013158\n",
      "0.00018339232\n"
     ]
    }
   ],
   "source": [
    "print(e[27, 1152, 1315])\n",
    "print(d[27, 1152, 1315])\n",
    "print(d[27, 1152, 1315] - e[27, 1152, 1315])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import njit, prange\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5002435831882374 0.5002436\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of the volume\n",
    "IMGSIZx = 3200\n",
    "IMGSIZy = 1280\n",
    "IMGSIZz = 58\n",
    "\n",
    "# Cuda Kernel to calculate mean of neighboring voxels for each voxel\n",
    "@cuda.jit(debug=True)\n",
    "def calculate_mean(outbuf, inbuf):\n",
    "    # Calculate the index of the voxel being considered\n",
    "    \n",
    "    ind_x, ind_y, ind_z = cuda.grid(3)\n",
    "    #ind_x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    #ind_y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "    #ind_z = cuda.blockIdx.z * cuda.blockDim.z + cuda.threadIdx.z\n",
    "    \n",
    "    if ind_x < 0 or  ind_y < 0 or  ind_z < 0:\n",
    "        return\n",
    "    \n",
    "    if ind_x > IMGSIZx-1 or ind_y > IMGSIZy-1 or ind_z > IMGSIZz-1:\n",
    "        return\n",
    "    \n",
    "    sum1    = 0.0\n",
    "    counter = 0\n",
    "    for ind_nr_z in range(ind_z-2, ind_z+3):\n",
    "        for ind_nr_y in range(ind_y-2, ind_y+3):\n",
    "            for ind_nr_x in range(ind_x-2, ind_x+3):\n",
    "                if ind_nr_x<0 or ind_nr_y<0 or ind_nr_z<0:\n",
    "                    continue\n",
    "                \n",
    "                if ind_nr_x>(IMGSIZx-1) or ind_nr_y>(IMGSIZy-1) or ind_nr_z>(IMGSIZz-1):\n",
    "                    continue\n",
    "                \n",
    "                sum1    = sum1 + inbuf[ind_nr_z, ind_nr_y, ind_nr_x]\n",
    "                counter = counter+1\n",
    "    \n",
    "    outbuf[ind_z, ind_y, ind_x] = sum1/counter\n",
    "    return\n",
    "\n",
    "\n",
    "# Create random input matrix for which to calculate voxel wise mean\n",
    "inbuf = np.random.rand(58, 1280, 3200)\n",
    "# Create output matrix to store the output \n",
    "outbuf  = np.zeros((58, 1280, 3200), np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Code to Launch the Cuda kernel in Numba\n",
    "# 8*400 = 3200 = IMGSIZx\n",
    "# 8*160 = 1280 = IMGSIZy\n",
    "# 2*29  = 58   = IMGSIZz\n",
    "THREADS_PER_BLOCK = (8, 8, 2)\n",
    "BLOCKS_PER_GRID   = (400, 160, 29)\n",
    "calculate_mean[BLOCKS_PER_GRID, THREADS_PER_BLOCK](outbuf, inbuf)\n",
    "cuda.synchronize()\n",
    "\n",
    "\n",
    "\n",
    "# Checking result for a random voxel (c_z, c_y, c_x)\n",
    "c_x = random.randint(0, IMGSIZx)\n",
    "c_y = random.randint(0, IMGSIZy)\n",
    "c_z = random.randint(0, IMGSIZz)\n",
    "result_manual = np.mean(inbuf[c_z-2:c_z+3,c_y-2:c_y+3,c_x-2:c_x+3])\n",
    "result_cuda   = outbuf[c_z, c_y, c_x]\n",
    "print(result_manual, result_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4530233603179261 0.45302337\n"
     ]
    }
   ],
   "source": [
    "# Checking result for a random voxel (c_z, c_y, c_x)\n",
    "c_x = random.randint(0, IMGSIZx)\n",
    "c_y = random.randint(0, IMGSIZy)\n",
    "c_z = random.randint(0, IMGSIZz)\n",
    "result_manual = np.mean(inbuf[c_z-2:c_z+3,c_y-2:c_y+3,c_x-2:c_x+3])\n",
    "result_cuda   = outbuf[c_z, c_y, c_x]\n",
    "print(result_manual, result_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 542 22\n"
     ]
    }
   ],
   "source": [
    "print(c_x, c_y, c_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_manual = np.mean(inbuf[c_z-2:c_z+3,c_y-2:c_y+3,c_x-2:c_x+3].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4530233603179261\n"
     ]
    }
   ],
   "source": [
    "print(result_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
