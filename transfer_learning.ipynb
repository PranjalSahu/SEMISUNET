{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [OLD] Code for classification\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/yu-hao/WindowsData/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Attribute and Category Model\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class MyAttrCateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model    = models.resnet18(pretrained=True)\n",
    "        self.model.fc = Identity()\n",
    "        \n",
    "        self.attr_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(128, 26, bias=False)\n",
    "                                       )\n",
    "        \n",
    "        self.cate_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(128, 50, bias=False))\n",
    "        #self.cate_layer = nn.Linear(512, 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1     = self.model(x)\n",
    "        attr_out = self.attr_layer(out1)\n",
    "        cate_out = self.cate_layer(out1)\n",
    "        #cate_out = torch.flatten(cate_out)\n",
    "        return attr_out, cate_out\n",
    "\n",
    "#model  = MyAttrCateModel()\n",
    "# x      = torch.randn(1, 3, 224, 224)\n",
    "# output = model(x)\n",
    "# print(output[0].shape, output[1].shape)\n",
    "\n",
    "#print(model)\n",
    "#model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     0,
     15,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Data Loaders for Fashion Dataset\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class AttrDataset(Dataset):\n",
    "    CLASSES = None\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_path,\n",
    "                 img_file,\n",
    "                 label_file,\n",
    "                 cate_file,\n",
    "                 bbox_file,\n",
    "                 landmark_file,\n",
    "                 img_size,\n",
    "                 idx2id=None):\n",
    "        self.img_path = img_path\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        # read img names\n",
    "        fp = open(img_file, 'r')\n",
    "        self.img_list = [x.strip() for x in fp]\n",
    "\n",
    "        # read attribute labels and category annotations\n",
    "        self.labels = np.loadtxt(label_file, dtype=np.float32)\n",
    "\n",
    "        # read categories\n",
    "        self.categories = []\n",
    "        catefn = open(cate_file).readlines()\n",
    "        for i, line in enumerate(catefn):\n",
    "            self.categories.append(line.strip('\\n'))\n",
    "\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def get_basic_item(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_path,\n",
    "                                      self.img_list[idx])).convert('RGB')\n",
    "\n",
    "        width, height  = img.size\n",
    "        # Very Important\n",
    "        # For getting the cropped and resized region of interest image\n",
    "        img.thumbnail(self.img_size, Image.ANTIALIAS)\n",
    "        img   = self.transform(img)\n",
    "\n",
    "        label    = torch.from_numpy(self.labels[idx])\n",
    "        cate     = torch.LongTensor([int(self.categories[idx]) - 1])\n",
    "\n",
    "        data = {'img': img, 'attr': label, 'cate': cate}\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_basic_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "img_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "img_size   = [224, 224]\n",
    "\n",
    "landmark_file = None\n",
    "bbox_file     = None\n",
    "\n",
    "d1 = AttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_cate.txt\"\n",
    "\n",
    "d2 = AttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_dataloader(dataset, batch_size, shuffle):\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=1,\n",
    "        pin_memory=False)\n",
    "    return data_loader\n",
    "\n",
    "train_data_loader = build_dataloader(d1, 4, True)\n",
    "val_data_loader   = build_dataloader(d2, 4, False)\n",
    "\n",
    "model  = MyAttrCateModel()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0,
     16,
     35
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model  1000 3.690223619222641\n",
      "Epoch >>  0 3.690223619222641 1.5942994530200958 2.09592416703701\n",
      "Saving the model  3.690223619222641 3.608524953722954\n",
      "Epoch >>  1 3.608524953722954 1.566652411341667 2.0418725418448447\n",
      "Saving the model  3.608524953722954 3.5421543832619986\n",
      "Epoch >>  2 3.5421543832619986 1.5386451533635457 2.003509228348732\n",
      "Saving the model  3.5421543832619986 3.488498245000839\n",
      "Epoch >>  3 3.488498245000839 1.5169405973255634 1.9715576469898224\n",
      "Saving the model  3.488498245000839 3.4378772028446196\n",
      "Epoch >>  4 3.4378772028446196 1.4969851865530015 1.940892015659809\n",
      "Saving the model  3.4378772028446196 3.402476228872935\n",
      "Epoch >>  5 3.402476228872935 1.4818117643793425 1.9206644642055035\n",
      "Saving the model  3.402476228872935 3.3707665661403112\n",
      "Epoch >>  6 3.3707665661403112 1.4675029742547443 1.9032635923198291\n",
      "Saving the model  3.3707665661403112 3.3418311989605427\n",
      "Epoch >>  7 3.3418311989605427 1.454921459555626 1.8869097400642931\n",
      "Saving the model  3.3418311989605427 3.311650281826655\n",
      "Epoch >>  8 3.311650281826655 1.4432479335665702 1.8684023494223754\n",
      "Saving the model  3.311650281826655 3.2819537463665007\n",
      "Epoch >>  9 3.2819537463665007 1.4314837447106838 1.850470002141595\n",
      "Saving the model  3.2819537463665007 3.2631696780161423\n",
      "Epoch >>  10 3.2631696780161423 1.4236098792498761 1.8395597990236499\n",
      "Saving the model  3.2631696780161423 3.245571951766809\n",
      "Epoch >>  11 3.245571951766809 1.4163582540651163 1.8292136979649465\n",
      "Saving the model  3.245571951766809 3.229203643624599\n",
      "Epoch >>  12 3.229203643624599 1.4095696493249674 1.8196339948865083\n",
      "Saving the model  3.229203643624599 3.212003877835614\n",
      "Epoch >>  13 3.212003877835614 1.4008138574234077 1.8111900208933014\n",
      "Saving the model  3.212003877835614 3.1964806222200393\n",
      "Epoch >>  14 3.1964806222200393 1.3940053141713142 1.802475308416287\n",
      "Saving the model  3.1964806222200393 3.1829317552521825\n",
      "Epoch >>  15 3.1829317552521825 1.3876769999861718 1.7952547555621714\n",
      "Saving the model  3.1829317552521825 3.170116834324949\n",
      "Epoch >>  16 3.170116834324949 1.3823158762454986 1.7878009584248067\n",
      "Saving the model  3.170116834324949 3.156915177173085\n",
      "Epoch >>  17 3.156915177173085 1.3770114438997374 1.7799037336872683\n",
      "Saving the model  3.156915177173085 3.1455797762055147\n",
      "Epoch >>  18 3.1455797762055147 1.37174651396902 1.773833262722743\n",
      "Saving the model  3.1455797762055147 3.134601428836584\n",
      "Epoch >>  19 3.134601428836584 1.3674886858582496 1.7671127436839045\n",
      "Saving the model  3.134601428836584 3.121715729469345\n",
      "Epoch >>  20 3.121715729469345 1.3623869955397787 1.7593287346086333\n",
      "Saving the model  3.121715729469345 3.1123576492558827\n",
      "Epoch >>  21 3.1123576492558827 1.3583890416622162 1.7539686082310297\n",
      "Saving the model  3.1123576492558827 3.104621760772622\n",
      "Epoch >>  22 3.104621760772622 1.354713615707729 1.7499081456149401\n",
      "Saving the model  3.104621760772622 3.097008789882064\n",
      "Epoch >>  23 3.097008789882064 1.3510903546114763 1.745918435877189\n",
      "Saving the model  3.097008789882064 3.0884390615224837\n",
      "Epoch >>  24 3.0884390615224837 1.3474177620315553 1.7410213001424075\n",
      "Saving the model  3.0884390615224837 3.0801430908212293\n",
      "Epoch >>  25 3.0801430908212293 1.3433276189130086 1.7368154723541094\n",
      "Saving the model  3.0801430908212293 3.0710960626381416\n",
      "Epoch >>  26 3.0710960626381416 1.3395453725324737 1.7315506904864753\n",
      "Saving the model  3.0710960626381416 3.062967847560133\n",
      "Epoch >>  27 3.062967847560133 1.336024650788733 1.7269431969076394\n",
      "Saving the model  3.062967847560133 3.0574488722044846\n",
      "Epoch >>  28 3.0574488722044846 1.3328572386359345 1.724591633556218\n",
      "Saving the model  3.0574488722044846 3.050800193576018\n",
      "Epoch >>  29 3.050800193576018 1.3298304875234763 1.7209697060336668\n",
      "Saving the model  3.050800193576018 3.044411187537255\n",
      "Epoch >>  30 3.044411187537255 1.327019381846151 1.7173918055113284\n",
      "Saving the model  3.044411187537255 3.0382363016456364\n",
      "Epoch >>  31 3.0382363016456364 1.3242026358880103 1.7140336657105946\n",
      "Saving the model  3.0382363016456364 3.0330197792450586\n",
      "Epoch >>  32 3.0330197792450586 1.3217689072110437 1.7112508719274493\n",
      "Saving the model  3.0330197792450586 3.0269992950172986\n",
      "Epoch >>  33 3.0269992950172986 1.3190976835576926 1.7079016113298782\n",
      "Saving the model  3.0269992950172986 3.021316155239514\n",
      "Epoch >>  34 3.021316155239514 1.316245890544142 1.7050702645608358\n",
      "Saving the model  3.021316155239514 3.0162449255519443\n",
      "Epoch >>  35 3.0162449255519443 1.3132683647258414 1.70297656056823\n",
      "Saving the model  3.0162449255519443 3.010100736060658\n",
      "Epoch >>  36 3.010100736060658 1.3106170997377988 1.6994836360228223\n",
      "Saving the model  3.010100736060658 3.00560016350056\n",
      "Epoch >>  37 3.00560016350056 1.3083575628337107 1.6972426004225487\n",
      "Saving the model  3.00560016350056 3.0025788856377966\n",
      "Epoch >>  38 3.0025788856377966 1.3060921305693114 1.6964867548090525\n",
      "Saving the model  3.0025788856377966 2.999308513648808\n",
      "Epoch >>  39 2.999308513648808 1.303823759303987 1.6954847539205105\n",
      "Saving the model  2.999308513648808 2.996262274694152\n",
      "Epoch >>  40 2.996262274694152 1.302260743410122 1.6940015308348144\n",
      "Saving the model  2.996262274694152 2.993182394568409\n",
      "Epoch >>  41 2.993182394568409 1.3002124351163704 1.692969959006423\n",
      "Saving the model  2.993182394568409 2.9900986221959425\n",
      "Epoch >>  42 2.9900986221959425 1.2984986149973647 1.6916000067300574\n",
      "Saving the model  2.9900986221959425 2.9867623880952596\n",
      "Epoch >>  43 2.9867623880952596 1.296410829684951 1.6903515579150143\n",
      "Saving the model  2.9867623880952596 2.9834338530951077\n",
      "Epoch >>  44 2.9834338530951077 1.2941689165128603 1.6892649362734622\n",
      "Saving the model  2.9834338530951077 2.9804351158388283\n",
      "Epoch >>  45 2.9804351158388283 1.2924427731289811 1.6879923424881114\n",
      "Saving the model  2.9804351158388283 2.977723392589295\n",
      "Epoch >>  46 2.977723392589295 1.2908845539226177 1.6868388385241653\n",
      "Saving the model  2.977723392589295 2.9764033673194548\n",
      "Epoch >>  47 2.9764033673194548 1.289480891469245 1.6869224757410897\n",
      "Saving the model  2.9764033673194548 2.974965536739145\n",
      "Epoch >>  48 2.974965536739145 1.2884233014468027 1.6865422351726769\n",
      "Saving the model  2.974965536739145 2.9730709629118444\n",
      "Epoch >>  49 2.9730709629118444 1.2869817208081484 1.686089242014438\n",
      "Saving the model  2.9730709629118444 2.9709791342242093\n",
      "Epoch >>  50 2.9709791342242093 1.2855633476417438 1.6854157864661778\n",
      "Saving the model  2.9709791342242093 2.969787168286168\n",
      "Epoch >>  51 2.969787168286168 1.2842581048544783 1.6855290633240858\n",
      "Saving the model  2.969787168286168 2.9680352000751586\n",
      "Epoch >>  52 2.9680352000751586 1.282907078826765 1.6851281211717792\n",
      "Saving the model  2.9680352000751586 2.966042692821335\n",
      "Epoch >>  53 2.966042692821335 1.2814412245910476 1.6846014680647188\n",
      "Saving the model  2.966042692821335 2.965560338058255\n",
      "Epoch >>  54 2.965560338058255 1.2806258460711348 1.6849344918473201\n",
      "Saving the model  2.965560338058255 2.964510640960719\n",
      "Epoch >>  55 2.964510640960719 1.2795117810629308 1.6849988598469645\n",
      "Saving the model  2.964510640960719 2.963481616447892\n",
      "Epoch >>  56 2.963481616447892 1.2785246366790513 1.6849569796059738\n",
      "Saving the model  2.963481616447892 2.962763324343953\n",
      "Epoch >>  57 2.962763324343953 1.2774349032953896 1.68532842089801\n",
      "Saving the model  2.962763324343953 2.9622884700934766\n",
      "Epoch >>  58 2.9622884700934766 1.2764728053242473 1.6858156646833582\n",
      "Saving the model  2.9622884700934766 2.9603144721160333\n",
      "Epoch >>  59 2.9603144721160333 1.2753313992599646 1.684983072727422\n",
      "Saving the model  2.9603144721160333 2.9594103094525024\n",
      "Epoch >>  60 2.9594103094525024 1.2745615679465356 1.6848487413784519\n",
      "Saving the model  2.9594103094525024 2.9579786111952795\n",
      "Epoch >>  61 2.9579786111952795 1.273281654507883 1.6846969566898\n",
      "Saving the model  2.9579786111952795 2.957160056340316\n",
      "Epoch >>  62 2.957160056340316 1.2722157866812889 1.6849442696183448\n",
      "Saving the model  2.957160056340316 2.9568548511350525\n",
      "Epoch >>  63 2.9568548511350525 1.2714155879952014 1.685439263159875\n",
      "Saving the model  2.9568548511350525 2.9563207010094934\n",
      "Epoch >>  64 2.9563207010094934 1.270425736712951 1.6858949642733885\n",
      "Epoch >>  65 2.9571611528550132 1.269985068675695 1.687176084184737\n",
      "Epoch >>  66 2.9581281831131054 1.2695135935264736 1.688614589668032\n",
      "Epoch >>  67 2.9573702591473565 1.2686191381325616 1.688751121105955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  68 2.956665665594564 1.267494587417962 1.6891710782802623\n",
      "Epoch >>  69 2.9565264448804514 1.2667804375571865 1.6897460074224642\n",
      "Saving the model  2.9563207010094934 2.955813009125246\n",
      "Epoch >>  70 2.955813009125246 1.2659008245892087 1.6899121846052962\n",
      "Epoch >>  71 2.9559859699979425 1.2652232928305036 1.6907626771850304\n",
      "Epoch >>  72 2.956251369578381 1.2647901169853668 1.6914612525362676\n",
      "Epoch >>  73 2.957545646793939 1.2646458292531 1.6928998174896916\n",
      "Epoch >>  74 2.9586551445746423 1.2642840610369046 1.6943710833811263\n",
      "Epoch >>  75 2.9583848070917944 1.2633912478575582 1.6949935590262575\n",
      "Epoch >>  76 2.959081439802399 1.2628419205670234 1.6962395190867028\n",
      "Epoch >>  77 2.959024699697892 1.2622757109839182 1.696748988455066\n",
      "Epoch >>  78 2.9602744655903384 1.261748933518989 1.6985255317866708\n",
      "Epoch >>  79 2.960561617884785 1.2610631421845406 1.6994984753759113\n",
      "Epoch >>  80 2.9610111980857674 1.2605426958251147 1.7004685019631354\n",
      "Epoch >>  81 2.96111784760763 1.2599423796734432 1.7011754675955035\n",
      "Epoch >>  82 2.962355915501175 1.2594534729439093 1.7029024422278696\n",
      "Epoch >>  83 2.9633861395468313 1.2589614078693447 1.7044247313859828\n",
      "Epoch >>  84 2.963037255595011 1.2581291707747122 1.7049080845194267\n",
      "Epoch >>  85 2.9642117676201254 1.2578888930837775 1.7063228742341663\n",
      "Epoch >>  86 2.964659572263559 1.2575079064403458 1.7071516654653283\n",
      "Epoch >>  87 2.9657383722344584 1.2571188461678948 1.7086195256622\n",
      "Epoch >>  88 2.9672219457994684 1.2567054365397838 1.7105165087749783\n",
      "Epoch >>  89 2.9687321864379776 1.2565214887264702 1.712210697148906\n",
      "Epoch >>  90 2.969784383451546 1.256059034295462 1.713725348573219\n",
      "Epoch >>  91 2.9709804038897802 1.2557910782166797 1.7151893250934132\n",
      "Epoch >>  92 2.9715453679837207 1.255220971099792 1.716324396332105\n",
      "Epoch >>  93 2.9722058734817707 1.2548338854480932 1.7173719874493618\n",
      "Epoch >>  94 2.9737360245397215 1.254442900643537 1.7192931233211568\n",
      "Epoch >>  95 2.974745142954091 1.2541784135137375 1.7205667288679785\n",
      "Epoch >>  96 2.9766889342770133 1.2538478074820385 1.7228411263089973\n",
      "Epoch >>  97 2.9780127785278827 1.2534713666411688 1.7245414114094966\n",
      "Epoch >>  98 2.979217221429854 1.253481164626702 1.725736056252298\n",
      "Epoch >>  99 2.9816433584427835 1.2534083780238032 1.728234979887046\n",
      "Epoch >>  100 2.9824547432354183 1.252870135999847 1.7295846066622083\n",
      "Epoch >>  101 2.983332968991177 1.252411328548602 1.7309216398645704\n",
      "Epoch >>  102 2.9849279576331664 1.2522637168206636 1.7326642402893349\n",
      "Epoch >>  103 2.9856459586723494 1.2519072007760406 1.7337387573594012\n",
      "Epoch >>  104 2.9862473556518556 1.2516259378583658 1.7346214172339511\n",
      "Epoch >>  105 2.9873463400771034 1.251401349858574 1.7359449896877457\n",
      "Epoch >>  106 2.9879368755583453 1.2510668152322837 1.7368700597840492\n",
      "Epoch >>  107 2.9889865749776363 1.2507829492856506 1.7382036251296737\n",
      "Epoch >>  108 2.9902775091911673 1.2506017227208395 1.7396757859377139\n",
      "Epoch >>  109 2.991883795717088 1.2504984467866747 1.7413853483996609\n",
      "Epoch >>  110 2.9931065800066468 1.2503996366710812 1.7427069427730466\n",
      "Epoch >>  111 2.994623015749667 1.250184622443414 1.744438392711005\n",
      "Epoch >>  112 2.9970285859978305 1.2503853955814797 1.7466431898514914\n",
      "Epoch >>  113 2.9980702653914166 1.2501349350117277 1.7479353298120723\n",
      "Epoch >>  114 2.999876578638865 1.250120345978115 1.749756232146271\n",
      "Epoch >>  115 3.00167423858622 1.2501012104395135 1.7515730276152748\n",
      "Epoch >>  116 3.0034859739159927 1.2499090023667385 1.7535769710533766\n",
      "Epoch >>  117 3.005849189934589 1.2499803315956715 1.7558688578797914\n",
      "Epoch >>  118 3.007918247847497 1.2501468625724816 1.7577713847952041\n",
      "Epoch >>  119 3.0093651841883857 1.2500829829615852 1.7592822007260285\n",
      "Epoch >>  120 3.0103716703855303 1.2497561940789714 1.7606154758027517\n",
      "Epoch >>  121 3.0119754079301826 1.249824539102736 1.762150868276012\n",
      "Epoch >>  122 3.014204017566956 1.2500539125729868 1.7641501045088337\n",
      "Epoch >>  123 3.0158973667789852 1.2498576679861835 1.766039698323265\n",
      "Epoch >>  124 3.017874192025423 1.2498520475876331 1.768022143954158\n",
      "Epoch >>  125 3.0199067630713423 1.2500368014451058 1.7698699611424218\n",
      "Epoch >>  126 3.0216905801261036 1.250045653920474 1.771644925733251\n",
      "Epoch >>  127 3.0235796495361718 1.2500795871978625 1.7735000618795749\n",
      "Epoch >>  128 3.024833304240029 1.2500726530732111 1.774760650686804\n",
      "Epoch >>  129 3.0259135657771274 1.249957934888968 1.7759556304383737\n",
      "Epoch >>  130 3.0276598897255558 1.2498920966195697 1.7777677926639848\n",
      "Epoch >>  131 3.02999798576737 1.2498480019562623 1.7801499833389836\n",
      "Epoch >>  132 3.031828472921499 1.2498187103255798 1.782009762189162\n",
      "Epoch >>  133 3.033012986919551 1.2496679243340867 1.7833450621593354\n",
      "Epoch >>  134 3.03493670627254 1.2497409904923704 1.7851957153102451\n",
      "Epoch >>  135 3.0368854250002872 1.2497502203840105 1.7871352041775552\n",
      "Epoch >>  136 3.03794700308329 1.2494763623813208 1.7884706402578303\n",
      "Epoch >>  137 3.039415281591856 1.2495842895926772 1.7898309915267956\n",
      "Epoch >>  138 3.0406847623322935 1.2495990479654975 1.7910857138770948\n",
      "Epoch >>  139 3.041973647563585 1.2495280278435774 1.7924456192413611\n",
      "Epoch >>  140 3.0441004059720544 1.249690757223689 1.794409648197551\n",
      "Epoch >>  141 3.046531277727913 1.2499395743459463 1.7965917028417207\n",
      "Epoch >>  142 3.047554708577536 1.249865121333124 1.7976895867207614\n",
      "Epoch >>  143 3.048142668192585 1.2496079073941542 1.7985347602959956\n",
      "Epoch >>  144 3.0495446519177536 1.2495354323452916 1.8000092191071217\n",
      "Epoch >>  145 3.0510874528762413 1.2496018802495035 1.8014855721173808\n",
      "Epoch >>  146 3.051924019616883 1.2493456155207692 1.8025784035702774\n",
      "Epoch >>  147 3.054662748504732 1.2496911715607386 1.8049715764207162\n",
      "Epoch >>  148 3.0570456420843235 1.2498842778081862 1.8071613637248192\n",
      "Epoch >>  149 3.058956457085212 1.2501545018212001 1.80880195469367\n",
      "Epoch >>  150 3.061039348176773 1.2501832471150436 1.810856100480325\n",
      "Epoch >>  151 3.0626668930410554 1.2501785381265769 1.812488354324884\n",
      "Epoch >>  152 3.063444832671312 1.2500998902287748 1.8133449418652936\n",
      "Epoch >>  153 3.065508440035117 1.2501536553602715 1.8153547840591508\n",
      "Epoch >>  154 3.0673207282270156 1.250229141055384 1.8170915865639945\n",
      "Epoch >>  155 3.069248883687533 1.2503715803374846 1.8188773027581508\n",
      "Epoch >>  156 3.07036623066398 1.25043358955072 1.8199326405212413\n",
      "Epoch >>  157 3.0720821239657794 1.2506151826160996 1.821466940718249\n",
      "Epoch >>  158 3.074227292830839 1.250783478909506 1.823443813268593\n",
      "Epoch >>  159 3.0757842836886646 1.2508009259842336 1.824983357054839\n",
      "Epoch >>  160 3.076583720753652 1.2507517433290527 1.8258319767433167\n",
      "Epoch >>  161 3.0774678950515795 1.2508402995667707 1.8266275947845978\n",
      "Epoch >>  162 3.078992611427249 1.2510467056941035 1.827945905053868\n",
      "Epoch >>  163 3.080289010025924 1.251203275381065 1.8290857339691393\n",
      "Epoch >>  164 3.0815152656593106 1.2512242140152237 1.8302910509945882\n",
      "Epoch >>  165 3.0827986500728204 1.25120640989007 1.8315922395423032\n",
      "Epoch >>  166 3.084575151160627 1.2515026215305942 1.833072529013107\n",
      "Epoch >>  167 3.0858658298876667 1.2515918731442874 1.8342739561547614\n",
      "Epoch >>  168 3.0874326550972886 1.2517290394179215 1.8357036150879056\n",
      "Epoch >>  169 3.0895613243136335 1.2520801859938047 1.837481137778051\n",
      "Epoch >>  170 3.0907023128415756 1.2521958467841845 1.8385064655011247\n",
      "Epoch >>  171 3.092247618281044 1.2525274252491287 1.839720192456488\n",
      "Epoch >>  172 3.0943705422104784 1.252751618920723 1.8416189227095197\n",
      "Epoch >>  173 3.096166090196405 1.2530570674528336 1.8431090221677522\n",
      "Epoch >>  174 3.098029644634553 1.2534025755608083 1.8446270684502009\n",
      "Epoch >>  175 3.099622992756184 1.2535183658105407 1.8461046262875844\n",
      "Epoch >>  176 3.1017019993856465 1.2538749935499018 1.847827005156316\n",
      "Epoch >>  177 3.1031213865167975 1.2541763317949985 1.8489450540814831\n",
      "Epoch >>  178 3.1048137648393657 1.254451873522897 1.850361890650219\n",
      "Epoch >>  179 3.106848055238691 1.2546091213918396 1.8522389331922917\n",
      "Epoch >>  180 3.108726542239525 1.2548339913615865 1.8538925502214385\n",
      "Epoch >>  181 3.110610333231288 1.255072003277121 1.8555383292636731\n",
      "Epoch >>  182 3.1120606732236555 1.2552593471738485 1.8568013253413151\n",
      "Epoch >>  183 3.113461696576003 1.2554341348059799 1.8580275610734598\n",
      "Epoch >>  184 3.115602742733665 1.2556484507109669 1.8599542913431704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  185 3.1183290121749203 1.255982209787574 1.8623468017018856\n",
      "Epoch >>  186 3.1199017165288887 1.2561858778772825 1.8637158379604677\n",
      "Epoch >>  187 3.1212104504246345 1.2563359587546041 1.8648744909745825\n",
      "Epoch >>  188 3.1231963836769894 1.2567102808931832 1.86648610209735\n",
      "Epoch >>  189 3.1248500226041203 1.2569519213358038 1.8678981005730033\n",
      "Epoch >>  190 3.1268425415637293 1.2573487345006766 1.869493806370483\n",
      "Epoch >>  191 3.129013066782461 1.257776467700644 1.871236598355087\n",
      "Epoch >>  192 3.1309644850629907 1.2582828646543112 1.8726816196817437\n",
      "Epoch >>  193 3.1326689955458504 1.2585734769095158 1.8740955179115617\n",
      "Epoch >>  194 3.134626731304175 1.2589060999038892 1.8757206307012182\n",
      "Epoch >>  195 3.1357026468054676 1.2591399723872239 1.8765626737151024\n",
      "Epoch >>  196 3.1369296870175654 1.2592976545332955 1.877632031777323\n",
      "Epoch >>  197 3.1384940660479996 1.2594856305477595 1.8790084347860545\n",
      "Epoch >>  198 3.1405880488611047 1.259929964412996 1.8806580837469282\n",
      "Epoch >>  199 3.141605634094626 1.2600754331718385 1.8815302002473153\n",
      "Epoch >>  200 3.1433602457050958 1.2603468369676996 1.8830134080991274\n",
      "Epoch >>  201 3.1445813409938377 1.2605961055760042 1.8839852347733652\n",
      "Epoch >>  202 3.1460246082346135 1.2608068886689952 1.8852177189392625\n",
      "Epoch >>  203 3.1475731386520405 1.2611343810774531 1.886438756927462\n",
      "Epoch >>  204 3.1491515631645193 1.2613587391322705 1.8877928233914871\n",
      "Epoch >>  205 3.150823880608076 1.2616968894363607 1.889126990503218\n",
      "Epoch >>  206 3.1526266896075383 1.262144620001604 1.8904820689542179\n",
      "Epoch >>  207 3.1542045647037718 1.2623938153959238 1.8918107486313966\n",
      "Epoch >>  208 3.155269071664536 1.2625583149050696 1.8927107560635303\n",
      "Epoch >>  209 3.156550910325845 1.2627469185514109 1.8938039910847566\n",
      "Epoch >>  210 3.1582009853683943 1.263093218532713 1.8951077661327553\n",
      "Epoch >>  211 3.159460075756289 1.26329768046322 1.8961623945825992\n",
      "Epoch >>  212 3.1607347085316415 1.263436247297226 1.8972984605292318\n",
      "Epoch >>  213 3.1623653141786563 1.2637711751319678 1.8985941382981772\n",
      "Epoch >>  214 3.1634276658892633 1.2639824682253045 1.8994451969246424\n",
      "Epoch >>  215 3.16477379129082 1.2643603382537625 1.9004134523007326\n",
      "Epoch >>  216 3.1658790567259087 1.2645542192973438 1.9013248367008864\n",
      "Epoch >>  217 3.167825098166225 1.2650048216448055 1.9028202758188586\n",
      "Epoch >>  218 3.1690947061336203 1.2651870852897293 1.9039076201874214\n",
      "Epoch >>  219 3.170399284505573 1.2654426876599816 1.9049565961947559\n",
      "Epoch >>  220 3.171929229212023 1.2657184797425616 1.9062107487922741\n",
      "Epoch >>  221 3.17348238959103 1.265965494137358 1.9075168947874053\n",
      "Epoch >>  222 3.1748690084467555 1.26619035950557 1.9086786482861169\n",
      "Epoch >>  223 3.1759418950269795 1.2664715192900704 1.9094703750738242\n",
      "Epoch >>  224 3.177691006884045 1.2668759802683194 1.9108150259490726\n",
      "Epoch >>  225 3.179229289152717 1.2670840954743656 1.9121451930150604\n",
      "Epoch >>  226 3.1808504137215636 1.2674772999365425 1.9133731130982548\n",
      "Epoch >>  227 3.1829417684546164 1.2680048544963725 1.9149369132433682\n",
      "Epoch >>  228 3.1844465105619495 1.2682330801034079 1.9162134297424032\n",
      "Epoch >>  229 3.185966773320022 1.2686034374492323 1.9173633351615387\n",
      "Epoch >>  230 3.187361274625832 1.2689574435384243 1.91840383031647\n",
      "Epoch >>  231 3.1889759868912657 1.2693832788102586 1.919592707335937\n",
      "Epoch >>  232 3.1905484024419293 1.2696997847662592 1.9208486169330263\n",
      "Epoch >>  233 3.1920057691394264 1.26994343086867 1.922062337550072\n",
      "Epoch >>  234 3.1935387764195178 1.2703868509309724 1.9231519247740032\n",
      "Epoch >>  235 3.1951328625471915 1.2707049887472668 1.9244278730973523\n",
      "Epoch >>  236 3.1969092572904843 1.2711185413077788 1.9257907152702551\n",
      "Epoch >>  237 3.19870420059161 1.2714929304320772 1.9272112694494425\n",
      "Epoch >>  238 3.200400657961059 1.2718484671203163 1.9285521901327431\n",
      "Epoch >>  239 3.2021127660127977 1.2721526584705958 1.929960106815747\n",
      "Epoch >>  240 3.2034499635684046 1.2723642338030814 1.9310857290279166\n",
      "Epoch >>  241 3.2049855565751386 1.2727119336474286 1.932273622159777\n",
      "Epoch >>  242 3.2066833681694766 1.273117292553677 1.9335660748604901\n",
      "Epoch >>  243 3.208335177342178 1.273544824886884 1.93479035169363\n",
      "Epoch >>  244 3.210416857017546 1.2740577904061394 1.936359065889762\n",
      "Epoch >>  245 3.21168989574885 1.274255258216727 1.9374346368048403\n",
      "Epoch >>  246 3.213066229114407 1.2745333943192052 1.9385328340747305\n",
      "Epoch >>  247 3.2146444653847044 1.2749483674926863 1.9396960971786879\n",
      "Epoch >>  248 3.2159364534326826 1.2753197176364293 1.9406167350728916\n",
      "Epoch >>  249 3.2176359437515734 1.2757081746398211 1.941927768375582\n",
      "Epoch >>  250 3.2191244764358875 1.2760755918167264 1.9430488839006614\n",
      "Epoch >>  251 3.220508382845493 1.2764250049141663 1.9440833772142494\n",
      "Epoch >>  252 3.221581920371225 1.2768019068700522 1.9447800128108135\n",
      "Epoch >>  253 3.2231125105014 1.2771239199631326 1.945988589837851\n",
      "Epoch >>  254 3.2249364411293877 1.2775851647800676 1.947351275667699\n",
      "Epoch >>  255 3.226037428393087 1.2778833168126293 1.9481541108994325\n",
      "Epoch >>  256 3.227337477948009 1.2782750925483755 1.949062384711815\n",
      "Epoch >>  257 3.2287984934934926 1.2786923868927036 1.950106105901605\n",
      "Epoch >>  258 3.2301353252480043 1.279073051825626 1.9510622727502809\n",
      "Epoch >>  259 3.231800297724112 1.2795472380032333 1.9522530590489013\n",
      "Epoch >>  260 3.2331877723658793 1.2799371157740154 1.9532506559315108\n",
      "Epoch >>  261 3.234613133051525 1.2801665786960312 1.954446553680158\n",
      "Epoch >>  262 3.2360697541418753 1.2805767549874665 1.9554929985152885\n",
      "Epoch >>  263 3.2375492345930614 1.2809439557031024 1.95660527826929\n",
      "Epoch >>  264 3.238746620746772 1.2812013431689648 1.9575452769735873\n",
      "Epoch >>  265 3.2403449909460162 1.2815502339876013 1.9587947563870356\n",
      "Epoch >>  266 3.24121070678041 1.2817970669729433 1.9594136392468497\n",
      "Epoch >>  267 3.242227521415911 1.2820846384807087 1.960142882346159\n",
      "Epoch >>  268 3.242924468002521 1.2822867580191353 1.9606377093687724\n",
      "Epoch >>  269 3.244666437943059 1.28287578710428 1.9617906502428473\n",
      "Epoch >>  270 3.245868498806309 1.2831691477932174 1.962699350402894\n",
      "Epoch >>  271 3.247012712976019 1.283402577139492 1.9636101352367525\n",
      "Epoch >>  272 3.2490468103989905 1.2839040018893204 1.9651428079113693\n",
      "Epoch >>  273 3.2507934337257796 1.2844456030478442 1.966347830039562\n",
      "Epoch >>  274 3.2518347098499536 1.284755078061819 1.9670796311474905\n",
      "Epoch >>  275 3.2528178773523995 1.2850841621837539 1.9677337145138964\n",
      "Epoch >>  276 3.2542498695970337 1.2855712680860762 1.9686786008751942\n",
      "Epoch >>  277 3.2562991378649855 1.2860503829411305 1.9702487542921612\n",
      "Epoch >>  278 3.25766910078721 1.2863822210860776 1.9712868790914377\n",
      "Epoch >>  279 3.259102094534625 1.2868430412550325 1.972259052649231\n",
      "Epoch >>  280 3.260262294210308 1.2871901844043379 1.9730721091526102\n",
      "Epoch >>  281 3.261824601539959 1.2877372804093075 1.974087320482805\n",
      "Epoch >>  282 3.263070053104851 1.2881027544909305 1.9749672979803554\n",
      "Epoch >>  283 3.2648927528917473 1.288616682836786 1.9762760694402337\n",
      "Epoch >>  284 3.2666390739767697 1.2891087187872905 1.9775303545623406\n",
      "Epoch >>  285 3.268067366759521 1.2894391260829892 1.9786282400436759\n",
      "Epoch >>  286 3.269663420001233 1.2899685640786012 1.9796948552656446\n",
      "Epoch >>  287 3.271068709613186 1.290365646758453 1.9807030621866393\n",
      "Epoch >>  288 3.2724432595842954 1.2906951078712992 1.9817481510672457\n",
      "Epoch >>  289 3.2734597960289697 1.2909768067788718 1.9824829885878061\n",
      "Epoch >>  290 3.27455097481337 1.2912720207537418 1.9832789533617707\n",
      "Epoch >>  291 3.275937385336432 1.2917727561743344 1.9841646284612113\n",
      "Epoch >>  292 3.2774237615969097 1.2922814899709698 1.985142270932989\n",
      "Epoch >>  293 3.278883147253522 1.2926438597373056 1.9862392868281566\n",
      "Epoch >>  294 3.280424465494085 1.2931038104018164 1.9873206544139919\n",
      "Epoch >>  295 3.2814255464723887 1.2934358943619488 1.987989651415929\n",
      "Epoch >>  296 3.2827384596256795 1.2937947922337185 1.9889436667139873\n",
      "Epoch >>  297 3.284001541443369 1.2941781174779787 1.9898234232638927\n",
      "Epoch >>  298 3.285275622007091 1.2945205413721328 1.9907550799235099\n",
      "Epoch >>  299 3.2862970305280386 1.29487202326787 1.9914250065606778\n",
      "Epoch >>  300 3.2877830950420264 1.2953087009517508 1.9924743933977314\n",
      "Epoch >>  301 3.2893643296414754 1.295783815881595 1.9935805130491244\n",
      "Epoch >>  302 3.290674401763888 1.2961251942858887 1.994549206775872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  303 3.2925863926173924 1.296655050983677 1.9959313409524506\n",
      "Epoch >>  304 3.293929070736199 1.2970149592163378 1.9969141108452366\n",
      "Epoch >>  305 3.2953734824094996 1.2973535350293555 1.998019946678165\n",
      "Epoch >>  306 3.296748968723785 1.297738177779176 1.9990107902541752\n",
      "Epoch >>  307 3.2978866980891044 1.2980806862422833 1.9998060111515081\n",
      "Epoch >>  308 3.2994277167704595 1.2985902288428064 2.0008374871956502\n",
      "Epoch >>  309 3.3004029619853825 1.2989911819169357 2.001411779344612\n",
      "Epoch >>  310 3.3014572921917082 1.2993101093359651 2.002147182133727\n",
      "Epoch >>  311 3.302616946122633 1.29968542823365 2.002931517141753\n",
      "Epoch >>  312 3.3037245636942765 1.3000066302523208 2.0037179327139842\n",
      "Epoch >>  313 3.305548523691097 1.3005436904180534 2.0050048325501835\n",
      "Epoch >>  314 3.3063283862044415 1.3007308312591817 2.0055975542561932\n",
      "Epoch >>  315 3.307871043614053 1.3012348677059282 2.0066361752068684\n",
      "Epoch >>  316 3.3089042853914696 1.3015695364149729 2.0073347482890544\n",
      "Epoch >>  317 3.3099913407785806 1.301891070276897 2.0081002698260795\n",
      "Epoch >>  318 3.3111591106306144 1.3022064458008704 2.0089526641680346\n",
      "Epoch >>  319 3.3124273905585055 1.302658142998838 2.009769246905338\n",
      "Epoch >>  320 3.313444429176851 1.3029601024662454 2.010484326055073\n",
      "Epoch >>  321 3.3143165624312454 1.3032172499650059 2.011099311822681\n",
      "Epoch >>  322 3.3153276762108406 1.303582640832907 2.01174503474383\n",
      "Epoch >>  323 3.3165665377997047 1.3040501019063602 2.012516435255085\n",
      "Epoch >>  324 3.317488791076816 1.3044171722712654 2.013071618156638\n",
      "Epoch >>  325 3.3191031362608623 1.3049472831503883 2.0141558524355294\n",
      "Epoch >>  326 3.320377566541417 1.30522389115441 2.015153674727384\n",
      "Epoch >>  327 3.321319026378824 1.30555113184418 2.015767893871578\n",
      "Epoch >>  328 3.322287510373536 1.3057757733401512 2.0165117363731695\n",
      "Epoch >>  329 3.323658243589013 1.3062110760847954 2.0174471668660683\n",
      "Epoch >>  330 3.324827274727902 1.3065399601693826 2.0182873139082487\n",
      "Epoch >>  331 3.3258241002117566 1.3068911155199474 2.018932984041185\n",
      "Epoch >>  332 3.326840628234429 1.3071865557464617 2.019654071841744\n",
      "Epoch >>  333 3.328095277469806 1.307638519047107 2.020456757778539\n",
      "Epoch >>  334 3.329267277537664 1.308008571659829 2.0212587052257454\n",
      "Epoch >>  335 3.330638282583184 1.3084600121787573 2.022178269741998\n",
      "Epoch >>  336 3.3314631972151534 1.3086843990315449 2.0227787975057847\n",
      "Epoch >>  337 3.3324663886870063 1.3089445062067866 2.023521881802142\n",
      "Epoch >>  338 3.333637146073237 1.3093525813301712 2.024284564066867\n",
      "Epoch >>  339 3.3349570600212934 1.3098270346366965 2.0251300247176847\n",
      "Epoch >>  340 3.3361754265313537 1.3101990418280542 2.0259763840408715\n",
      "Epoch >>  341 3.337825089198935 1.3106626042770058 2.0271624842589735\n",
      "Epoch >>  342 3.3390146497341617 1.3110297677523237 2.02798488134194\n",
      "Epoch >>  343 3.339729873359853 1.3111996394493075 2.028530233277841\n",
      "Epoch >>  344 3.340626515136033 1.311479610537986 2.0291469039702323\n",
      "Epoch >>  345 3.341383051495116 1.3117460385447508 2.029637012301837\n",
      "Epoch >>  346 3.3426269989241697 1.3121186232155655 2.0305083750581474\n",
      "Epoch >>  347 3.3434713899158797 1.3123450838454367 2.031126305419307\n",
      "Epoch >>  348 3.345141813015656 1.3129016021199118 2.0322402102734105\n",
      "Epoch >>  349 3.3463848684604254 1.3133232230692464 2.033061644751079\n",
      "Epoch >>  350 3.347280376417727 1.3136692763903415 2.0336110993836645\n",
      "Epoch >>  351 3.348528206424322 1.3139891375742048 2.034539068227878\n",
      "Epoch >>  352 3.3494931890961555 1.3144173664468164 2.0350758220319967\n",
      "Epoch >>  353 3.3506093351944104 1.3147879504414728 2.0358213841428467\n",
      "Epoch >>  354 3.3517039363760763 1.3151794110737847 2.03652452468362\n",
      "Epoch >>  355 3.353346775922422 1.3157013635872916 2.037645411705184\n",
      "Epoch >>  356 3.3541486530899585 1.3160391945594123 2.0381094578944823\n",
      "Epoch >>  357 3.355201236139421 1.316359064103817 2.0388421714270972\n",
      "Epoch >>  358 3.356081492924383 1.316619741447602 2.039461750858836\n",
      "Epoch >>  359 3.357082657085318 1.3169396866041545 2.040142969885336\n",
      "Epoch >>  360 3.3580403037191346 1.3173152096119407 2.040725093509106\n",
      "Epoch >>  361 3.3587933283693143 1.3175933393195314 2.0411999884798875\n",
      "Epoch >>  362 3.3599861861974327 1.3179687651554046 2.042017420475909\n",
      "Epoch >>  363 3.361015790488325 1.3182955949968649 2.0427201949570417\n",
      "Epoch >>  364 3.361579358658031 1.3184547640254032 2.043124594094744\n",
      "Epoch >>  365 3.3623924409740202 1.3188174047736156 2.043575035684917\n",
      "Epoch >>  366 3.363676357291172 1.3191672105819434 2.0445091462091045\n",
      "Epoch >>  367 3.3648376604951515 1.3195369635797105 2.0453006964255254\n",
      "Epoch >>  368 3.3658202027432558 1.3198657810649455 2.045954421203473\n",
      "Epoch >>  369 3.3665472607407616 1.3201948030380986 2.046352457229474\n",
      "Epoch >>  370 3.3673810417956176 1.3204544960844589 2.0469265452152214\n",
      "Epoch >>  371 3.368246156015263 1.3207710506105494 2.047475104928185\n",
      "Epoch >>  372 3.3693012625653767 1.321185015626771 2.0481162464741764\n",
      "Epoch >>  373 3.370406463816244 1.3215244326717992 2.0488820306748976\n",
      "Epoch >>  374 3.371833888147871 1.3219841712531846 2.049849716397293\n",
      "Epoch >>  375 3.3728917569845045 1.3223116073106078 2.0505801491773057\n",
      "Epoch >>  376 3.374310423043465 1.3228069072427222 2.0515035152986165\n",
      "Epoch >>  377 3.3757087345801726 1.3233103075557877 2.052398426521535\n",
      "Epoch >>  378 3.3764573395343835 1.3236392174127825 2.0528181216325265\n",
      "Epoch >>  379 3.3772658217625007 1.323923973483455 2.0533418477732615\n",
      "Epoch >>  380 3.3783193001841387 1.3242509061359162 2.054068393533871\n",
      "Epoch >>  381 3.3789290629747417 1.3244599911349566 2.0544690713057054\n",
      "Epoch >>  382 3.3801200440917016 1.3249393843994635 2.0551806591486623\n",
      "Epoch >>  383 3.3805695972104632 1.3251659831592697 2.055403613498369\n",
      "Epoch >>  384 3.3815890664598 1.3255341937151242 2.0560548721782017\n",
      "Epoch >>  385 3.382369422222705 1.325828815136761 2.0565406065131877\n",
      "Epoch >>  386 3.383210880903965 1.3260428103318471 2.057168069983137\n",
      "Epoch >>  387 3.3840784667139276 1.3264015046532276 2.0576769614686024\n",
      "Epoch >>  388 3.3851863732296965 1.3267811355681798 2.0584052370629347\n",
      "Epoch >>  389 3.3859894550678438 1.327090163908536 2.058899290563683\n",
      "Epoch >>  390 3.3869508383765323 1.327420633267011 2.0595302045266126\n",
      "Epoch >>  391 3.3878179500474386 1.3278126880096044 2.0600052614533717\n",
      "Epoch >>  392 3.3887890288643634 1.3281442088221433 2.060644819466861\n",
      "Epoch >>  393 3.390218717867537 1.3286202738318158 2.0615984434488803\n",
      "Epoch >>  394 3.3911168347943055 1.3288982814186547 2.062218552785297\n",
      "Epoch >>  395 3.391687120488685 1.3291463813194535 2.0625407385854015\n",
      "Epoch >>  396 3.3924232255915925 1.3294704187156383 2.0629528062835543\n",
      "Epoch >>  397 3.3934910549026114 1.329808652256433 2.063682402050806\n",
      "Epoch >>  398 3.3943061075406638 1.330088514369065 2.0642175925721875\n",
      "Epoch >>  399 3.3952635837026315 1.330396780172456 2.064866802935811\n",
      "Epoch >>  400 3.395847057786554 1.3306085658602769 2.0652384913456534\n",
      "Epoch >>  401 3.3968331082813554 1.330950453106351 2.065882654576902\n",
      "Epoch >>  402 3.397903820122035 1.3312944884208349 2.066609331071223\n",
      "Epoch >>  403 3.3985751643374487 1.3315221721880912 2.0670529915043763\n",
      "Epoch >>  404 3.39982941847175 1.332015170444217 2.067814247395357\n",
      "Epoch >>  405 3.4010238279819562 1.3324384332069292 2.068585394144281\n",
      "Epoch >>  406 3.4022807816714047 1.3328021702808706 2.0694786107374323\n",
      "Epoch >>  407 3.40300764903123 1.3330182963356778 2.069989352050822\n",
      "Epoch >>  408 3.4042557242274065 1.3335253049317193 2.0707304186494726\n",
      "Epoch >>  409 3.4052217569060805 1.3338528991489693 2.0713688571036815\n",
      "Epoch >>  410 3.4065779680231554 1.3343069578770848 2.0722710094979067\n",
      "Epoch >>  411 3.407476182880513 1.3346482959270876 2.072827886302892\n",
      "Epoch >>  412 3.40823273327207 1.334955431396083 2.0732773012498296\n",
      "Epoch >>  413 3.4091387420004766 1.3352878985406949 2.0738508428396614\n",
      "Epoch >>  414 3.4099598335253547 1.3355818660155656 2.0743779668786706\n",
      "Epoch >>  415 3.411175942762504 1.3359614555886767 2.075214486550506\n",
      "Epoch >>  416 3.4119827947211707 1.3362452173109214 2.075737576793001\n",
      "Epoch >>  417 3.412957458768842 1.3365843960521608 2.07637306209722\n",
      "Epoch >>  418 3.413963839504023 1.3369568519088704 2.077006986979831\n",
      "Epoch >>  419 3.4147574936166762 1.3372716283124118 2.0774858646778216\n",
      "Epoch >>  420 3.415763767599458 1.3377002762459802 2.0780634907207336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  421 3.4174605781616405 1.3382242627878325 2.0792363147452826\n",
      "Epoch >>  422 3.418581321829987 1.3385504693772212 2.0800308517915185\n",
      "Epoch >>  423 3.419435430847284 1.338817377760655 2.080618052429788\n",
      "Epoch >>  424 3.420297248153441 1.339149890047382 2.08114735745118\n",
      "Epoch >>  425 3.421334933944886 1.339532569532523 2.0818023637250636\n",
      "Epoch >>  426 3.4218529493125107 1.3397295559530584 2.082123392696729\n",
      "Epoch >>  427 3.4228635008686403 1.3401101493921443 2.082753350826989\n",
      "Epoch >>  428 3.423809532649902 1.3404420932572692 2.083367438754428\n",
      "Epoch >>  429 3.424567071967797 1.3407142230999332 2.0838528482215617\n",
      "Epoch >>  430 3.4255899831199383 1.3411112972609267 2.084478685204578\n",
      "Epoch >>  431 3.426672223867262 1.3414266856936392 2.0852455375151853\n",
      "Epoch >>  432 3.427475879135673 1.3417178437472352 2.0857580347409326\n",
      "Epoch >>  433 3.428565472745435 1.3420854100744295 2.0864800620216\n",
      "Epoch >>  434 3.4300881273700243 1.3425862959328738 2.0875018307839808\n",
      "Epoch >>  435 3.430876593000879 1.3428183664020594 2.0880582259456313\n",
      "Epoch >>  436 3.432017313523921 1.343217455932423 2.088799856940704\n",
      "Epoch >>  437 3.432953069998147 1.3435718371269365 2.0893812322369985\n",
      "Epoch >>  438 3.4339694363375393 1.3438851846811577 2.0900842510126476\n",
      "Epoch >>  439 3.43478546358181 1.3441872709134763 2.090598192029153\n",
      "Epoch >>  440 3.4355213251604177 1.344436792255428 2.0910845322537828\n",
      "Epoch >>  441 3.436517426781626 1.3448410329781304 2.091676393166086\n",
      "Epoch >>  442 3.4373807216948298 1.3450760378956323 2.0923046831623355\n",
      "Epoch >>  443 3.4382894971699263 1.3453847855037195 2.092904711037357\n",
      "Epoch >>  444 3.4391690002356685 1.3457476429451383 2.0934213566523368\n",
      "Epoch >>  445 3.4400824879644096 1.3460769670897001 2.0940055202376295\n",
      "Epoch >>  446 3.441481394710257 1.3465951276385544 2.094886266417927\n",
      "Epoch >>  447 3.442178788911518 1.3468297558102225 2.0953490324491044\n",
      "Epoch >>  448 3.4430563833848606 1.3471170115700544 2.0959393711577285\n",
      "Epoch >>  449 3.443809884959956 1.347382643170092 2.0964272411431213\n",
      "Epoch >>  450 3.44427592583265 1.3476349559078724 2.0966409692791363\n",
      "Epoch >>  451 3.445406741434263 1.348075400942063 2.0973313398509172\n",
      "Epoch >>  452 3.4463642924653315 1.348346569020293 2.0980177228123833\n",
      "Epoch >>  453 3.447431597024104 1.3487230357843152 2.0987085606262563\n",
      "Epoch >>  454 3.4482602198538217 1.3490600074378016 2.0992002118007207\n",
      "Epoch >>  455 3.4491960650847475 1.3493797584383895 2.099816306039086\n",
      "Epoch >>  456 3.4501087880250987 1.3496916870682845 2.1004171003714003\n",
      "Epoch >>  457 3.4509084373260626 1.3499754169743068 2.1009330197706415\n",
      "Epoch >>  458 3.4520196639849665 1.3503780546194035 2.1016416087589906\n",
      "Epoch >>  459 3.4527900788283543 1.3507139750590629 2.102076103150732\n",
      "Epoch >>  460 3.4535821302185026 1.351019787125548 2.10256234248065\n",
      "Epoch >>  461 3.4543941639920783 1.3512938431939232 2.1031003201804754\n",
      "Epoch >>  462 3.455238503246373 1.3516623893187545 2.103576113286302\n",
      "Epoch >>  463 3.455880322281432 1.351936008723854 2.1039443129101283\n",
      "Epoch >>  464 3.4563134684032972 1.3521139979608117 2.1041994697815873\n",
      "Epoch >>  465 3.4573761122833955 1.3525327795696778 2.104843332045179\n",
      "Epoch >>  466 3.4583078762892416 1.3528372847069159 2.105470590906618\n",
      "Epoch >>  467 3.4591022467002848 1.353215050349394 2.105887195681052\n",
      "Epoch >>  468 3.4600930914899815 1.3535882870487725 2.1065048037941176\n",
      "Epoch >>  469 3.4610711280415667 1.3539931355600185 2.1070779918109612\n",
      "Epoch >>  470 3.461947525533711 1.3543547298046592 2.1075927950615556\n",
      "Epoch >>  471 3.4629179529739007 1.3546857851793106 2.1082321671237865\n",
      "Epoch >>  472 3.463587105539145 1.3549363248956412 2.108650779980069\n",
      "Epoch >>  473 3.4646884492212586 1.3553081006534902 2.1093803479149753\n",
      "Epoch >>  474 3.465346569905846 1.35562050055656 2.109726068698805\n",
      "Epoch >>  475 3.466069463785268 1.3559259224686646 2.110143540649496\n",
      "Epoch >>  476 3.466718529128366 1.3562424863982219 2.1104760420730067\n",
      "Epoch >>  477 3.4677050116712453 1.3565436736799652 2.1111613373473808\n",
      "Epoch >>  478 3.468730794832515 1.3569169977021585 2.1118137964781\n",
      "Epoch >>  479 3.46940032950826 1.3571878705051883 2.112212458349654\n",
      "Epoch >>  480 3.4699615183668424 1.3574170918600197 2.112544425837956\n",
      "Epoch >>  481 3.4709645699602687 1.3577501239446281 2.113214445342234\n",
      "Epoch >>  482 3.4719307713212433 1.358098786230953 2.11383198440345\n",
      "Epoch >>  483 3.472627591645422 1.358338437477835 2.1142891534616806\n",
      "Epoch >>  484 3.4731977224367183 1.3585868079669543 2.114610913753555\n",
      "Epoch >>  485 3.4738586936819944 1.3588421067823444 2.115016586175841\n",
      "Epoch >>  486 3.4748736111836265 1.3592222533759486 2.115651357079752\n",
      "Epoch >>  487 3.475800919224493 1.3595769390597114 2.1162239794236224\n",
      "Epoch >>  488 3.476450080070515 1.3598532400391006 2.1165968392947043\n",
      "Epoch >>  489 3.4770334204684716 1.3601345088173693 2.116898910914229\n",
      "Epoch >>  490 3.478039487626912 1.3605199574969118 2.117519529389157\n",
      "Epoch >>  491 3.4786269460009125 1.360817757613548 2.11780918764217\n",
      "Epoch >>  492 3.479419530071182 1.3611268693129956 2.118292660021501\n",
      "Epoch >>  493 3.480377672223427 1.3614482380614619 2.1189294334493076\n",
      "Epoch >>  494 3.4810816275517147 1.3616540518242453 2.1194275749892393\n",
      "Epoch >>  495 3.4817129182906883 1.3618596102779341 2.1198533072742696\n",
      "Epoch >>  496 3.482427987386763 1.3621485561942401 2.120279430457908\n",
      "Epoch >>  497 3.4830906796735452 1.3623905872764088 2.1207000916762704\n",
      "Epoch >>  498 3.483605715802294 1.3625472343984342 2.121058480661131\n",
      "Epoch >>  499 3.4843394000256063 1.3628132741159051 2.1215261251617745\n",
      "Epoch >>  500 3.484903363208333 1.3630523797685128 2.1218509826823313\n",
      "Epoch >>  501 3.4855134254904145 1.363248040070913 2.1222653846682227\n",
      "Epoch >>  502 3.4862489528451004 1.3635688202585987 2.1226801318203585\n",
      "Epoch >>  503 3.4870390436700176 1.3638687303274248 2.1231703125575936\n",
      "Epoch >>  504 3.488155736028912 1.3642263156409753 2.1239294196123377\n",
      "Epoch >>  505 3.48904794454339 1.3645102719876459 2.1245376717861064\n",
      "Epoch >>  506 3.489856700880758 1.3647831552585759 2.125073544830693\n",
      "Epoch >>  507 3.4904578657379064 1.3649322007984128 2.125525664153842\n",
      "Epoch >>  508 3.490955668668967 1.3651198294958893 2.12583583839115\n",
      "Epoch >>  509 3.491693407259151 1.3653816193213504 2.126311787160087\n",
      "Epoch >>  510 3.4925388687299654 1.365657454971625 2.1268814129905342\n",
      "Epoch >>  511 3.493382921264856 1.3660038124086422 2.1273791080915023\n",
      "Epoch >>  512 3.4939492950842626 1.3662353682477195 2.1277139260853244\n",
      "Epoch >>  513 3.494773763167371 1.366551566637661 2.1282221957938354\n",
      "Epoch >>  514 3.4954062275269657 1.3668166639801802 2.128589562814488\n",
      "Epoch >>  515 3.4961228303886775 1.3671196663431506 2.1290031633249407\n",
      "Epoch >>  516 3.4968112239695266 1.3674406333807965 2.129370589865134\n",
      "Epoch >>  517 3.497642966252324 1.3677369381710167 2.1299060273548\n",
      "Epoch >>  518 3.498146500053376 1.3679967211448303 2.130149778188513\n",
      "Epoch >>  519 3.4989460963981656 1.3682997093765639 2.1306463862955463\n",
      "Epoch >>  520 3.4995506672591485 1.368517329918603 2.131033336609158\n",
      "Epoch >>  521 3.500090103368764 1.368718787875255 2.1313713147599356\n",
      "Epoch >>  522 3.5006170703851924 1.3689370078700194 2.1316800617804863\n",
      "Epoch >>  523 3.5012634924144006 1.3692234387706452 2.13204005291144\n",
      "Epoch >>  524 3.5020852961238225 1.3694825303547582 2.13260276505088\n",
      "Epoch >>  525 3.502964185695798 1.36978116808479 2.1331830168774015\n",
      "Epoch >>  526 3.503689111103482 1.3700517958584586 2.1336373145215055\n",
      "Epoch >>  527 3.5042829246360694 1.3702960782533964 2.1339868456524402\n",
      "Epoch >>  528 3.5051940314732564 1.3706414395838848 2.1345525911700904\n",
      "Epoch >>  529 3.505747656338957 1.370860295925943 2.134887359676696\n",
      "Epoch >>  530 3.5063404906838853 1.371121897264775 2.13521859268098\n",
      "Epoch >>  531 3.507203258203945 1.3714784446924966 2.135724812765394\n",
      "Epoch >>  532 3.5078634927334305 1.37174465149344 2.1361188405089395\n",
      "Epoch >>  533 3.508557216475854 1.372060948885161 2.1364962668477476\n",
      "Epoch >>  534 3.5094250868093186 1.372380467611377 2.137044618447213\n",
      "Epoch >>  535 3.5102065108889966 1.3726636931194174 2.137542817019396\n",
      "Epoch >>  536 3.5107579868783274 1.3729327169665286 2.137825269167678\n",
      "Epoch >>  537 3.5114257192715836 1.373196064759939 2.138229653775547\n",
      "Epoch >>  538 3.512246596407581 1.373467014691846 2.138779580985803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  539 3.5131390344615334 1.3738006944695764 2.139338339269234\n",
      "Epoch >>  540 3.5140855596129423 1.3741321249405014 2.1399534339523276\n",
      "Epoch >>  541 3.514839899195956 1.374442422780789 2.140397475717511\n",
      "Epoch >>  542 3.515759898149506 1.37472521100526 2.1410346864287955\n",
      "Epoch >>  543 3.5165028193463734 1.3750821084155194 2.1414207102106992\n",
      "Epoch >>  544 3.517257994642826 1.3753402078262915 2.141917786101625\n",
      "Epoch >>  545 3.5178210928781786 1.3755729909711363 2.1422481011939896\n",
      "Epoch >>  546 3.5186143660253317 1.3758429063961959 2.1427714589154734\n",
      "Epoch >>  547 3.5194338784353794 1.3761540289299536 2.143279848783576\n",
      "Epoch >>  548 3.5201408702191 1.376367744910987 2.1437731245894662\n",
      "Epoch >>  549 3.520865468002991 1.3766129201154478 2.144252547158633\n",
      "Epoch >>  550 3.521615390607644 1.3769305132008927 2.144684876667069\n",
      "Epoch >>  551 3.5222635029578124 1.377198249526936 2.145065252695817\n",
      "Epoch >>  552 3.5228611032804085 1.3774388862495222 2.1454222162890666\n",
      "Epoch >>  553 3.523446479547083 1.3776769942531988 2.145769484557172\n",
      "Epoch >>  554 3.523962080301546 1.377897165183139 2.146064914374112\n",
      "Epoch >>  555 3.5246011178421943 1.3781087376656707 2.146492379444743\n",
      "Epoch >>  556 3.525832785636374 1.378528353088119 2.147304431825344\n",
      "Epoch >>  557 3.5264332579526263 1.3787214817879756 2.147711775429977\n",
      "Epoch >>  558 3.52703554544234 1.3789249714656344 2.1481105732426196\n",
      "Epoch >>  559 3.527727611047056 1.3792344657118498 2.148493144618769\n",
      "Epoch >>  560 3.5280562971528893 1.3793878301838596 2.1486684662647084\n",
      "Epoch >>  561 3.5283447345892145 1.3795093615385654 2.1488353723501867\n",
      "Epoch >>  562 3.528651731725087 1.3796169412265702 2.149034789784219\n",
      "Epoch >>  563 3.5292265771028473 1.3798481297984335 2.1493784466066876\n",
      "Epoch >>  564 3.530076070923642 1.3802139212592448 2.1498621489673773\n",
      "Epoch >>  565 3.5308246011906976 1.3804928706376731 2.1503317298620064\n",
      "Epoch >>  566 3.531491968413851 1.3807759026882338 2.1507160650403527\n",
      "Epoch >>  567 3.5322028719608896 1.3810295262855583 2.1511733450040476\n",
      "Epoch >>  568 3.53252159616139 1.381190222522018 2.1513313729698966\n",
      "Epoch >>  569 3.5331088916922084 1.3814082171174453 2.151700673914087\n",
      "Epoch >>  570 3.533722932098965 1.381618746867785 2.152104184548412\n",
      "Epoch >>  571 3.534398755433702 1.3818609778440747 2.1525377769157545\n",
      "Epoch >>  572 3.5349376319169634 1.3820455325501166 2.1528920986909412\n",
      "Epoch >>  573 3.5355874893305765 1.3823425642265923 2.153244924427314\n",
      "Epoch >>  574 3.5360634720506616 1.382572837118189 2.1534906342549354\n",
      "Epoch >>  575 3.5365304670733035 1.3827311890392449 2.153799277362581\n",
      "Epoch >>  576 3.5371019029412514 1.3829547631984553 2.154147139080028\n",
      "Epoch >>  577 3.537816995390739 1.383169119142041 2.1546478755739638\n",
      "Epoch >>  578 3.538364716574807 1.383395257276182 2.1549694586380705\n",
      "Epoch >>  579 3.5386612678559435 1.383560814067124 2.1551004531164475\n",
      "Epoch >>  580 3.5392710406850774 1.383831789108253 2.155439250905592\n",
      "Epoch >>  581 3.539625480877959 1.3839812720756133 2.1556442081411555\n",
      "Epoch >>  582 3.540180170906832 1.3842221848322318 2.1559579853880075\n",
      "Epoch >>  583 3.540633744483303 1.3844074830681992 2.1562262607132996\n",
      "Epoch >>  584 3.5413088494403495 1.3846834973240192 2.156625351415474\n",
      "Epoch >>  585 3.541962881470881 1.3849436214302981 2.1570192593119466\n",
      "Epoch >>  586 3.5423785097650056 1.3851129342123574 2.1572655748268077\n",
      "Epoch >>  587 3.5427718558391796 1.385259738341601 2.1575121167761626\n",
      "Epoch >>  588 3.5432783163795296 1.385469481921124 2.1578088337382413\n",
      "Epoch >>  589 3.544040741569233 1.385792357623293 2.15824838323386\n",
      "Epoch >>  590 3.5444222619441006 1.3859380600294775 2.1584842012046366\n",
      "Epoch >>  591 3.5450237298164065 1.3861743221832377 2.1588494069191064\n",
      "Epoch >>  592 3.5457952332413467 1.3864648653563256 2.1593303671678914\n",
      "Epoch >>  593 3.5463231423757846 1.3866114713775326 2.159711670284249\n",
      "Epoch >>  594 3.5468929403637888 1.386845459169093 2.1600474804826013\n",
      "Epoch >>  595 3.547483218573309 1.3870860576971853 2.160397160154725\n",
      "Epoch >>  596 3.5481014796326957 1.3872952514930974 2.1608062273979893\n",
      "Epoch >>  597 3.5488956064153525 1.3876158034259947 2.1612798022411477\n",
      "Epoch >>  598 3.549615989231083 1.3879141998517581 2.1617017886295953\n",
      "Epoch >>  599 3.5502778404650837 1.3880910320513633 2.1621868076618997\n",
      "Epoch >>  600 3.5508184952618627 1.3883149693911452 2.162503525116637\n",
      "Epoch >>  601 3.55152918547132 1.3885823104751058 2.1629468742455074\n",
      "Epoch >>  602 3.552229319284681 1.3888518811038466 2.163377437451003\n",
      "Epoch >>  603 3.5529887709699275 1.3891340669198826 2.1638547033154607\n",
      "Epoch >>  604 3.5533792410463096 1.389269422498881 2.1641098178134244\n",
      "Epoch >>  605 3.553841738161548 1.3894798209869268 2.1643619164575916\n",
      "Epoch >>  606 3.554492792333542 1.3897295446949636 2.1647632469210616\n",
      "Epoch >>  607 3.555227943311862 1.3899937350901126 2.1652342075107756\n",
      "Epoch >>  608 3.555992259922849 1.3902941740102064 2.1656980852040424\n",
      "Epoch >>  609 3.556339872379552 1.3904213287692273 2.1659185429145023\n",
      "Epoch >>  610 3.5568876831521528 1.3906321396172814 2.166255542838529\n",
      "Epoch >>  611 3.557453786666365 1.3908543611448563 2.16659942481931\n",
      "Epoch >>  612 3.558044216103572 1.3911075955761283 2.1669366198101336\n",
      "Epoch >>  613 3.5586387674759723 1.3913348301475994 2.1673039366104643\n",
      "Epoch >>  614 3.559366306902698 1.3915578360282974 2.1678084701603533\n",
      "Epoch >>  615 3.5597661501031563 1.391713674734909 2.1680524746560486\n",
      "Epoch >>  616 3.5603893722365076 1.3919227334994346 2.168466638027716\n",
      "Epoch >>  617 3.5610357598445126 1.3921247263237002 2.168911032828237\n",
      "Epoch >>  618 3.5620131031033826 1.3924702554228316 2.1695428469922238\n",
      "Epoch >>  619 3.562639562978259 1.3927513732187389 2.1698881890754276\n",
      "Epoch >>  620 3.5631696101638957 1.3929069649941332 2.170262644489405\n",
      "Epoch >>  621 3.5636485604449053 1.3931010405535231 2.1705475192078483\n",
      "Epoch >>  622 3.5641159014474857 1.3932909961356146 2.17082490462814\n",
      "Epoch >>  623 3.5644249449113192 1.3934414154755799 2.170983528729255\n",
      "Epoch >>  624 3.5649483445846317 1.393630220972246 2.1713181229066634\n",
      "Epoch >>  625 3.565515733274289 1.3938172019636705 2.1716985306192846\n",
      "Epoch >>  626 3.5661629444690366 1.394078660762713 2.1720842830014946\n",
      "Epoch >>  627 3.5668932188750975 1.394329785108857 2.172563433064604\n",
      "Epoch >>  628 3.5674380825190126 1.3945475943613093 2.172890487449616\n",
      "Epoch >>  629 3.5683294280846676 1.3948665723771863 2.173462855000826\n",
      "Epoch >>  630 3.5688456777186617 1.3950751190388224 2.1737705579875053\n",
      "Epoch >>  631 3.569114140539772 1.3951860945663133 2.1739280452778007\n",
      "Epoch >>  632 3.5696193456615646 1.3954033256177194 2.1742160193494824\n",
      "Epoch >>  633 3.5700797988360375 1.3955627008820193 2.174517097261738\n",
      "Epoch >>  634 3.570665332669137 1.3957390987061902 2.174926233274099\n",
      "Epoch >>  635 3.5712769870770007 1.3959670248818672 2.17530996150386\n",
      "Epoch >>  636 3.5717095635488185 1.3960989031204822 2.175610659727542\n",
      "Epoch >>  637 3.572202103807705 1.39626209792138 2.175940005180965\n",
      "Epoch >>  638 3.572901940323261 1.3965029652809697 2.17639897434151\n",
      "Epoch >>  639 3.573465113139502 1.39673751296986 2.1767275994634425\n",
      "Epoch >>  640 3.5742440746065562 1.3969851961416047 2.1772588777576614\n",
      "Epoch >>  641 3.5746401676958732 1.3971475130542474 2.1774926539438058\n",
      "Epoch >>  642 3.5751204822327405 1.3973421539881934 2.1777783275422196\n",
      "Epoch >>  643 3.5756559138812403 1.3975261851943255 2.1781297279910876\n",
      "Epoch >>  644 3.576091199434843 1.397707658566829 2.1783835401794773\n",
      "Epoch >>  645 3.5765201945207297 1.397884923287007 2.178635270542264\n",
      "Epoch >>  646 3.5771794465956446 1.3981319594133295 2.179047486491361\n",
      "Epoch >>  647 3.57792787465851 1.398403440279317 2.1795244336862867\n",
      "Epoch >>  648 3.5785496377161774 1.3986877076616697 2.179861929348586\n",
      "Epoch >>  649 3.5792529219105838 1.398946071171216 2.1803068500401794\n",
      "Epoch >>  650 3.5799070696597504 1.3991469704539052 2.1807600985194844\n",
      "Epoch >>  651 3.58047815717887 1.3993255477872923 2.181152608710782\n",
      "Epoch >>  652 3.580920850608036 1.3995513102966568 2.1813695396390864\n",
      "Epoch >>  653 3.5814261887890346 1.3997350614297202 2.181691126688142\n",
      "Epoch >>  654 3.581901299527788 1.3999151478327696 2.1819861510297236\n",
      "Epoch >>  655 3.582511560620808 1.4001976217549215 2.1823139381873635\n",
      "Epoch >>  656 3.58289104110032 1.4003779747998073 2.182513065619093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  657 3.5833069315732295 1.4005386509738897 2.1827682799140753\n",
      "Epoch >>  658 3.5838473817039738 1.400720632624537 2.1831267483964405\n",
      "Epoch >>  659 3.5843545013936406 1.4008767618080262 2.1834777388939797\n",
      "Epoch >>  660 3.584849993643011 1.4010685051206482 2.183781487834501\n",
      "Epoch >>  661 3.5852905723243906 1.4012555591829199 2.1840350124548853\n",
      "Epoch >>  662 3.5857597785672315 1.4014491404632154 2.1843106374179495\n",
      "Epoch >>  663 3.5862475733239294 1.4016437575610254 2.184603815071855\n",
      "Epoch >>  664 3.5868678542720867 1.4018563347284114 2.1850115188426367\n",
      "Epoch >>  665 3.587390923395283 1.4020873072399804 2.1853036154687913\n",
      "Epoch >>  666 3.588111240370811 1.4023505081323546 2.1857607315390224\n",
      "Epoch >>  667 3.5888454234290665 1.4026271845075597 2.1862182382341815\n",
      "Epoch >>  668 3.5892330135688164 1.4027795514573576 2.186453461419676\n",
      "Epoch >>  669 3.5899211335262478 1.4030566542000524 2.1868644786261755\n",
      "Epoch >>  670 3.5905981845896795 1.403314076267044 2.187284107638588\n",
      "Epoch >>  671 3.59109787003778 1.403520382187113 2.187577487167163\n",
      "Epoch >>  672 3.5917135606112534 1.4037681945476952 2.187945365366333\n",
      "Epoch >>  673 3.5921658516138217 1.403987533078178 2.188178317840758\n",
      "Epoch >>  674 3.5928817374159894 1.404316966526867 2.1885647701786843\n",
      "Epoch >>  675 3.5934829215327353 1.4045507822118137 2.18893213860406\n",
      "Epoch >>  676 3.594025067304513 1.4047407955395517 2.189284271051189\n",
      "Epoch >>  677 3.5946330452887754 1.4049666272103183 2.1896664173657125\n",
      "Epoch >>  678 3.5955295551607334 1.405282912891446 2.1902466415567425\n",
      "Epoch >>  679 3.5961742366747163 1.4055183097022257 2.1906559262642724\n",
      "Epoch >>  680 3.5968515134782213 1.4057435931019244 2.1911079196834318\n",
      "Epoch >>  681 3.597334982705103 1.4059381865566087 2.191396795457623\n",
      "Epoch >>  682 3.598070806529149 1.4062222680923193 2.191848537742074\n",
      "Epoch >>  683 3.5986226100721423 1.4064789320090676 2.192143677366688\n",
      "Epoch >>  684 3.5993753329639233 1.406770847818609 2.192604484449587\n",
      "Epoch >>  685 3.5998952991356177 1.4069719359329345 2.192923362506246\n",
      "Epoch >>  686 3.600595819291785 1.4072044972188966 2.193391321374587\n",
      "Epoch >>  687 3.601321103792746 1.4074670804624045 2.193854022621921\n",
      "Epoch >>  688 3.6017365135992776 1.4076423575983954 2.1940941552896813\n",
      "Epoch >>  689 3.6021832550867097 1.4078451372965792 2.1943381170776233\n",
      "Epoch >>  690 3.602594463096708 1.408032885789812 2.194561576591325\n",
      "Epoch >>  691 3.602986887352078 1.408228234538997 2.1947586521001434\n",
      "Epoch >>  692 3.603563146535216 1.4084119615792035 2.195151184244821\n",
      "Epoch >>  693 3.6040930206185418 1.4086283157509765 2.1954647041538116\n",
      "Epoch >>  694 3.6046072103276314 1.408815510605163 2.1957916990094333\n",
      "Epoch >>  695 3.6050658350822182 1.4089869184612895 2.1960789159053378\n",
      "Epoch >>  696 3.6056854321799925 1.4092085806821317 2.19647685079711\n",
      "Epoch >>  697 3.6063283773146377 1.4094303049224144 2.196898071693981\n",
      "Epoch >>  698 3.606697800341999 1.4096172343396738 2.1970805652806247\n",
      "Epoch >>  699 3.607300442151576 1.4098302127272264 2.1974702286916346\n",
      "Epoch >>  700 3.608049568873393 1.4101022338800386 2.197947334248808\n",
      "Epoch >>  701 3.608287661233243 1.410198860582288 2.198088799899286\n",
      "Epoch >>  702 3.6088649592344275 1.4104194629181912 2.1984454955704966\n",
      "Epoch >>  703 3.6093612604472067 1.4106072322110041 2.19875402748214\n",
      "Epoch >>  704 3.6100969143401436 1.410904302050855 2.1991926115347713\n",
      "Epoch >>  705 3.6106551327342986 1.411129727783663 2.1995254041972143\n",
      "Epoch >>  706 3.6113281717782435 1.4114275937491956 2.199900577276628\n",
      "Epoch >>  707 3.611761820409994 1.4116106130539434 2.2001512065966327\n",
      "Epoch >>  708 3.61220391747635 1.4118113774822603 2.200392539252417\n",
      "Epoch >>  709 3.6128673265315068 1.4121002309855704 2.200767094804133\n",
      "Epoch >>  710 3.6134672333948346 1.4123244073351402 2.2011428252988106\n",
      "Epoch >>  711 3.6140158516552634 1.4125666228550633 2.2014492280261275\n",
      "Epoch >>  712 3.6143153225685696 1.4127000098917972 2.201615311905243\n",
      "Epoch >>  713 3.614933852731409 1.4129237492627382 2.2020101026824523\n",
      "Epoch >>  714 3.6154730308548344 1.4130868787536306 2.202386151313967\n",
      "Epoch >>  715 3.615986837758844 1.4133458219654702 2.202641015006063\n",
      "Epoch >>  716 3.6166294938989054 1.413556455688585 2.203073037432837\n",
      "Epoch >>  717 3.617028688177736 1.413784237101783 2.2032444503082322\n",
      "Epoch >>  718 3.6176515452444593 1.4140506422258603 2.2036009022540264\n",
      "Epoch >>  719 3.618105213497807 1.4142401452639917 2.2038650674724005\n",
      "Epoch >>  720 3.618643685489748 1.414494471953895 2.204149212761142\n",
      "Epoch >>  721 3.6190913518064365 1.41468335352304 2.2044079975181425\n",
      "Epoch >>  722 3.6196807132066326 1.4149499388760727 2.2047307735702093\n",
      "Epoch >>  723 3.6200224875584452 1.415144570742672 2.20487791605943\n",
      "Epoch >>  724 3.6205850349040483 1.4153802289728306 2.2052048051755504\n",
      "Epoch >>  725 3.621006652643591 1.4155597831075832 2.205446868775963\n",
      "Epoch >>  726 3.6215810893149087 1.415737596850463 2.2058434916891474\n",
      "Epoch >>  727 3.622193422644 1.415961970077362 2.2062314518013726\n",
      "Epoch >>  728 3.622678074089404 1.4161487954547796 2.2065292778708074\n",
      "Epoch >>  729 3.623092150366286 1.4163446227442371 2.2067475268504566\n",
      "Epoch >>  730 3.6233439881069187 1.4165112133280982 2.2068327740138822\n",
      "Epoch >>  731 3.623838912220662 1.4167007321274274 2.2071381793370968\n",
      "Epoch >>  732 3.6242706265245217 1.4169010775372497 2.207369548231968\n",
      "Epoch >>  733 3.6247813427502478 1.4171111330361605 2.207670208967708\n",
      "Epoch >>  734 3.625235159217967 1.417246213238139 2.20798894522761\n",
      "Epoch >>  735 3.625665432936289 1.417424237063767 2.208241195117633\n",
      "Epoch >>  736 3.626391713190667 1.4176714266196875 2.208720285824902\n",
      "Epoch >>  737 3.626756486900524 1.4178226699469647 2.208933816214131\n",
      "Epoch >>  738 3.6271868486102665 1.4180400501619042 2.209146797702277\n",
      "Epoch >>  739 3.6276219145423556 1.4182527782962682 2.209369135511759\n",
      "Epoch >>  740 3.628179160781627 1.418424601249241 2.209754558785462\n",
      "Epoch >>  741 3.6286796600984816 1.4186227221552363 2.2100569371902115\n",
      "Epoch >>  742 3.6292077568628365 1.4188531647018265 2.210354591409837\n",
      "Epoch >>  743 3.6295279704505496 1.4189789129895138 2.210549056704885\n",
      "Epoch >>  744 3.630117573354238 1.419256165547514 2.210861407035555\n",
      "Epoch >>  745 3.630399261378261 1.4194071481337287 2.210992112456027\n",
      "Epoch >>  746 3.6308560014727065 1.4195624661120416 2.21129353457322\n",
      "Epoch >>  747 3.6311512980067575 1.419714004678318 2.2114372925535304\n",
      "Epoch >>  748 3.631711681489165 1.4199145970467841 2.2117970836713043\n",
      "Epoch >>  749 3.6322177744952144 1.4201229093825967 2.2120948643436917\n",
      "Epoch >>  750 3.6326127955746177 1.4203022775046588 2.212310517294391\n",
      "Epoch >>  751 3.633248020566455 1.4205398963550055 2.2127081234300636\n",
      "Epoch >>  752 3.6335700761793084 1.4206588194567122 2.212911255942815\n",
      "Epoch >>  753 3.633913808042643 1.4208478786579484 2.2130659286067007\n",
      "Epoch >>  754 3.634360342490367 1.4210171390099586 2.2133432026873745\n",
      "Epoch >>  755 3.6346865793487955 1.421137590089739 2.2135489884661035\n",
      "Epoch >>  756 3.6350787524869235 1.4212963774125271 2.2137823742748974\n",
      "Epoch >>  757 3.6354966911073667 1.421507669451069 2.2139890208510944\n",
      "Epoch >>  758 3.636011687514035 1.4217379271004775 2.214273759603226\n",
      "Epoch >>  759 3.636403815445549 1.42188499143429 2.2145188231944704\n",
      "Epoch >>  760 3.6369676982216177 1.4220909724581483 2.2148767249487005\n",
      "Epoch >>  761 3.6374307894442435 1.4222594803704043 2.2151713082632725\n",
      "Epoch >>  762 3.637896435044081 1.4224633082249207 2.2154331260070204\n",
      "Epoch >>  763 3.63848952544267 1.4227022985605504 2.215787226072091\n",
      "Epoch >>  764 3.6390005882849197 1.42291352833433 2.2160870591410458\n",
      "Epoch >>  765 3.6392173885230084 1.4230438404869696 2.2161735472248276\n",
      "Epoch >>  766 3.639570439249734 1.4232373194461336 2.216333118993458\n",
      "Epoch >>  767 3.6400020603447825 1.4234667190647403 2.2165353404663364\n",
      "Epoch >>  768 3.640539213952641 1.4236802072242454 2.2168590059066626\n",
      "Epoch >>  769 3.6409826756475323 1.4238415161723779 2.2171411586655023\n",
      "Epoch >>  770 3.641450849721974 1.4240557935238223 2.2173950553932147\n",
      "Epoch >>  771 3.64177224185919 1.4241960630807495 2.217576177984591\n",
      "Epoch >>  772 3.6422396059514615 1.424405188003914 2.2178344171496933\n",
      "Epoch >>  773 3.642811750393111 1.424587978090268 2.2182237715146855\n",
      "Epoch >>  774 3.6433130184640325 1.4248130258290566 2.2184999918338315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  775 3.6438507582758715 1.4250387873804706 2.218811970094136\n",
      "Epoch >>  776 3.6443023883359675 1.4252989604380246 2.2190034271048984\n",
      "Epoch >>  777 3.644649330749064 1.4254945509471046 2.219154779016245\n",
      "Epoch >>  778 3.6452536241227667 1.4257468456606954 2.219506777677135\n",
      "Epoch >>  779 3.6456763178388947 1.4259193726433728 2.219756944411649\n",
      "Epoch >>  780 3.6460892572594417 1.4261211368575855 2.2199681196103134\n",
      "Epoch >>  781 3.646569341067682 1.4263531275338939 2.220216212735993\n",
      "Epoch >>  782 3.6471102925128616 1.4265450360926433 2.220565255612028\n",
      "Epoch >>  783 3.6476644515033945 1.4267354175566591 2.2209290331353695\n",
      "Epoch >>  784 3.648050745137017 1.4269140690488373 2.2211366752824935\n",
      "Epoch >>  785 3.648505544643887 1.427111589491704 2.2213939543501002\n",
      "Epoch >>  786 3.64890676338858 1.4273133776588116 2.221593384922937\n",
      "Epoch >>  787 3.649202896868432 1.4274496729031423 2.2217532231738253\n",
      "Epoch >>  788 3.64968148120047 1.427639173014701 2.2220423073828193\n",
      "Epoch >>  789 3.6504292675982075 1.4279053392817844 2.2225239275107116\n",
      "Epoch >>  790 3.650722242170834 1.4280131971540846 2.222709044212007\n",
      "Epoch >>  791 3.651359071301042 1.4282134075381303 2.223145662945669\n",
      "Epoch >>  792 3.6517993240086892 1.4283470389001673 2.223452284284516\n",
      "Epoch >>  793 3.65218243596944 1.4285518793439826 2.223630555799688\n",
      "Epoch >>  794 3.6526267717207284 1.4286896188636762 2.2239371520272355\n",
      "Epoch >>  795 3.6531175807795613 1.428918082163351 2.2241994977871764\n",
      "Epoch >>  796 3.653581070299383 1.429138379156641 2.224442690309499\n",
      "Epoch >>  797 3.653922924728798 1.4293084413228243 2.2246144825696668\n",
      "Epoch >>  798 3.6542220830289516 1.429423506080484 2.2247985761081317\n",
      "Epoch >>  799 3.6549030924655774 1.429690659510619 2.2252124321030204\n",
      "Epoch >>  800 3.6553733758947504 1.4298693536426756 2.2255040213933865\n",
      "Epoch >>  801 3.655962049505982 1.430065601060232 2.225896447591067\n",
      "Epoch >>  802 3.6565265425113243 1.4302671024457854 2.2262594392141004\n",
      "Epoch >>  803 3.6569712928534943 1.4304182068076106 2.226553085188662\n",
      "Epoch >>  804 3.657295896134456 1.4305848920565722 2.226711003230884\n",
      "Epoch >>  805 3.657701161954145 1.430746894031888 2.226954267086957\n",
      "Epoch >>  806 3.658213429860974 1.4309361808820682 2.22727724813451\n",
      "Epoch >>  807 3.6586598798898833 1.4310746419478235 2.2275852371016254\n",
      "Epoch >>  808 3.658940160614884 1.4311659861994384 2.2277741735754777\n",
      "Epoch >>  809 3.6593956148733238 1.431354327297482 2.2280412867348933\n",
      "Epoch >>  810 3.6598572313781785 1.4315083651560072 2.228348865383432\n",
      "Epoch >>  811 3.660301536019864 1.4316447647247106 2.22865677046057\n",
      "Epoch >>  812 3.6608644379154396 1.4318685395513944 2.2289958975259143\n",
      "Epoch >>  813 3.6613638225111296 1.432078915629925 2.229284906052343\n",
      "Epoch >>  814 3.6616111041463135 1.4321891148928072 2.2294219884163535\n",
      "Epoch >>  815 3.6618946970110193 1.432304484190853 2.229590211967058\n",
      "Epoch >>  816 3.662256852693035 1.432442407996078 2.22981444384498\n",
      "Epoch >>  817 3.662707512384804 1.4326091993104593 2.2300983122382423\n",
      "Epoch >>  818 3.66314188723882 1.4327801092004628 2.2303617771952076\n",
      "Epoch >>  819 3.6635904892624063 1.4329167294868745 2.2306737589235106\n",
      "Epoch >>  820 3.663861082712776 1.4330505047214517 2.2308105771306925\n",
      "Epoch >>  821 3.664306720313138 1.4332095910330096 2.231097128415599\n",
      "Epoch >>  822 3.664720477964933 1.4334193421804446 2.2313011349125356\n",
      "Epoch >>  823 3.665211224324341 1.4335997542641759 2.23161146918937\n",
      "Epoch >>  824 3.6656402211119823 1.4337457565363023 2.2318944637151144\n",
      "Epoch >>  825 3.666020851002325 1.4339206949405092 2.2321001551990203\n",
      "Epoch >>  826 3.6663680916976955 1.4340573977386712 2.232310693104155\n",
      "Epoch >>  827 3.6669706330407985 1.43427397459502 2.2326966575875136\n",
      "Epoch >>  828 3.6672264723131307 1.4343704745426136 2.2328559969105055\n",
      "Epoch >>  829 3.667453116609883 1.4344678054744175 2.2329853102870274\n",
      "Epoch >>  830 3.6679016375558096 1.4346582025668564 2.2332434341341827\n",
      "Epoch >>  831 3.6682562716635068 1.4348260958971983 2.2334301749170473\n",
      "Epoch >>  832 3.668870768589302 1.4350344653487483 2.233836302392981\n",
      "Epoch >>  833 3.6695593771793984 1.43531318912719 2.2342461872048682\n",
      "Epoch >>  834 3.669929922299926 1.4354709360832003 2.2344589853666936\n",
      "Epoch >>  835 3.6702602700552855 1.4356214298763263 2.2346388393371495\n",
      "Epoch >>  836 3.6706336485845625 1.435803147532157 2.2348305002162405\n",
      "Epoch >>  837 3.6711782927031145 1.4360039158308733 2.2351743760400398\n",
      "Epoch >>  838 3.6715999773810655 1.4362052635867482 2.2353947129502036\n",
      "Epoch >>  839 3.6721378667873554 1.4364205152338947 2.2357173507084798\n",
      "Epoch >>  840 3.672616453232959 1.4366047168604534 2.2360117355333493\n",
      "Epoch >>  841 3.672917414512981 1.4367187859362696 2.2361986277346344\n",
      "Epoch >>  842 3.673277107366101 1.4368910779118105 2.2363860286073023\n",
      "Epoch >>  843 3.6735857601407917 1.4370421344774655 2.236543624831626\n",
      "Epoch >>  844 3.673875342488915 1.4371705354882194 2.2367048061703083\n",
      "Epoch >>  845 3.6743594230150554 1.4374090223183753 2.236950399875183\n",
      "Epoch >>  846 3.674752612778673 1.4375863295457818 2.237166282421667\n",
      "Epoch >>  847 3.6751480643468697 1.437758584930645 2.237389478597982\n",
      "Epoch >>  848 3.6753904220423146 1.4378869500180884 2.2375034712039548\n",
      "Epoch >>  849 3.675767106206198 1.4380700259962809 2.237697079386743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-bd41d5d062a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [STAR] Training Loop\n",
    "\n",
    "loss_hist1  = Averager()\n",
    "loss_hist2  = Averager()\n",
    "loss_hist3  = Averager()\n",
    "\n",
    "\n",
    "ce_loss  = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "batch_size = 4\n",
    "counter    = 0\n",
    "model.train()\n",
    "prev_min = 1000\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for t1 in train_data_loader:\n",
    "        new_images  = torch.Tensor(t1['img']).to(device)\n",
    "        attr_target = t1['attr'].to(device)\n",
    "        cate_target = t1['cate'].to(device)\n",
    "\n",
    "        out1, out2  = model(new_images)\n",
    "        cate_target = torch.reshape(cate_target, [batch_size])\n",
    "        #print(out1.shape, out2.shape, cate_target.shape, attr_target.shape)\n",
    "\n",
    "        loss1      = 5*bce_loss(out1, attr_target)\n",
    "        loss2      = ce_loss(out2,  cate_target)\n",
    "\n",
    "        losses     = loss1 + loss2 #sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        counter =  counter+1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t1 in val_data_loader:\n",
    "            new_images  = torch.Tensor(t1['img']).to(device)\n",
    "            attr_target = t1['attr'].to(device)\n",
    "            cate_target = t1['cate'].to(device)\n",
    "\n",
    "            out1, out2  = model(new_images)\n",
    "            cate_target = torch.reshape(cate_target, [batch_size])\n",
    "            #print(out1.shape, out2.shape, cate_target.shape, attr_target.shape)\n",
    "\n",
    "            loss1      = 5*bce_loss(out1, attr_target)\n",
    "            loss2      = ce_loss(out2,  cate_target)\n",
    "\n",
    "            losses     = loss1 + loss2\n",
    "\n",
    "            loss_hist1.send(loss1.data.item())\n",
    "            loss_hist2.send(loss2.data.item())\n",
    "            loss_hist3.send(losses.data.item())\n",
    "    \n",
    "    if loss_hist3.value < prev_min:\n",
    "        print('Saving the model ', prev_min, loss_hist3.value)\n",
    "        torch.save(model.state_dict(), 'fashion_cate_attr.pth')\n",
    "        prev_min = loss_hist3.value\n",
    "    \n",
    "    print('Epoch >> ', epoch, loss_hist3.value, loss_hist1.value, loss_hist2.value)\n",
    "\n",
    "#a = next(dloader)\n",
    "#print(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0,
     4,
     37,
     72,
     128,
     221,
     227,
     232
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the functions for reading the data for Wheat Dataset\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    r1 = [float(x) for x in r]\n",
    "    r = r1\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df = pd.read_csv('/media/yu-hao/WindowsData/WheatDataset/train.csv')\n",
    "train_df.shape\n",
    "\n",
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "temp = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "#train_df[['x', 'y', 'w', 'h']] = \n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df['x'] = temp[:, 0]#train_df['x'].astype(np.float)\n",
    "train_df['y'] = temp[:, 1]#train_df['y'].astype(np.float)\n",
    "train_df['w'] = temp[:, 2]#train_df['w'].astype(np.float)\n",
    "train_df['h'] = temp[:, 3]#train_df['h'].astype(np.float)\n",
    "\n",
    "# df['bbox'] = df['bbox'].apply(lambda x: np.array(x))\n",
    "# x = np.array(list(df['bbox']))\n",
    "# print(x)\n",
    "# for i, dim in enumerate(['x', 'y', 'w', 'h']):\n",
    "#     df[dim] = x[:, i]\n",
    "\n",
    "# # df.drop('bbox', axis=1, inplace=True)\n",
    "# #df.head()\n",
    "\n",
    "class WheatDatasetOld(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.image_ids  = self.df['image_id'].unique()\n",
    "        self.image_dir  = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records  = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        im_name = image_id + '.jpg'\n",
    "        img = Image.open(self.image_dir/im_name).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']  = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class DBTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_set = 1, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.counter = 0\n",
    "        if train_set == 1:\n",
    "            self.train_start  = 0\n",
    "            self.train_end    = 150\n",
    "        else:\n",
    "            self.train_start  = 150\n",
    "            self.train_end    = 200\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        suffix_str  = ''#random.choice(['_m2', '_m1', '_p1', '_p2', ''])\n",
    "        print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "        self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "        self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        \n",
    "#         self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#         self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy.npy')[self.train_start:self.train_end]\n",
    "#         self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx.npy')[self.train_start:self.train_end]\n",
    "#         self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy.npy')[self.train_start:self.train_end]\n",
    "#         self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr.npy')[self.train_start:self.train_end]\n",
    "#         self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        self.transforms1 = A.Compose(\n",
    "                                    [A.HorizontalFlip(p=0.5),  A.VerticalFlip(p=0.5), ],\n",
    "                                     #A.Downscale(scale_min=0.75, scale_max=0.75,interpolation=3),],\n",
    "                                    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "                                   )\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        self.counter = self.counter+1\n",
    "        #if self.counter % 10 == 0:\n",
    "        #    print('Counter is ', self.counter)\n",
    "        \n",
    "#         if self.train_set == 1 and self.counter % 150 == 0 and random.random() < 0.2:\n",
    "#             suffix_str  = random.choice([ '_m1', '_p1', ''])\n",
    "#             print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "#             self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#             self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "                \n",
    "        img = self.trainx[idx, 0].astype('float32')/60000.0\n",
    "        img[img > 1] = 1\n",
    "        img = ndimage.interpolation.zoom(img, 0.25)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.concatenate([img, img, img], axis=0)\n",
    "        #if(0):\n",
    "        if(self.train_set == 1):\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        \n",
    "        boxes = np.array([self.coordx[idx]/4, self.coordy[idx]/4, self.width_arr[idx]/4, self.height_arr[idx]/4])#records[['x', 'y', 'w', 'h']].values\n",
    "        boxes = np.expand_dims(boxes, axis=0)\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        \n",
    "        area = self.width_arr[idx] * self.height_arr[idx]\n",
    "        area = torch.Tensor(area)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels =  torch.ones((1,)).type(torch.int64)\n",
    "        \n",
    "        if(self.train_set == 1):\n",
    "        #if(0):\n",
    "            transformed = self.transforms1(image=img, bboxes=boxes, labels=labels)\n",
    "            image    = transformed['image']\n",
    "            boxes    = np.array(transformed['bboxes'])\n",
    "            img      = np.moveaxis(image, 2, 0)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.Tensor(np.array([0])).type(torch.int64)\n",
    "        \n",
    "        target              = {}\n",
    "        target['boxes']     = torch.Tensor(boxes)\n",
    "        target['labels']    = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        target['area']      = area\n",
    "        target['iscrowd']   = iscrowd\n",
    "        \n",
    "        return img, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.trainx.shape[0]\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25006, 8) (122787, 8)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Wheat Dataset and Model Creation\n",
    "\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(valid_df.shape, train_df.shape)\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DIR_INPUT = '/media/yu-hao/WindowsData/WheatDataset'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST  = f'{DIR_INPUT}/test'\n",
    "\n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING NEW FILE >>    <<\n",
      "READING NEW FILE >>    <<\n"
     ]
    }
   ],
   "source": [
    "# [STAR] DBT Dataset and Model Creation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset     = DBTDataset(train_set=1)\n",
    "valid_dataset     = DBTDataset(train_set=0)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "#model       = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Training loop for DBT dataset\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "# params       = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer    = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
    "# lr_scheduler = None\n",
    "\n",
    "loss_hist     = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "prev_min   = 1000\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    model.train()\n",
    "    itr = 1\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "        \n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in valid_data_loader:\n",
    "            new_images  = []\n",
    "            for img in images:\n",
    "                new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "            images    = new_images\n",
    "            targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses     = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_hist.send(loss_value)\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "            itr = itr+1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} Train loss: {loss_hist.value}\")\n",
    "    print(f\"Epoch #{epoch} Val   loss: {val_loss_hist.value}\")\n",
    "    \n",
    "    if val_loss_hist.value < prev_min:\n",
    "        print('Saving the model ', prev_min, val_loss_hist.value)\n",
    "        torch.save(model.state_dict(), 'fasterrcnn_resnet50_dbt26.pth')\n",
    "        prev_min = val_loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #10 loss: 0.122077077627182\n",
      "Validation Iteration #20 loss: 0.14720208942890167\n",
      "Validation Iteration #30 loss: 0.1716037094593048\n",
      "Validation Iteration #40 loss: 0.11694562435150146\n",
      "Validation Iteration #50 loss: 0.16063688695430756\n",
      "0.1624028943479061\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For printing the loss of the trained model\n",
    "\n",
    "# fasterrcnn_resnet50_dbt7.pth  0.24080992616713048\n",
    "# fasterrcnn_resnet50_dbt8.pth  0.1653416310250759\n",
    "# fasterrcnn_resnet50_dbt9.pth  0.17630461007356643\n",
    "# fasterrcnn_resnet50_dbt10.pth 0.17438715264201166\n",
    "# fasterrcnn_resnet50_dbt11.pth 0.16590506657958032\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "val_loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt26.pth'))\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        if itr % 10 == 0:\n",
    "            print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "        itr = itr+1\n",
    "\n",
    "print(val_loss_hist.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_target1  = all_target\n",
    "all_scores1  = all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing inference of the model\n",
    "\n",
    "all_target = []\n",
    "all_scores = []\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt8.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "for images, targets, image_ids in valid_data_loader:\n",
    "    new_images  = []\n",
    "    for img in images:\n",
    "        new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "    images    = new_images\n",
    "    targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images)\n",
    "    #print(loss_dict)\n",
    "    \n",
    "    all_scores.append(loss_dict[0]['scores'].data.cpu().numpy())\n",
    "    all_target.append(loss_dict[0]['boxes'].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth  [[353.    51.   459.75 151.5 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] Code to compare the ground truth and predicted mask\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "case_index        = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c = valid_dataset[case_index]\n",
    "\n",
    "print('Ground Truth ', b['boxes'].data.cpu().numpy())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "ax   = plt.gca()\n",
    "\n",
    "if(len(all_target1[case_index]) > 0):\n",
    "    #print(all_target1[index])\n",
    "    #print(all_scores1[index])\n",
    "    \n",
    "    temp  = all_target1[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='yellow', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 9')\n",
    "\n",
    "if(len(all_target[case_index]) > 0):\n",
    "    #print(all_target[index])\n",
    "    #print(all_scores[index])\n",
    "    \n",
    "    temp  = all_target[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 8')\n",
    "\n",
    "temp  = b['boxes'].data.cpu().numpy()#all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='red', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing the inference on the test images\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Some code to do visualization\n",
    "\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "fp    = \"/home/yu-hao/Downloads/coco_sample.png\"\n",
    "image = cv2.imread(fp)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Plot image 1\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#train_dataset1     = DBTDataset(train_set=1)\n",
    "#index = random.randint(0, len(train_dataset)-1)\n",
    "#image, b, c = train_dataset[index]\n",
    "\n",
    "case_index       = random.randint(0, len(train_dataset)-1)\n",
    "image, b, c = train_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "# transform = A.Compose(\n",
    "#     [A.HorizontalFlip(p=0.95)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "# )\n",
    "\n",
    "# temp_box = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "# temp_box[0][0] = temp_box[0][0]\n",
    "# temp_box[0][2] = temp_box[0][2]\n",
    "# temp_box[0][3] = temp_box[0][3]\n",
    "# temp_box[0][1] = temp_box[0][1]\n",
    "\n",
    "# random.seed(7)\n",
    "# transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "\n",
    "plt.imshow(image)\n",
    "ax   = plt.gca()\n",
    "\n",
    "temp  = b['boxes']#b[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Some code to test the augmentation\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#case_index  = random.randint(0, len(valid_dataset)-1)\n",
    "image, b, c = valid_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "transform = A.Compose(\n",
    "    #[A.HorizontalFlip(p=0.99)],\n",
    "    [A.VerticalFlip(p=0.99)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    ")\n",
    "\n",
    "temp_box       = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "temp_box[0][0] = temp_box[0][0]\n",
    "temp_box[0][2] = temp_box[0][2]\n",
    "temp_box[0][3] = temp_box[0][3]\n",
    "temp_box[0][1] = temp_box[0][1]\n",
    "temp           = temp_box\n",
    "\n",
    "if(0):\n",
    "    transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "    image    = transformed['image']\n",
    "    temp     = transformed['bboxes']\n",
    "\n",
    "plt.imshow(image)\n",
    "ax    = plt.gca()\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images  = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes  = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the images\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = DBTDataset()\n",
    "#valid_dataset = DBTDataset()\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.5)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "#train_dataset = td1\n",
    "#model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for k in range(len(train_dataset)-1):\n",
    "        images, targets, image_ids = train_dataset[k]\n",
    "        #for images, targets, image_ids in train_data_loader:\n",
    "        #print(images, targets)\n",
    "        \n",
    "        images  = [images]\n",
    "        targets = [targets]\n",
    "        #images  = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        #losses = loss_dict['loss_rpn_box_reg'] + loss_dict['loss_objectness']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(loss_dict)\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "\n",
    "train_cate_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "train_attr_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "\n",
    "\n",
    "img_list = open(img_list_path).read()\n",
    "img_list = img_list.split(\"\\n\")[:-1]\n",
    "\n",
    "print(len(img_list))\n",
    "basepath = \"\"\n",
    "\n",
    "for i in tqdm(range(len(img_list))):\n",
    "    #print(img_list[i])\n",
    "    img_path = basepath+\"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"+img_list[i]\n",
    "    \n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    #print(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    \n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
