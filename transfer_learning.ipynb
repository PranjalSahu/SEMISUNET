{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [OLD] Code for classification\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/yu-hao/WindowsData/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Attribute and Category Model\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class MyAttrCateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model    = models.resnet18(pretrained=True)\n",
    "        self.model.fc = Identity()\n",
    "        \n",
    "        self.attr_layer = nn.Linear(512, 26)\n",
    "        self.cate_layer = nn.Linear(512, 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1     = self.model(x)\n",
    "        attr_out = self.attr_layer(out1)\n",
    "        cate_out = self.cate_layer(out1)\n",
    "        #cate_out = torch.flatten(cate_out)\n",
    "        return attr_out, cate_out\n",
    "\n",
    "# model  = MyAttrCateModel()\n",
    "# x      = torch.randn(1, 3, 224, 224)\n",
    "# output = model(x)\n",
    "# print(output[0].shape, output[1].shape)\n",
    "\n",
    "#print(model)\n",
    "#model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     16,
     51
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class AttrDataset(Dataset):\n",
    "    CLASSES = None\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_path,\n",
    "                 img_file,\n",
    "                 label_file,\n",
    "                 cate_file,\n",
    "                 bbox_file,\n",
    "                 landmark_file,\n",
    "                 img_size,\n",
    "                 idx2id=None):\n",
    "        self.img_path = img_path\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        # read img names\n",
    "        fp = open(img_file, 'r')\n",
    "        self.img_list = [x.strip() for x in fp]\n",
    "\n",
    "        # read attribute labels and category annotations\n",
    "        self.labels = np.loadtxt(label_file, dtype=np.float32)\n",
    "\n",
    "        # read categories\n",
    "        self.categories = []\n",
    "        catefn = open(cate_file).readlines()\n",
    "        for i, line in enumerate(catefn):\n",
    "            self.categories.append(line.strip('\\n'))\n",
    "\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def get_basic_item(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_path,\n",
    "                                      self.img_list[idx])).convert('RGB')\n",
    "\n",
    "        width, height  = img.size\n",
    "        # Very Important\n",
    "        # For getting the cropped and resized region of interest image\n",
    "        img.thumbnail(self.img_size, Image.ANTIALIAS)\n",
    "        img   = self.transform(img)\n",
    "\n",
    "        label    = torch.from_numpy(self.labels[idx])\n",
    "        cate     = torch.LongTensor([int(self.categories[idx]) - 1])\n",
    "\n",
    "        data = {'img': img, 'attr': label, 'cate': cate}\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_basic_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "img_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "img_size   = [224, 224]\n",
    "\n",
    "landmark_file = None\n",
    "bbox_file     = None\n",
    "\n",
    "d1 = AttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_dataloader(dataset, shuffle):\n",
    "    batch_size = 4\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=1,\n",
    "        pin_memory=False)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "train_data_loader = build_dataloader(d1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.531012058258057 0.7262052893638611 3.80480694770813\n",
      "0 5.088752746582031 0.7524378895759583 4.336314678192139\n",
      "0 4.794372081756592 0.6866499185562134 4.107722282409668\n",
      "0 4.411550998687744 0.6785265207290649 3.7330245971679688\n",
      "0 4.846611976623535 0.7013101577758789 4.145301818847656\n",
      "0 4.3664631843566895 0.6719178557395935 3.694545269012451\n",
      "0 4.092128753662109 0.6378979682922363 3.454231023788452\n",
      "0 3.709634304046631 0.6647182703018188 3.0449161529541016\n",
      "0 4.35942268371582 0.6420027017593384 3.7174201011657715\n",
      "0 4.091453552246094 0.660407304763794 3.4310460090637207\n",
      "0 4.7758989334106445 0.6002215147018433 4.175677299499512\n",
      "0 3.745849847793579 0.6061161160469055 3.1397337913513184\n",
      "0 4.1957926750183105 0.6125605702400208 3.5832319259643555\n",
      "0 4.179666519165039 0.5843924283981323 3.5952742099761963\n",
      "0 3.6139867305755615 0.6039941310882568 3.0099925994873047\n",
      "0 3.1789731979370117 0.5732211470603943 2.6057519912719727\n",
      "0 4.41791296005249 0.5395430326461792 3.8783698081970215\n",
      "0 3.828383445739746 0.5906289219856262 3.2377545833587646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c04075cab6ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcounter\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mnew_images\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mattr_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model  = MyAttrCateModel()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None\n",
    "\n",
    "ce_loss  = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "batch_size = 4\n",
    "counter    = 0\n",
    "model.train()\n",
    "for t1 in train_data_loader:\n",
    "    new_images  = torch.Tensor(t1['img']).to(device)\n",
    "    attr_target = t1['attr'].to(device)\n",
    "    cate_target = t1['cate'].to(device)\n",
    "    \n",
    "    out1, out2  = model(new_images)\n",
    "    cate_target = torch.reshape(cate_target, [batch_size])\n",
    "    #print(out1.shape, out2.shape, cate_target.shape, attr_target.shape)\n",
    "    \n",
    "    loss1      = bce_loss(out1, attr_target)\n",
    "    loss2      = ce_loss(out2,  cate_target)\n",
    "    \n",
    "    losses     = loss1 + loss2 #sum(loss for loss in loss_dict.values())\n",
    "    \n",
    "    if counter %50 == 0:\n",
    "        print(counter, losses.data.item(), loss1.data.item(), loss2.data.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    counter = counter+1\n",
    "\n",
    "#a = next(dloader)\n",
    "#print(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 120]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = Variable(torch.randn(10, 120).float())\n",
    "target = Variable(torch.FloatTensor(10).uniform_(0, 120).long())\n",
    "\n",
    "print(output.shape, target.shape)\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     4,
     37,
     72,
     128,
     221,
     227,
     232
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the functions for reading the data for Wheat Dataset\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    r1 = [float(x) for x in r]\n",
    "    r = r1\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df = pd.read_csv('/media/yu-hao/WindowsData/WheatDataset/train.csv')\n",
    "train_df.shape\n",
    "\n",
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "temp = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "#train_df[['x', 'y', 'w', 'h']] = \n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df['x'] = temp[:, 0]#train_df['x'].astype(np.float)\n",
    "train_df['y'] = temp[:, 1]#train_df['y'].astype(np.float)\n",
    "train_df['w'] = temp[:, 2]#train_df['w'].astype(np.float)\n",
    "train_df['h'] = temp[:, 3]#train_df['h'].astype(np.float)\n",
    "\n",
    "# df['bbox'] = df['bbox'].apply(lambda x: np.array(x))\n",
    "# x = np.array(list(df['bbox']))\n",
    "# print(x)\n",
    "# for i, dim in enumerate(['x', 'y', 'w', 'h']):\n",
    "#     df[dim] = x[:, i]\n",
    "\n",
    "# # df.drop('bbox', axis=1, inplace=True)\n",
    "# #df.head()\n",
    "\n",
    "class WheatDatasetOld(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.image_ids  = self.df['image_id'].unique()\n",
    "        self.image_dir  = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records  = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        im_name = image_id + '.jpg'\n",
    "        img = Image.open(self.image_dir/im_name).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']  = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class DBTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_set = 1, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.counter = 0\n",
    "        if train_set == 1:\n",
    "            self.train_start  = 0\n",
    "            self.train_end    = 150\n",
    "        else:\n",
    "            self.train_start  = 150\n",
    "            self.train_end    = 200\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        suffix_str  = ''#random.choice(['_m2', '_m1', '_p1', '_p2', ''])\n",
    "        print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "        self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "        self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        \n",
    "#         self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#         self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy.npy')[self.train_start:self.train_end]\n",
    "#         self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx.npy')[self.train_start:self.train_end]\n",
    "#         self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy.npy')[self.train_start:self.train_end]\n",
    "#         self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr.npy')[self.train_start:self.train_end]\n",
    "#         self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        self.transforms1 = A.Compose(\n",
    "                                    [A.HorizontalFlip(p=0.5),  A.VerticalFlip(p=0.5), ],\n",
    "                                     #A.Downscale(scale_min=0.75, scale_max=0.75,interpolation=3),],\n",
    "                                    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "                                   )\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        self.counter = self.counter+1\n",
    "        #if self.counter % 10 == 0:\n",
    "        #    print('Counter is ', self.counter)\n",
    "        \n",
    "#         if self.train_set == 1 and self.counter % 150 == 0 and random.random() < 0.2:\n",
    "#             suffix_str  = random.choice([ '_m1', '_p1', ''])\n",
    "#             print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "#             self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#             self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "                \n",
    "        img = self.trainx[idx, 0].astype('float32')/60000.0\n",
    "        img[img > 1] = 1\n",
    "        img = ndimage.interpolation.zoom(img, 0.25)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.concatenate([img, img, img], axis=0)\n",
    "        #if(0):\n",
    "        if(self.train_set == 1):\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        \n",
    "        boxes = np.array([self.coordx[idx]/4, self.coordy[idx]/4, self.width_arr[idx]/4, self.height_arr[idx]/4])#records[['x', 'y', 'w', 'h']].values\n",
    "        boxes = np.expand_dims(boxes, axis=0)\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        \n",
    "        area = self.width_arr[idx] * self.height_arr[idx]\n",
    "        area = torch.Tensor(area)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels =  torch.ones((1,)).type(torch.int64)\n",
    "        \n",
    "        if(self.train_set == 1):\n",
    "        #if(0):\n",
    "            transformed = self.transforms1(image=img, bboxes=boxes, labels=labels)\n",
    "            image    = transformed['image']\n",
    "            boxes    = np.array(transformed['bboxes'])\n",
    "            img      = np.moveaxis(image, 2, 0)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.Tensor(np.array([0])).type(torch.int64)\n",
    "        \n",
    "        target              = {}\n",
    "        target['boxes']     = torch.Tensor(boxes)\n",
    "        target['labels']    = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        target['area']      = area\n",
    "        target['iscrowd']   = iscrowd\n",
    "        \n",
    "        return img, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.trainx.shape[0]\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25006, 8) (122787, 8)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Wheat Dataset and Model Creation\n",
    "\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(valid_df.shape, train_df.shape)\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DIR_INPUT = '/media/yu-hao/WindowsData/WheatDataset'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST  = f'{DIR_INPUT}/test'\n",
    "\n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING NEW FILE >>    <<\n",
      "READING NEW FILE >>    <<\n"
     ]
    }
   ],
   "source": [
    "# [STAR] DBT Dataset and Model Creation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset     = DBTDataset(train_set=1)\n",
    "valid_dataset     = DBTDataset(train_set=0)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "#model       = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Training loop for DBT dataset\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "# params       = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer    = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
    "# lr_scheduler = None\n",
    "\n",
    "loss_hist     = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "prev_min   = 1000\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    model.train()\n",
    "    itr = 1\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "        \n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in valid_data_loader:\n",
    "            new_images  = []\n",
    "            for img in images:\n",
    "                new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "            images    = new_images\n",
    "            targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses     = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_hist.send(loss_value)\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "            itr = itr+1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} Train loss: {loss_hist.value}\")\n",
    "    print(f\"Epoch #{epoch} Val   loss: {val_loss_hist.value}\")\n",
    "    \n",
    "    if val_loss_hist.value < prev_min:\n",
    "        print('Saving the model ', prev_min, val_loss_hist.value)\n",
    "        torch.save(model.state_dict(), 'fasterrcnn_resnet50_dbt26.pth')\n",
    "        prev_min = val_loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #10 loss: 0.122077077627182\n",
      "Validation Iteration #20 loss: 0.14720208942890167\n",
      "Validation Iteration #30 loss: 0.1716037094593048\n",
      "Validation Iteration #40 loss: 0.11694562435150146\n",
      "Validation Iteration #50 loss: 0.16063688695430756\n",
      "0.1624028943479061\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For printing the loss of the trained model\n",
    "\n",
    "# fasterrcnn_resnet50_dbt7.pth  0.24080992616713048\n",
    "# fasterrcnn_resnet50_dbt8.pth  0.1653416310250759\n",
    "# fasterrcnn_resnet50_dbt9.pth  0.17630461007356643\n",
    "# fasterrcnn_resnet50_dbt10.pth 0.17438715264201166\n",
    "# fasterrcnn_resnet50_dbt11.pth 0.16590506657958032\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "val_loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt26.pth'))\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        if itr % 10 == 0:\n",
    "            print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "        itr = itr+1\n",
    "\n",
    "print(val_loss_hist.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_target1  = all_target\n",
    "all_scores1  = all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing inference of the model\n",
    "\n",
    "all_target = []\n",
    "all_scores = []\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt8.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "for images, targets, image_ids in valid_data_loader:\n",
    "    new_images  = []\n",
    "    for img in images:\n",
    "        new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "    images    = new_images\n",
    "    targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images)\n",
    "    #print(loss_dict)\n",
    "    \n",
    "    all_scores.append(loss_dict[0]['scores'].data.cpu().numpy())\n",
    "    all_target.append(loss_dict[0]['boxes'].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth  [[353.    51.   459.75 151.5 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] Code to compare the ground truth and predicted mask\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "case_index        = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c = valid_dataset[case_index]\n",
    "\n",
    "print('Ground Truth ', b['boxes'].data.cpu().numpy())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "ax   = plt.gca()\n",
    "\n",
    "if(len(all_target1[case_index]) > 0):\n",
    "    #print(all_target1[index])\n",
    "    #print(all_scores1[index])\n",
    "    \n",
    "    temp  = all_target1[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='yellow', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 9')\n",
    "\n",
    "if(len(all_target[case_index]) > 0):\n",
    "    #print(all_target[index])\n",
    "    #print(all_scores[index])\n",
    "    \n",
    "    temp  = all_target[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 8')\n",
    "\n",
    "temp  = b['boxes'].data.cpu().numpy()#all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='red', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing the inference on the test images\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Some code to do visualization\n",
    "\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "fp    = \"/home/yu-hao/Downloads/coco_sample.png\"\n",
    "image = cv2.imread(fp)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Plot image 1\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#train_dataset1     = DBTDataset(train_set=1)\n",
    "#index = random.randint(0, len(train_dataset)-1)\n",
    "#image, b, c = train_dataset[index]\n",
    "\n",
    "case_index       = random.randint(0, len(train_dataset)-1)\n",
    "image, b, c = train_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "# transform = A.Compose(\n",
    "#     [A.HorizontalFlip(p=0.95)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "# )\n",
    "\n",
    "# temp_box = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "# temp_box[0][0] = temp_box[0][0]\n",
    "# temp_box[0][2] = temp_box[0][2]\n",
    "# temp_box[0][3] = temp_box[0][3]\n",
    "# temp_box[0][1] = temp_box[0][1]\n",
    "\n",
    "# random.seed(7)\n",
    "# transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "\n",
    "plt.imshow(image)\n",
    "ax   = plt.gca()\n",
    "\n",
    "temp  = b['boxes']#b[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Some code to test the augmentation\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#case_index  = random.randint(0, len(valid_dataset)-1)\n",
    "image, b, c = valid_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "transform = A.Compose(\n",
    "    #[A.HorizontalFlip(p=0.99)],\n",
    "    [A.VerticalFlip(p=0.99)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    ")\n",
    "\n",
    "temp_box       = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "temp_box[0][0] = temp_box[0][0]\n",
    "temp_box[0][2] = temp_box[0][2]\n",
    "temp_box[0][3] = temp_box[0][3]\n",
    "temp_box[0][1] = temp_box[0][1]\n",
    "temp           = temp_box\n",
    "\n",
    "if(0):\n",
    "    transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "    image    = transformed['image']\n",
    "    temp     = transformed['bboxes']\n",
    "\n",
    "plt.imshow(image)\n",
    "ax    = plt.gca()\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images  = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes  = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the images\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = DBTDataset()\n",
    "#valid_dataset = DBTDataset()\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.5)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "#train_dataset = td1\n",
    "#model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for k in range(len(train_dataset)-1):\n",
    "        images, targets, image_ids = train_dataset[k]\n",
    "        #for images, targets, image_ids in train_data_loader:\n",
    "        #print(images, targets)\n",
    "        \n",
    "        images  = [images]\n",
    "        targets = [targets]\n",
    "        #images  = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        #losses = loss_dict['loss_rpn_box_reg'] + loss_dict['loss_objectness']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(loss_dict)\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "\n",
    "train_cate_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "train_attr_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "\n",
    "\n",
    "img_list = open(img_list_path).read()\n",
    "img_list = img_list.split(\"\\n\")[:-1]\n",
    "\n",
    "print(len(img_list))\n",
    "basepath = \"\"\n",
    "\n",
    "for i in tqdm(range(len(img_list))):\n",
    "    #print(img_list[i])\n",
    "    img_path = basepath+\"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"+img_list[i]\n",
    "    \n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    #print(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    \n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
