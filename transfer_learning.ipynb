{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "import csv\n",
    "from scipy import ndimage, misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [OLD] Code for classification\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/yu-hao/WindowsData/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Attribute and Category Model\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class MyAttrCateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model    = models.resnet18(pretrained=True)\n",
    "        self.model.fc = Identity()\n",
    "        \n",
    "        #self.attr_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 26, bias=False)\n",
    "        #                               )\n",
    "        \n",
    "        #self.cate_layer = nn.Sequential(nn.Linear(512, 128, bias=False), \n",
    "        #                                nn.ReLU(inplace=True),\n",
    "        #                                nn.Linear(128, 50, bias=False))\n",
    "        self.attr_layer = nn.Linear(512, 26)\n",
    "        self.cate_layer = nn.Linear(512, 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1     = self.model(x)\n",
    "        attr_out = self.attr_layer(out1)\n",
    "        cate_out = self.cate_layer(out1)\n",
    "        #cate_out = torch.flatten(cate_out)\n",
    "        return attr_out, cate_out\n",
    "\n",
    "#model  = MyAttrCateModel()\n",
    "# x      = torch.randn(1, 3, 224, 224)\n",
    "# output = model(x)\n",
    "# print(output[0].shape, output[1].shape)\n",
    "\n",
    "#print(model)\n",
    "#model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [
     15,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Data Loaders for Fashion Dataset\n",
    "\n",
    "from __future__ import division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class AttrDataset(Dataset):\n",
    "    CLASSES = None\n",
    "    \n",
    "    def __init__(self,\n",
    "                 img_path,\n",
    "                 img_file,\n",
    "                 label_file,\n",
    "                 cate_file,\n",
    "                 bbox_file,\n",
    "                 landmark_file,\n",
    "                 img_size,\n",
    "                 idx2id=None):\n",
    "        self.img_path = img_path\n",
    "\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size[0]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "        # read img names\n",
    "        fp = open(img_file, 'r')\n",
    "        self.img_list = [x.strip() for x in fp]\n",
    "\n",
    "        # read attribute labels and category annotations\n",
    "        self.labels = np.loadtxt(label_file, dtype=np.float32)\n",
    "\n",
    "        # read categories\n",
    "        self.categories = []\n",
    "        catefn = open(cate_file).readlines()\n",
    "        for i, line in enumerate(catefn):\n",
    "            self.categories.append(line.strip('\\n'))\n",
    "\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def get_basic_item(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_path,\n",
    "                                      self.img_list[idx])).convert('RGB')\n",
    "\n",
    "        width, height  = img.size\n",
    "        # Very Important\n",
    "        # For getting the cropped and resized region of interest image\n",
    "        img.thumbnail(self.img_size, Image.ANTIALIAS)\n",
    "        img   = self.transform(img)\n",
    "\n",
    "        label    = torch.from_numpy(self.labels[idx])\n",
    "        cate     = torch.LongTensor([int(self.categories[idx]) - 1])\n",
    "\n",
    "        data = {'img': img, 'attr': label, 'cate': cate}\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_basic_item(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "img_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "img_size   = [224, 224]\n",
    "\n",
    "landmark_file = None\n",
    "bbox_file     = None\n",
    "\n",
    "d1 = AttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "img_file   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val.txt\"\n",
    "label_file = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_attr.txt\"\n",
    "cate_file  = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/val_cate.txt\"\n",
    "\n",
    "d2 = AttrDataset(img_path, img_file, label_file, cate_file, bbox_file, landmark_file, img_size, idx2id=None)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def build_dataloader(dataset, batch_size, shuffle):\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=1,\n",
    "        pin_memory=False)\n",
    "    return data_loader\n",
    "\n",
    "train_data_loader = build_dataloader(d1, 4, True)\n",
    "val_data_loader   = build_dataloader(d2, 4, False)\n",
    "\n",
    "model  = MyAttrCateModel()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model  1000 3.706733783245087\n",
      "Epoch >>  0 3.706733783245087 1.5852622615098952 2.121471521139145\n",
      "Saving the model  3.706733783245087 3.608093111038208\n",
      "Epoch >>  1 3.608093111038208 1.5632715427279473 2.0448215703070165\n",
      "Saving the model  3.608093111038208 3.54785923631986\n",
      "Epoch >>  2 3.54785923631986 1.5387376344998678 2.0091216034293176\n",
      "Saving the model  3.54785923631986 3.4802480981349944\n",
      "Epoch >>  3 3.4802480981349944 1.5195611244142055 1.9606869755089282\n",
      "Saving the model  3.4802480981349944 3.4373667705059052\n",
      "Epoch >>  4 3.4373667705059052 1.502665749144554 1.9347010218739509\n",
      "Saving the model  3.4373667705059052 3.3982977171738944\n",
      "Epoch >>  5 3.3982977171738944 1.489756272315979 1.908541444003582\n",
      "Saving the model  3.3982977171738944 3.3664242402826035\n",
      "Epoch >>  6 3.3664242402826035 1.4763552834647042 1.8900689571499825\n",
      "Saving the model  3.3664242402826035 3.3373798230290412\n",
      "Epoch >>  7 3.3373798230290412 1.4659362310767174 1.8714435933977365\n",
      "Saving the model  3.3373798230290412 3.311692082617018\n",
      "Epoch >>  8 3.311692082617018 1.4561298532088598 1.85556223074264\n",
      "Saving the model  3.311692082617018 3.2898435755252837\n",
      "Epoch >>  9 3.2898435755252837 1.448099224603176 1.8417443519920111\n",
      "Saving the model  3.2898435755252837 3.2670461579452863\n",
      "Epoch >>  10 3.2670461579452863 1.4388573986617001 1.8281887600340627\n",
      "Saving the model  3.2670461579452863 3.24755376102527\n",
      "Epoch >>  11 3.24755376102527 1.430348778426647 1.8172049836081763\n",
      "Saving the model  3.24755376102527 3.2266909006192135\n",
      "Epoch >>  12 3.2266909006192135 1.4226666749715806 1.8040242263181852\n",
      "Saving the model  3.2266909006192135 3.2060500978657176\n",
      "Epoch >>  13 3.2060500978657176 1.4143637125747544 1.791686385922134\n",
      "Saving the model  3.2060500978657176 3.1925706651767096\n",
      "Epoch >>  14 3.1925706651767096 1.4079408996025722 1.7846297661771378\n",
      "Saving the model  3.1925706651767096 3.177514464430511\n",
      "Epoch >>  15 3.177514464430511 1.4016513608917593 1.7758631041301414\n",
      "Saving the model  3.177514464430511 3.167025299598189\n",
      "Epoch >>  16 3.167025299598189 1.3972978972722503 1.7697274026932086\n",
      "Saving the model  3.167025299598189 3.1532048953705365\n",
      "Epoch >>  17 3.1532048953705365 1.3908009351988633 1.7624039606112574\n",
      "Saving the model  3.1532048953705365 3.1448001604268425\n",
      "Epoch >>  18 3.1448001604268425 1.3868739371393857 1.7579262233870594\n",
      "Saving the model  3.1448001604268425 3.1343386335551737\n",
      "Epoch >>  19 3.1343386335551737 1.3817679327696561 1.752570700725913\n",
      "Saving the model  3.1343386335551737 3.122762404402097\n",
      "Epoch >>  20 3.122762404402097 1.376755231238547 1.7460071732557956\n",
      "Saving the model  3.122762404402097 3.110803522348404\n",
      "Epoch >>  21 3.110803522348404 1.371627854455601 1.7391756678331982\n",
      "Saving the model  3.110803522348404 3.1026282415700996\n",
      "Epoch >>  22 3.1026282415700996 1.3675910765865575 1.7350371647606726\n",
      "Saving the model  3.1026282415700996 3.0967976085940996\n",
      "Epoch >>  23 3.0967976085940996 1.3649200514083106 1.7318775569802771\n",
      "Saving the model  3.0967976085940996 3.088679738693237\n",
      "Epoch >>  24 3.088679738693237 1.3614958798265457 1.7271838586443662\n",
      "Saving the model  3.088679738693237 3.0800198659163254\n",
      "Epoch >>  25 3.0800198659163254 1.3570451427973234 1.7229747229098127\n",
      "Saving the model  3.0800198659163254 3.072652842609971\n",
      "Epoch >>  26 3.072652842609971 1.3533480484971294 1.7193047939373387\n",
      "Saving the model  3.072652842609971 3.067621755263635\n",
      "Epoch >>  27 3.067621755263635 1.3501445681388888 1.717477187049708\n",
      "Saving the model  3.067621755263635 3.0622918807638104\n",
      "Epoch >>  28 3.0622918807638104 1.3473896242976189 1.7149022564708158\n",
      "Saving the model  3.0622918807638104 3.057267521282037\n",
      "Epoch >>  29 3.057267521282037 1.3441071814397971 1.7131603396251798\n",
      "Saving the model  3.057267521282037 3.0511972603874824\n",
      "Epoch >>  30 3.0511972603874824 1.3414605259818415 1.7097367342580712\n",
      "Saving the model  3.0511972603874824 3.046697902422398\n",
      "Epoch >>  31 3.046697902422398 1.338642712144181 1.7080551899764687\n",
      "Saving the model  3.046697902422398 3.041227789846334\n",
      "Epoch >>  32 3.041227789846334 1.3356953023563731 1.7055324873526891\n",
      "Saving the model  3.041227789846334 3.036635764995042\n",
      "Epoch >>  33 3.036635764995042 1.3331736993877328 1.7034620653101626\n",
      "Saving the model  3.036635764995042 3.0329640556948525\n",
      "Epoch >>  34 3.0329640556948525 1.3308069950427328 1.7021570603983742\n",
      "Saving the model  3.0329640556948525 3.0309290158583058\n",
      "Epoch >>  35 3.0309290158583058 1.3289582466334104 1.7019707690419423\n",
      "Saving the model  3.0309290158583058 3.025894615933702\n",
      "Epoch >>  36 3.025894615933702 1.326096817989607 1.6997977976436551\n",
      "Saving the model  3.025894615933702 3.02232378565324\n",
      "Epoch >>  37 3.02232378565324 1.3238816898308303 1.69844209564438\n",
      "Saving the model  3.02232378565324 3.0206221072979464\n",
      "Epoch >>  38 3.0206221072979464 1.3223497825387196 1.6982723245628368\n",
      "Saving the model  3.0206221072979464 3.016116029268503\n",
      "Epoch >>  39 3.016116029268503 1.320027668593824 1.6960883602440358\n",
      "Saving the model  3.016116029268503 3.0110411173221543\n",
      "Epoch >>  40 3.0110411173221543 1.3172720842346912 1.6937690327062112\n",
      "Saving the model  3.0110411173221543 3.0101878265341124\n",
      "Epoch >>  41 3.0101878265341124 1.31605242997266 1.6941353962141132\n",
      "Saving the model  3.0101878265341124 3.0083085907046185\n",
      "Epoch >>  42 3.0083085907046185 1.3142813178120658 1.6940272725210634\n",
      "Saving the model  3.0083085907046185 3.0050660020207816\n",
      "Epoch >>  43 3.0050660020207816 1.3121112320314754 1.69295476955277\n",
      "Saving the model  3.0050660020207816 3.0022081839203834\n",
      "Epoch >>  44 3.0022081839203834 1.3106190718041526 1.691589111618201\n",
      "Saving the model  3.0022081839203834 2.9996441232678683\n",
      "Epoch >>  45 2.9996441232678683 1.3087453627586365 1.690898759725787\n",
      "Saving the model  2.9996441232678683 2.997242667232422\n",
      "Epoch >>  46 2.997242667232422 1.3068914289170124 1.6903512374681045\n",
      "Saving the model  2.997242667232422 2.99583850233381\n",
      "Epoch >>  47 2.99583850233381 1.3053750832589963 1.6904634181917646\n",
      "Saving the model  2.99583850233381 2.9949965103584892\n",
      "Epoch >>  48 2.9949965103584892 1.3040131286285361 1.6909833805723762\n",
      "Saving the model  2.9949965103584892 2.992740984095335\n",
      "Epoch >>  49 2.992740984095335 1.3021840839648247 1.6905568989233672\n",
      "Saving the model  2.992740984095335 2.991830701291561\n",
      "Epoch >>  50 2.991830701291561 1.3010195103638313 1.6908111897968485\n",
      "Saving the model  2.991830701291561 2.9904121684879064\n",
      "Epoch >>  51 2.9904121684879064 1.2996727364773935 1.690739430959838\n",
      "Saving the model  2.9904121684879064 2.9876103277375115\n",
      "Epoch >>  52 2.9876103277375115 1.2980468999186776 1.689563426830718\n",
      "Saving the model  2.9876103277375115 2.986848818164181\n",
      "Epoch >>  53 2.986848818164181 1.2969823362424417 1.689866480940194\n",
      "Saving the model  2.986848818164181 2.9863429348111152\n",
      "Epoch >>  54 2.9863429348111152 1.295987161867727 1.6903557720103046\n",
      "Saving the model  2.9863429348111152 2.9854646429302436\n",
      "Epoch >>  55 2.9854646429302436 1.2946067865517523 1.6908578553644142\n",
      "Saving the model  2.9854646429302436 2.9849482126183675\n",
      "Epoch >>  56 2.9849482126183675 1.2939340956059464 1.691014115968033\n",
      "Saving the model  2.9849482126183675 2.9840109977711893\n",
      "Epoch >>  57 2.9840109977711893 1.2929745493926879 1.6910364472372779\n",
      "Saving the model  2.9840109977711893 2.9835444567294442\n",
      "Epoch >>  58 2.9835444567294442 1.291907742006799 1.6916367136789328\n",
      "Saving the model  2.9835444567294442 2.982447170200944\n",
      "Epoch >>  59 2.982447170200944 1.2910634501233698 1.6913837188759198\n",
      "Saving the model  2.982447170200944 2.9820432534305774\n",
      "Epoch >>  60 2.9820432534305774 1.2900736477790309 1.691969604467515\n",
      "Saving the model  2.9820432534305774 2.981046881257526\n",
      "Epoch >>  61 2.981046881257526 1.2889946450230576 1.692052235131302\n",
      "Saving the model  2.981046881257526 2.9805772763840737\n",
      "Epoch >>  62 2.9805772763840737 1.2881207759176927 1.6924564994488445\n",
      "Saving the model  2.9805772763840737 2.980455071357079\n",
      "Epoch >>  63 2.980455071357079 1.2872259847861716 1.6932290854469174\n",
      "Saving the model  2.980455071357079 2.9801599820072835\n",
      "Epoch >>  64 2.9801599820072835 1.2862021037564828 1.693957877013775\n",
      "Epoch >>  65 2.980556497543147 1.285486159956365 1.695070336340616\n",
      "Epoch >>  66 2.9812575182647847 1.2846565917556856 1.6966009253491439\n",
      "Epoch >>  67 2.980815791173893 1.2837441140641184 1.6970716759665485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch >>  68 2.98108657586229 1.283104816812536 1.6979817579505236\n",
      "Epoch >>  69 2.9807398827484675 1.2820206053733825 1.6987192762831491\n",
      "Epoch >>  70 2.981746399137336 1.2813100512094902 1.7004363469283137\n",
      "Epoch >>  71 2.9832570031765435 1.2807669837441709 1.7024900184373062\n",
      "Epoch >>  72 2.983599851280043 1.2800341404782583 1.7035657096943013\n",
      "Epoch >>  73 2.983886082318989 1.2793920789699296 1.704494002258254\n",
      "Epoch >>  74 2.984089857764244 1.278799746875763 1.7052901098461946\n",
      "Epoch >>  75 2.985128415437121 1.2782139584688763 1.706914455915458\n",
      "Epoch >>  76 2.986864750374447 1.2779637269966013 1.7089010224030292\n",
      "Epoch >>  77 2.9878246316649975 1.2774413354519085 1.7103832952198215\n",
      "Epoch >>  78 2.9888871601299396 1.277007777889318 1.7118793812864284\n",
      "Epoch >>  79 2.9907124207504094 1.2768066825799644 1.713905737194838\n",
      "Epoch >>  80 2.99288799489869 1.276807951696125 1.7160800422932723\n",
      "Epoch >>  81 2.9941176470881556 1.2766283639792988 1.7174892821852117\n",
      "Epoch >>  82 2.994952917599534 1.2762016176768096 1.7187512990059892\n",
      "Epoch >>  83 2.995729803870831 1.2758314417739 1.719898361158025\n",
      "Epoch >>  84 2.9967281573975786 1.2754846250074752 1.7212435314440553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-2b9fa688276d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [STAR] Training Loop\n",
    "\n",
    "loss_hist1  = Averager()\n",
    "loss_hist2  = Averager()\n",
    "loss_hist3  = Averager()\n",
    "\n",
    "\n",
    "ce_loss  = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "batch_size = 4\n",
    "counter    = 0\n",
    "model.train()\n",
    "prev_min = 1000\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for t1 in train_data_loader:\n",
    "        new_images  = torch.Tensor(t1['img']).to(device)\n",
    "        attr_target = t1['attr'].to(device)\n",
    "        cate_target = t1['cate'].to(device)\n",
    "\n",
    "        out1, out2  = model(new_images)\n",
    "        cate_target = torch.reshape(cate_target, [batch_size])\n",
    "        #print(out1.shape, out2.shape, cate_target.shape, attr_target.shape)\n",
    "\n",
    "        loss1      = 5*bce_loss(out1, attr_target)\n",
    "        loss2      = ce_loss(out2,  cate_target)\n",
    "\n",
    "        losses     = loss1 + loss2 #sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        counter =  counter+1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t1 in val_data_loader:\n",
    "            new_images  = torch.Tensor(t1['img']).to(device)\n",
    "            attr_target = t1['attr'].to(device)\n",
    "            cate_target = t1['cate'].to(device)\n",
    "\n",
    "            out1, out2  = model(new_images)\n",
    "            cate_target = torch.reshape(cate_target, [batch_size])\n",
    "            #print(out1.shape, out2.shape, cate_target.shape, attr_target.shape)\n",
    "\n",
    "            loss1      = 5*bce_loss(out1, attr_target)\n",
    "            loss2      = ce_loss(out2,  cate_target)\n",
    "\n",
    "            losses     = loss1 + loss2\n",
    "\n",
    "            loss_hist1.send(loss1.data.item())\n",
    "            loss_hist2.send(loss2.data.item())\n",
    "            loss_hist3.send(losses.data.item())\n",
    "    \n",
    "    if loss_hist3.value < prev_min:\n",
    "        print('Saving the model ', prev_min, loss_hist3.value)\n",
    "        torch.save(model.state_dict(), 'fashion_cate_attr_resnet18_single_linear.pth')\n",
    "        prev_min = loss_hist3.value\n",
    "    \n",
    "    print('Epoch >> ', epoch, loss_hist3.value, loss_hist1.value, loss_hist2.value)\n",
    "\n",
    "#a = next(dloader)\n",
    "#print(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0,
     4,
     37,
     72,
     128,
     221,
     227,
     232
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the functions for reading the data for Wheat Dataset\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    r1 = [float(x) for x in r]\n",
    "    r = r1\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df = pd.read_csv('/media/yu-hao/WindowsData/WheatDataset/train.csv')\n",
    "train_df.shape\n",
    "\n",
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "temp = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "#train_df[['x', 'y', 'w', 'h']] = \n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df['x'] = temp[:, 0]#train_df['x'].astype(np.float)\n",
    "train_df['y'] = temp[:, 1]#train_df['y'].astype(np.float)\n",
    "train_df['w'] = temp[:, 2]#train_df['w'].astype(np.float)\n",
    "train_df['h'] = temp[:, 3]#train_df['h'].astype(np.float)\n",
    "\n",
    "# df['bbox'] = df['bbox'].apply(lambda x: np.array(x))\n",
    "# x = np.array(list(df['bbox']))\n",
    "# print(x)\n",
    "# for i, dim in enumerate(['x', 'y', 'w', 'h']):\n",
    "#     df[dim] = x[:, i]\n",
    "\n",
    "# # df.drop('bbox', axis=1, inplace=True)\n",
    "# #df.head()\n",
    "\n",
    "class WheatDatasetOld(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.image_ids  = self.df['image_id'].unique()\n",
    "        self.image_dir  = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records  = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        im_name = image_id + '.jpg'\n",
    "        img = Image.open(self.image_dir/im_name).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']  = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class DBTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_set = 1, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.counter = 0\n",
    "        if train_set == 1:\n",
    "            self.train_start  = 0\n",
    "            self.train_end    = 150\n",
    "        else:\n",
    "            self.train_start  = 150\n",
    "            self.train_end    = 200\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        suffix_str  = ''#random.choice(['_m2', '_m1', '_p1', '_p2', ''])\n",
    "        print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "        self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "        self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        \n",
    "#         self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#         self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy.npy')[self.train_start:self.train_end]\n",
    "#         self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx.npy')[self.train_start:self.train_end]\n",
    "#         self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy.npy')[self.train_start:self.train_end]\n",
    "#         self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr.npy')[self.train_start:self.train_end]\n",
    "#         self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        self.transforms1 = A.Compose(\n",
    "                                    [A.HorizontalFlip(p=0.5),  A.VerticalFlip(p=0.5), ],\n",
    "                                     #A.Downscale(scale_min=0.75, scale_max=0.75,interpolation=3),],\n",
    "                                    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "                                   )\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        self.counter = self.counter+1\n",
    "        #if self.counter % 10 == 0:\n",
    "        #    print('Counter is ', self.counter)\n",
    "        \n",
    "#         if self.train_set == 1 and self.counter % 150 == 0 and random.random() < 0.2:\n",
    "#             suffix_str  = random.choice([ '_m1', '_p1', ''])\n",
    "#             print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "#             self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#             self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "                \n",
    "        img = self.trainx[idx, 0].astype('float32')/60000.0\n",
    "        img[img > 1] = 1\n",
    "        img = ndimage.interpolation.zoom(img, 0.25)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.concatenate([img, img, img], axis=0)\n",
    "        #if(0):\n",
    "        if(self.train_set == 1):\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        \n",
    "        boxes = np.array([self.coordx[idx]/4, self.coordy[idx]/4, self.width_arr[idx]/4, self.height_arr[idx]/4])#records[['x', 'y', 'w', 'h']].values\n",
    "        boxes = np.expand_dims(boxes, axis=0)\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        \n",
    "        area = self.width_arr[idx] * self.height_arr[idx]\n",
    "        area = torch.Tensor(area)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels =  torch.ones((1,)).type(torch.int64)\n",
    "        \n",
    "        if(self.train_set == 1):\n",
    "        #if(0):\n",
    "            transformed = self.transforms1(image=img, bboxes=boxes, labels=labels)\n",
    "            image    = transformed['image']\n",
    "            boxes    = np.array(transformed['bboxes'])\n",
    "            img      = np.moveaxis(image, 2, 0)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.Tensor(np.array([0])).type(torch.int64)\n",
    "        \n",
    "        target              = {}\n",
    "        target['boxes']     = torch.Tensor(boxes)\n",
    "        target['labels']    = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        target['area']      = area\n",
    "        target['iscrowd']   = iscrowd\n",
    "        \n",
    "        return img, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.trainx.shape[0]\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25006, 8) (122787, 8)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Wheat Dataset and Model Creation\n",
    "\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(valid_df.shape, train_df.shape)\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DIR_INPUT = '/media/yu-hao/WindowsData/WheatDataset'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST  = f'{DIR_INPUT}/test'\n",
    "\n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING NEW FILE >>    <<\n",
      "READING NEW FILE >>    <<\n"
     ]
    }
   ],
   "source": [
    "# [STAR] DBT Dataset and Model Creation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset     = DBTDataset(train_set=1)\n",
    "valid_dataset     = DBTDataset(train_set=0)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "#model       = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] Training loop for DBT dataset\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "# params       = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer    = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
    "# lr_scheduler = None\n",
    "\n",
    "loss_hist     = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "prev_min   = 1000\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    model.train()\n",
    "    itr = 1\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "        \n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in valid_data_loader:\n",
    "            new_images  = []\n",
    "            for img in images:\n",
    "                new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "            images    = new_images\n",
    "            targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses     = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_hist.send(loss_value)\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "            itr = itr+1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} Train loss: {loss_hist.value}\")\n",
    "    print(f\"Epoch #{epoch} Val   loss: {val_loss_hist.value}\")\n",
    "    \n",
    "    if val_loss_hist.value < prev_min:\n",
    "        print('Saving the model ', prev_min, val_loss_hist.value)\n",
    "        torch.save(model.state_dict(), 'fasterrcnn_resnet50_dbt26.pth')\n",
    "        prev_min = val_loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #10 loss: 0.122077077627182\n",
      "Validation Iteration #20 loss: 0.14720208942890167\n",
      "Validation Iteration #30 loss: 0.1716037094593048\n",
      "Validation Iteration #40 loss: 0.11694562435150146\n",
      "Validation Iteration #50 loss: 0.16063688695430756\n",
      "0.1624028943479061\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For printing the loss of the trained model\n",
    "\n",
    "# fasterrcnn_resnet50_dbt7.pth  0.24080992616713048\n",
    "# fasterrcnn_resnet50_dbt8.pth  0.1653416310250759\n",
    "# fasterrcnn_resnet50_dbt9.pth  0.17630461007356643\n",
    "# fasterrcnn_resnet50_dbt10.pth 0.17438715264201166\n",
    "# fasterrcnn_resnet50_dbt11.pth 0.16590506657958032\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "val_loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt26.pth'))\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        if itr % 10 == 0:\n",
    "            print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "        itr = itr+1\n",
    "\n",
    "print(val_loss_hist.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_target1  = all_target\n",
    "all_scores1  = all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing inference of the model\n",
    "\n",
    "all_target = []\n",
    "all_scores = []\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt8.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "for images, targets, image_ids in valid_data_loader:\n",
    "    new_images  = []\n",
    "    for img in images:\n",
    "        new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "    images    = new_images\n",
    "    targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images)\n",
    "    #print(loss_dict)\n",
    "    \n",
    "    all_scores.append(loss_dict[0]['scores'].data.cpu().numpy())\n",
    "    all_target.append(loss_dict[0]['boxes'].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth  [[353.    51.   459.75 151.5 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] Code to compare the ground truth and predicted mask\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "case_index        = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c = valid_dataset[case_index]\n",
    "\n",
    "print('Ground Truth ', b['boxes'].data.cpu().numpy())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "ax   = plt.gca()\n",
    "\n",
    "if(len(all_target1[case_index]) > 0):\n",
    "    #print(all_target1[index])\n",
    "    #print(all_scores1[index])\n",
    "    \n",
    "    temp  = all_target1[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='yellow', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 9')\n",
    "\n",
    "if(len(all_target[case_index]) > 0):\n",
    "    #print(all_target[index])\n",
    "    #print(all_scores[index])\n",
    "    \n",
    "    temp  = all_target[case_index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 8')\n",
    "\n",
    "temp  = b['boxes'].data.cpu().numpy()#all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='red', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing the inference on the test images\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Some code to do visualization\n",
    "\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "fp    = \"/home/yu-hao/Downloads/coco_sample.png\"\n",
    "image = cv2.imread(fp)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Plot image 1\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#train_dataset1     = DBTDataset(train_set=1)\n",
    "#index = random.randint(0, len(train_dataset)-1)\n",
    "#image, b, c = train_dataset[index]\n",
    "\n",
    "case_index       = random.randint(0, len(train_dataset)-1)\n",
    "image, b, c = train_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "# transform = A.Compose(\n",
    "#     [A.HorizontalFlip(p=0.95)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "# )\n",
    "\n",
    "# temp_box = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "# temp_box[0][0] = temp_box[0][0]\n",
    "# temp_box[0][2] = temp_box[0][2]\n",
    "# temp_box[0][3] = temp_box[0][3]\n",
    "# temp_box[0][1] = temp_box[0][1]\n",
    "\n",
    "# random.seed(7)\n",
    "# transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "\n",
    "plt.imshow(image)\n",
    "ax   = plt.gca()\n",
    "\n",
    "temp  = b['boxes']#b[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Some code to test the augmentation\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#case_index  = random.randint(0, len(valid_dataset)-1)\n",
    "image, b, c = valid_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "transform = A.Compose(\n",
    "    #[A.HorizontalFlip(p=0.99)],\n",
    "    [A.VerticalFlip(p=0.99)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    ")\n",
    "\n",
    "temp_box       = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "temp_box[0][0] = temp_box[0][0]\n",
    "temp_box[0][2] = temp_box[0][2]\n",
    "temp_box[0][3] = temp_box[0][3]\n",
    "temp_box[0][1] = temp_box[0][1]\n",
    "temp           = temp_box\n",
    "\n",
    "if(0):\n",
    "    transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "    image    = transformed['image']\n",
    "    temp     = transformed['bboxes']\n",
    "\n",
    "plt.imshow(image)\n",
    "ax    = plt.gca()\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images  = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes  = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the images\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = DBTDataset()\n",
    "#valid_dataset = DBTDataset()\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.5)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "#train_dataset = td1\n",
    "#model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for k in range(len(train_dataset)-1):\n",
    "        images, targets, image_ids = train_dataset[k]\n",
    "        #for images, targets, image_ids in train_data_loader:\n",
    "        #print(images, targets)\n",
    "        \n",
    "        images  = [images]\n",
    "        targets = [targets]\n",
    "        #images  = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        #losses = loss_dict['loss_rpn_box_reg'] + loss_dict['loss_objectness']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(loss_dict)\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list_path   = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train.txt\"\n",
    "\n",
    "train_cate_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_cate.txt\"\n",
    "train_attr_path = \"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Anno_fine/train_attr.txt\"\n",
    "\n",
    "\n",
    "img_list = open(img_list_path).read()\n",
    "img_list = img_list.split(\"\\n\")[:-1]\n",
    "\n",
    "print(len(img_list))\n",
    "basepath = \"\"\n",
    "\n",
    "for i in tqdm(range(len(img_list))):\n",
    "    #print(img_list[i])\n",
    "    img_path = basepath+\"/media/yu-hao/WindowsData/MMFASHION-DATASET/CategoryandAttributePredictionBenchmark/Img/\"+img_list[i]\n",
    "    \n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    #print(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    \n",
    "    print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
