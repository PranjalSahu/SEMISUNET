{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [OLD] Code for classification\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/yu-hao/WindowsData/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     4,
     37,
     72,
     128,
     220,
     226,
     231
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the functions for reading the data for Wheat Dataset\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    r1 = [float(x) for x in r]\n",
    "    r = r1\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df = pd.read_csv('/media/yu-hao/WindowsData/WheatDataset/train.csv')\n",
    "train_df.shape\n",
    "\n",
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "temp = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "#train_df[['x', 'y', 'w', 'h']] = \n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df['x'] = temp[:, 0]#train_df['x'].astype(np.float)\n",
    "train_df['y'] = temp[:, 1]#train_df['y'].astype(np.float)\n",
    "train_df['w'] = temp[:, 2]#train_df['w'].astype(np.float)\n",
    "train_df['h'] = temp[:, 3]#train_df['h'].astype(np.float)\n",
    "\n",
    "# df['bbox'] = df['bbox'].apply(lambda x: np.array(x))\n",
    "# x = np.array(list(df['bbox']))\n",
    "# print(x)\n",
    "# for i, dim in enumerate(['x', 'y', 'w', 'h']):\n",
    "#     df[dim] = x[:, i]\n",
    "\n",
    "# # df.drop('bbox', axis=1, inplace=True)\n",
    "# #df.head()\n",
    "\n",
    "class WheatDatasetOld(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.image_ids  = self.df['image_id'].unique()\n",
    "        self.image_dir  = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records  = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        im_name = image_id + '.jpg'\n",
    "        img = Image.open(self.image_dir/im_name).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']  = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class DBTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_set = 1, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.counter = 0\n",
    "        if train_set == 1:\n",
    "            self.train_start  = 0\n",
    "            self.train_end    = 150\n",
    "        else:\n",
    "            self.train_start  = 150\n",
    "            self.train_end    = 200\n",
    "        \n",
    "        self.train_set = train_set\n",
    "        suffix_str  = ''#random.choice(['_m2', '_m1', '_p1', '_p2', ''])\n",
    "        print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "        self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "        self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        \n",
    "#         self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#         self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy.npy')[self.train_start:self.train_end]\n",
    "#         self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx.npy')[self.train_start:self.train_end]\n",
    "#         self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy.npy')[self.train_start:self.train_end]\n",
    "#         self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr.npy')[self.train_start:self.train_end]\n",
    "#         self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        self.transforms1 = A.Compose(\n",
    "                                    [A.HorizontalFlip(p=0.5),  A.VerticalFlip(p=0.5)],\n",
    "                                    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "                                   )\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        self.counter = self.counter+1\n",
    "        #if self.counter % 10 == 0:\n",
    "        #    print('Counter is ', self.counter)\n",
    "        \n",
    "#         if self.train_set == 1 and self.counter % 150 == 0 and random.random() < 0.2:\n",
    "#             suffix_str  = random.choice([ '_m1', '_p1', ''])\n",
    "#             print('READING NEW FILE >> ', suffix_str, ' <<')\n",
    "#             self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx'+suffix_str+'.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "#             self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "#             self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr'+suffix_str+'.npy')[self.train_start:self.train_end]\n",
    "                \n",
    "        img = self.trainx[idx, 0].astype('float32')/60000.0\n",
    "        img[img > 1] = 1\n",
    "        img = ndimage.interpolation.zoom(img, 0.25)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.concatenate([img, img, img], axis=0)\n",
    "        #if(0):\n",
    "        if(self.train_set == 1):\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "        \n",
    "        boxes = np.array([self.coordx[idx]/4, self.coordy[idx]/4, self.width_arr[idx]/4, self.height_arr[idx]/4])#records[['x', 'y', 'w', 'h']].values\n",
    "        boxes = np.expand_dims(boxes, axis=0)\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        \n",
    "        area = self.width_arr[idx] * self.height_arr[idx]\n",
    "        area = torch.Tensor(area)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels =  torch.ones((1,)).type(torch.int64)\n",
    "        \n",
    "        if(self.train_set == 1):\n",
    "        #if(0):\n",
    "            transformed = self.transforms1(image=img, bboxes=boxes, labels=labels)\n",
    "            image    = transformed['image']\n",
    "            boxes    = np.array(transformed['bboxes'])\n",
    "            img      = np.moveaxis(image, 2, 0)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.Tensor(np.array([0])).type(torch.int64)\n",
    "        \n",
    "        target              = {}\n",
    "        target['boxes']     = torch.Tensor(boxes)\n",
    "        target['labels']    = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        target['area']      = area\n",
    "        target['iscrowd']   = iscrowd\n",
    "        \n",
    "        return img, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.trainx.shape[0]\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25006, 8) (122787, 8)\n"
     ]
    }
   ],
   "source": [
    "# [STAR] Wheat Dataset and Model Creation\n",
    "\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(valid_df.shape, train_df.shape)\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DIR_INPUT = '/media/yu-hao/WindowsData/WheatDataset'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST  = f'{DIR_INPUT}/test'\n",
    "\n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING NEW FILE >>    <<\n",
      "READING NEW FILE >>    <<\n"
     ]
    }
   ],
   "source": [
    "# [STAR] DBT Dataset and Model Creation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset     = DBTDataset(train_set=1)\n",
    "valid_dataset     = DBTDataset(train_set=0)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "#model       = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.01)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.17090913653373718\n",
      "Epoch #0 Train loss: 0.25494575951444476\n",
      "Epoch #0 Val   loss: 0.21131338000297548\n",
      "Saving the model  1000 0.21131338000297548\n",
      "Validation Iteration #50 loss: 0.1508047878742218\n",
      "Epoch #1 Train loss: 0.20478282535546705\n",
      "Epoch #1 Val   loss: 0.19949753321707248\n",
      "Saving the model  0.21131338000297548 0.19949753321707248\n",
      "Validation Iteration #50 loss: 0.1471974402666092\n",
      "Epoch #2 Train loss: 0.19420352342881655\n",
      "Epoch #2 Val   loss: 0.2022747333844503\n",
      "Validation Iteration #50 loss: 0.13538601994514465\n",
      "Epoch #3 Train loss: 0.1775948271939629\n",
      "Epoch #3 Val   loss: 0.194413424693048\n",
      "Saving the model  0.19949753321707248 0.194413424693048\n",
      "Validation Iteration #50 loss: 0.19617493450641632\n",
      "Epoch #4 Train loss: 0.17426497881349765\n",
      "Epoch #4 Val   loss: 0.19177891567349434\n",
      "Saving the model  0.194413424693048 0.19177891567349434\n",
      "Validation Iteration #50 loss: 0.12641407549381256\n",
      "Epoch #5 Train loss: 0.15724163188746101\n",
      "Epoch #5 Val   loss: 0.19111673625806969\n",
      "Saving the model  0.19177891567349434 0.19111673625806969\n",
      "Validation Iteration #50 loss: 0.15363261103630066\n",
      "Epoch #6 Train loss: 0.1478390568181088\n",
      "Epoch #6 Val   loss: 0.1905852931525026\n",
      "Saving the model  0.19111673625806969 0.1905852931525026\n",
      "Validation Iteration #50 loss: 0.11813465505838394\n",
      "Epoch #7 Train loss: 0.15284190366142675\n",
      "Epoch #7 Val   loss: 0.18822601845487952\n",
      "Saving the model  0.1905852931525026 0.18822601845487952\n",
      "Validation Iteration #50 loss: 0.06825067102909088\n",
      "Epoch #8 Train loss: 0.14143547592194458\n",
      "Epoch #8 Val   loss: 0.18948357552289963\n",
      "Validation Iteration #50 loss: 0.0710659846663475\n",
      "Epoch #9 Train loss: 0.11900582870370463\n",
      "Epoch #9 Val   loss: 0.18768433655798436\n",
      "Saving the model  0.18822601845487952 0.18768433655798436\n",
      "Validation Iteration #50 loss: 0.09347592294216156\n",
      "Epoch #10 Train loss: 0.13855987473538048\n",
      "Epoch #10 Val   loss: 0.18573295888575642\n",
      "Saving the model  0.18768433655798436 0.18573295888575642\n",
      "Validation Iteration #50 loss: 0.09430127590894699\n",
      "Epoch #11 Train loss: 0.12809280443348384\n",
      "Epoch #11 Val   loss: 0.18721124143650134\n",
      "Validation Iteration #50 loss: 0.1532929241657257\n",
      "Epoch #12 Train loss: 0.12089205670513604\n",
      "Epoch #12 Val   loss: 0.1876932238500852\n",
      "Validation Iteration #50 loss: 0.07809046655893326\n",
      "Epoch #13 Train loss: 0.12233687603944227\n",
      "Epoch #13 Val   loss: 0.19113068358174393\n",
      "Validation Iteration #50 loss: 0.0952974334359169\n",
      "Epoch #14 Train loss: 0.12248554355219791\n",
      "Epoch #14 Val   loss: 0.1908941174844901\n",
      "Validation Iteration #50 loss: 0.07979447394609451\n",
      "Epoch #15 Train loss: 0.10289857889476575\n",
      "Epoch #15 Val   loss: 0.18998810245655476\n",
      "Validation Iteration #50 loss: 0.1277792900800705\n",
      "Epoch #16 Train loss: 0.10156419892844401\n",
      "Epoch #16 Val   loss: 0.18999880501452615\n",
      "Validation Iteration #50 loss: 0.07941966503858566\n",
      "Epoch #17 Train loss: 0.09868485617794488\n",
      "Epoch #17 Val   loss: 0.1900923535393344\n",
      "Validation Iteration #50 loss: 0.08485665172338486\n",
      "Epoch #18 Train loss: 0.09267645290023402\n",
      "Epoch #18 Val   loss: 0.18995614772564487\n",
      "Validation Iteration #50 loss: 0.08597462624311447\n",
      "Epoch #19 Train loss: 0.08822955710715369\n",
      "Epoch #19 Val   loss: 0.1906862893924117\n",
      "Validation Iteration #50 loss: 0.07978016138076782\n",
      "Epoch #20 Train loss: 0.08314946048745983\n",
      "Epoch #20 Val   loss: 0.191573991356861\n",
      "Validation Iteration #50 loss: 0.0972193032503128\n",
      "Epoch #21 Train loss: 0.08672984522816382\n",
      "Epoch #21 Val   loss: 0.1910560247369788\n",
      "Validation Iteration #50 loss: 0.05661020800471306\n",
      "Epoch #22 Train loss: 0.08455407913578183\n",
      "Epoch #22 Val   loss: 0.19089765538340028\n",
      "Validation Iteration #50 loss: 0.11987480521202087\n",
      "Epoch #23 Train loss: 0.08041771775797794\n",
      "Epoch #23 Val   loss: 0.19186134032905103\n",
      "Validation Iteration #50 loss: 0.06988624483346939\n",
      "Epoch #24 Train loss: 0.07935151104864321\n",
      "Epoch #24 Val   loss: 0.19321942259669303\n",
      "Validation Iteration #50 loss: 0.134355366230011\n",
      "Epoch #25 Train loss: 0.06969456521696166\n",
      "Epoch #25 Val   loss: 0.195862935970609\n",
      "Validation Iteration #50 loss: 0.1009906679391861\n",
      "Epoch #26 Train loss: 0.07306026549715745\n",
      "Epoch #26 Val   loss: 0.1962917736724571\n",
      "Validation Iteration #50 loss: 0.09876114130020142\n",
      "Epoch #27 Train loss: 0.07502716122881363\n",
      "Epoch #27 Val   loss: 0.1966675904127104\n",
      "Validation Iteration #50 loss: 0.1063142716884613\n",
      "Epoch #28 Train loss: 0.07322524850697894\n",
      "Epoch #28 Val   loss: 0.19747602807036763\n",
      "Validation Iteration #50 loss: 0.07417716085910797\n",
      "Epoch #29 Train loss: 0.0676062360013786\n",
      "Epoch #29 Val   loss: 0.19698881961405276\n",
      "Validation Iteration #50 loss: 0.05395570397377014\n",
      "Epoch #30 Train loss: 0.0637684361518998\n",
      "Epoch #30 Val   loss: 0.19673934770687934\n",
      "Validation Iteration #50 loss: 0.08278007805347443\n",
      "Epoch #31 Train loss: 0.0545354095336638\n",
      "Epoch #31 Val   loss: 0.19733856562059374\n",
      "Validation Iteration #50 loss: 0.06442857533693314\n",
      "Epoch #32 Train loss: 0.05790157292626406\n",
      "Epoch #32 Val   loss: 0.19839087689916293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/yu-hao/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/yu-hao/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/yu-hao/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/yu-hao/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fcd09262331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mimages\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mnew_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtargets\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlosses\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 147\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 147\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [STAR] Training loop for DBT dataset\n",
    "optimizer    = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "# params       = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer    = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
    "# lr_scheduler = None\n",
    "\n",
    "loss_hist     = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "prev_min   = 1000\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    model.train()\n",
    "    itr = 1\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "        \n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in valid_data_loader:\n",
    "            new_images  = []\n",
    "            for img in images:\n",
    "                new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "            images    = new_images\n",
    "            targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses     = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_hist.send(loss_value)\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "            itr = itr+1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} Train loss: {loss_hist.value}\")\n",
    "    print(f\"Epoch #{epoch} Val   loss: {val_loss_hist.value}\")\n",
    "    \n",
    "    if val_loss_hist.value < prev_min:\n",
    "        print('Saving the model ', prev_min, val_loss_hist.value)\n",
    "        torch.save(model.state_dict(), 'fasterrcnn_resnet50_dbt8.pth')\n",
    "        prev_min = val_loss_hist.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #10 loss: 0.1284685879945755\n",
      "Validation Iteration #20 loss: 0.14913664758205414\n",
      "Validation Iteration #30 loss: 0.16377830505371094\n",
      "Validation Iteration #40 loss: 0.13241739571094513\n",
      "Validation Iteration #50 loss: 0.18574297428131104\n",
      "0.1653416310250759\n"
     ]
    }
   ],
   "source": [
    "# [STAR] For printing the loss of the trained model\n",
    "\n",
    "# fasterrcnn_resnet50_dbt7.pth 0.24080992616713048\n",
    "# fasterrcnn_resnet50_dbt8.pth 0.1653416310250759\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "val_loss_hist = Averager()\n",
    "itr = 1\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt8.pth'))\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, image_ids in valid_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        #print(loss_dict)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_hist.send(loss_value)\n",
    "\n",
    "        if itr % 10 == 0:\n",
    "            print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "        itr = itr+1\n",
    "\n",
    "print(val_loss_hist.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing inference of the model\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt7.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "for images, targets, image_ids in valid_data_loader:\n",
    "    new_images  = []\n",
    "    for img in images:\n",
    "        new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "    images    = new_images\n",
    "    targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images)\n",
    "    #print(loss_dict)\n",
    "    \n",
    "    all_scores.append(loss_dict[0]['scores'].data.cpu().numpy())\n",
    "    all_target.append(loss_dict[0]['boxes'].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth  [[138.   265.   242.25 389.5 ]]\n",
      "[[154.61232  288.77435  232.8334   359.83374 ]\n",
      " [ 22.693367 164.965    203.38766  254.69232 ]\n",
      " [163.37927  299.561    220.11179  348.53287 ]\n",
      " [142.14096  267.54834  248.3329   378.92053 ]\n",
      " [  0.       419.9271    17.922033 433.68756 ]\n",
      " [ 74.37224  176.97662  162.8691   260.99304 ]\n",
      " [ 13.630266 136.45874  237.48563  299.47635 ]\n",
      " [197.81192  315.69077  242.92274  357.86285 ]\n",
      " [107.25957  178.59505  155.86446  234.40219 ]\n",
      " [205.24304  318.82568  227.19505  351.6955  ]]\n",
      "[0.8826992  0.75953615 0.7415096  0.6634392  0.27222005 0.23247214\n",
      " 0.07135642 0.06954542 0.06638096 0.05164719]\n",
      "[[ 82.55275 286.06796 169.2084  371.04608]\n",
      " [ 66.75437 274.86603 218.28217 375.17014]]\n",
      "[0.9640061  0.10090566]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALkAAAD8CAYAAAArOAWDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29eYzk53ke+Hx1/ur6VVVX39M9HA451JCKZFGXLcsbU7S1UhTDzgrYhQMnkYMA2Sh7xEgWsbz5I0AMA9oEMBxg18YKSXYVHdZ6LWktOTG5tLwESZOUeYoczj09M313dVXXfR/f/lH1vP39mkNOD9k9XVP9PcBgqqvr+FX1+73fezzv8ymtNSwsxhm+o74AC4vDhjVyi7GHNXKLsYc1couxhzVyi7GHNXKLscehGblS6vNKqUtKqatKqa8c1vtYWNwO6jDq5EopP4DLAD4LYBXASwD+ttb6/IG/mYXFbXBYnvyTAK5qrZe01m0A3wHwK4f0XhYW74rAIb3uCQArxs+rAH76nR7s8/m0z2fTA4v3jl6vl9NaT93qd4dl5OoW93niIqXUPwTwDwHA5/PBdd1DuhSL44BCoXDznX53WO5zFcCi8fMCgHXzAVrrr2mtP661/rhSt1oTFhYHg8My8pcAnFFK3a+UCgH4VQA/OKT3srB4VxxKuKK17iql/nsATwLwA/gPWuu3DuO9LCxuh8OKyaG1/s8A/vNhvb6FxX5hSxoWYw9r5BZjD2vkFmMPa+QWYw9r5BZjD2vkFmMPa+QWYw9r5BZjD2vkFmMPa+QWYw9r5BZjD2vkFmMPa+QWY4+RMHI7NGFxmDg0qu2d4OTJk/jyl7+Mn/zkJzh37hyKxSLK5TKs4q7FQWAkjLzb7eLMmTN49NFHEYlEUK/Xcf78eTz99NN48803USwW0ev1jvoyLe5RHIruyp1iZmZGf+ELX4BSCq7r4sSJE0gmk0in0+h0Orhw4QJefvllXLlyBeVyGd1u96gv2WLEUCgUXtFaf/xWvxsJT66UQrPZRLPZRKPRwPr6OhzHgeu6SCQScF0Xn/vc5/DJT34SN27cwMWLF7GxsYFms2kN3uK2GAkj7/f76Ha7aDabyOVyCAaDCIVC2NnZQSKRQCQSgeM4mJubw6lTp3DfffdhZ2cH+Xwey8vLWFtbQ71etwZvcUuMhJHTOPv9Pvr9Pur1OlqtFsLhMLrdLlqtFvx+P5rNJlKpFOLxOBYWFpBKpXD//fdjbW0NKysryOfzyOfzaDQaNmm1ENzWyJVS/wHALwHIaq3/2vC+CQD/F4BTAG4A+G+01oXh734LwD8A0APwP2qtn7zde/T7fdRqNTSbTbTbbXQ6HQSDQfT7fXQ6HdRqNQQCAeTzeUSjUYRCIcTjcSilEIvFcP/992N6ehqVSgXr6+vY2tpCNptFpVJBv99/j1+NxbhgP578/wTwvwL4j8Z9XwHwI631V4eKtV8B8JtKqUcw0Fj5IIB5AH+ulHpIa/2upREaebfbhdYaSin4/X50Oh20222prPj9flQqFQQCg8tmGFMsFhEOh6G1huu6cBwHiUQCzWYTxWIRuVwO7Xb7zr4Zi7HBbY1ca/2MUurUnrt/BcBjw9tfB/A0gN8c3v8drXULwHWl1FUMxD9fuM17oNfrQSmFSCQCAGg2mwAGEnJKKQk/tNbo9/vw+XzodDoAgO3tbfj9fvh8PiSTSQQCAUxMTCAej6NaraJYLKJUKknsbnG88F5j8hmt9QYAaK03lFLTw/tPAHjReNzq8L63wdRCDIfDcBxHwpNGo4FGowGfzwetNQKBgCwEevpwOCyP11ojFAohGo2iWq0iEomg2+2iWq3C7/cjGo0iHo/j7NmzuHTpEm7evIlWq/UeP7rFvYaDTjxvK/Qpd2r9NQBfA4BkMql9Ph9qtRr6/T601mLgwCAxVUohEAggGAwCGIQuSil0Oh0EAgEx6mg0il6vh0QigX6/j2aziXA4DL/fj1qthlOnTuH06dPIZrO4dOkSarXaAX8FFqOG92rkW0qpuaEXnwOQHd5/W6HPW4ExOTAIT4LBIHw+H7rdrvxMI/f7/eh2u+j1euj3++j1enK73++jXC4DAKLRKHw+HxzHQTqdliSU983Pz2Nubg4XL17EysqKjdnHGO/VyH8A4EsAvjr8/0+M+7+tlPpdDBLPMwD+6nYvRo8dCoXEEJVS8Pl8Ysh8jFJKCF18bL/fRyAQQCgUAjAIf7hoWH8vlUoIh8MIBoOoVquo1WqIxWJ48MEHsbi4iGw2i2vXrtkwZgyxnxLiH2KQZE4qpVYB/EsMjPuPlFL/AMAygP8aALTWbyml/gjAeQBdAP/d7SorwMB7M+GMRCLw+/3o9XpotVoSujBWZ6WFXj0QCMii6PV68Pv98jzW3zc2NqC1RqvVkvdxHAfb29vIZDIIBAI4efIkTp48iQsXLmB1ddVyZcYII8FdcV1Xf/KTn5TSYK1WQ7vdRjAYRDqdRjAYRKPRkNZ/s9mUWDwYDIrBK6XQ7XbR7XbFuwcCAbTbbfAkC1ZqmKh2Oh3x8PV6HadPn0a1WsVrr72Gra2to/xaLO4A78ZdGSkjbzabEprQ8MxSIVv8bP/TYwOQBeLz+eDz+RAKhWQXMJNZxu6dTkcWRTgcRiAQgOu6aLVaOHnyJMLhMC5duoQ33nhD3t9idDHyBC0mmYFAAJ1OR+JwVke63S4cxwEwqJ/H43FMTk6iXq9LWNNutyVEYfhihjQ0bt7P12Z5stFoAACCwSCWl5cRjUZx4sQJJBIJvPnmm8jn80f5FVm8D4yEkZOfAkC8sN/vR7VaFWPvdrsSsrB2zvZ/KBRCr9fzVF3MOJ2LiC1+hjHRaBSO44hXZyjT6XRQr9dRqVQwNTWFxx57DFevXsX58+etV78HMRJG3uv10Gg0EAgEoJQSj8v2vs/nQ7vdFuNtt9twHEdq6TRuYOCJmXDSsGncNHZ68EQiIY/j+/r9fvj9frTbbUQiEWxvb6Pf7+Ps2bO4//778cwzz2BnZ+doviiL94SRMHIA4kF9Pp80d8ywwzTofr+PdruNeDyObrcrNW4moc1mU3YGhj8sMfp8PvH6bPEzVGJ1JhwOo9lsygKrVCro9XoIhUJ4/PHHcf78eVy4cMEyHe8RjISRMyE0z/IMhUJC2KLBA4PFQP4KjTMcDqNSqcjPfFy73ZbnmuxGYJfDzvdimANAKjtcHExUu90uKpUKPvGJTyCRSODVV1+14cs9gJEwcnpEenEA0slkGMLfMeRgG58eOpVKod/vSwLpOA4ikYhwXujRtdbw+/2yCBjOME5nItrpdNBqteA4DhzHQblcxszMDKLRKAqFAh599FHMzMzgL//yL21SOuIYCSMHIFUQUmbpxWnkrLiY3BWGGD6fD+l0Wm6bxszEtNlsShfU/H273ZbqDB9nLrZqtQqtNaLRKKLRKABgbW0N/X4fqVQKv/iLv4jnn38eKysrt/hUFqOAkTFytvHN6kgoFPI0cxiy0ACVUnAcR4wvEAgglUpJPM5Ekt44FAqh1Wqh0+lI7N3v91EsFqWbysXF+B6AxOQ3btwAMFhgGxsb0kn9yEc+AsdxsLS0ZDulI4iRMHKzdk1eOfkppNXubeYwtAgGg1Jp4X2RSMSzIJRS8jg2nMwdIpVKSVfVXCCM2Wn87I72+33h2QSDQZRKJXzxi1/EzZs38YMf/MBy1kcMI2HkADzswnA47DFCM1kEIHF4OBxGJBJBOByWDimNOhQKSVmw1+shEokgHo+Lx2YzqN1uy3MJVnn2js6Zc6cAEIvFoJRCq9XC1tYWHnnkEfR6Pfzwhz+UoQ+Lo8fIGDmN0xx9M6stbMHTKweDQeGJm6DHbzQa6Pf74nkZc3M4Q2sNx3HE4FmnLxaLHqkLNp34euTI8DVIMzh37hw2NzfhOA5+6qd+Cm+++ab16COCkTHyVqsFn88Hv98v8hJsy5OjQgNjQqm1lsYOJ/oBiCHX63XhpSilEI1G5bnmY4EBPZc5QL1el1Z/u92WfIGhCXMA7jLtdhuxWAyO4+Dy5ctYWFjApz71Kbz++uu28jICGAkjZ/zL5JJhAg2dzSHGwiwBcvSNrX4aY61Wk0YOFwEAYR/Ss2utkUqlZMCCyW65XJZwhl690+mgWq1KJQYY7C71el0Wx40bN9Dr9ZDL5TA9PY3Pfvaz+NGPfoTt7e0j+24tRsTIAXjKhWank7Vx3mboQKotuedmSx6AdC8BSDLJx3U6HRmLY12dnj8ajcJ1XfHgXCCcXnJdV6Tq+Dt2XsvlsiSwa2trqFQq+NjHPoaXXnrJevQjxMgYebfbFe8bDAbFSAOBgFRPWAcHvBNCHGljuEOPbsbrpPCajEUA0jDy+/2IxWJSsTET0larhX6/j6mpKSlbZrNZ8eIkkzHh5WLK5XJIJBL4zGc+g2effdby048II2Pk5lS+OdnD0ISxOeNgzn0yeWQSatbAaegmO9GkCHBnaDabSCaTSKVSWFhYAAA0Gg1sbGyg0WigUCig1+vhxIkTaDQacF0XPp8PGxsbqFarWFtbk2uMx+OIxWLCg9/Y2MDU1BQ+/elP47nnnkM2m323r8HiEDASRs6kkGEJY3J6W9bFXdeV+Njn8yEWi8F1XQC7O4GZWDLRBAaenN1MChCZ/JdMJoNkMolYLIZarQbHcdBut9FsNjE1NYVisSj0XJYnHcfB6uoqCoUCwuEwotEoisUiOp0OXNdFLBYTQw8EAnj88cfx1FNP2dDlLmNkjJxemGEI410aIZM7sxxIBS3+4+PpxR3HkYQ0GAwiHo8jHo9LZYa0AIZGSink83n4/X70+324ritydK7rQmuNSqUiO4XrutIoarVaKJVKiMVi8nMikRC+ei6XQyAQwGOPPYY///M/R6lUOsqv/FhhJIwcgCSdDEkymYx401AoJI0eNnVo1PF4HK7rIhwOi1Aow5Fer4dAIIBYLOYhazEE4uM4zU+vTxouvTMnk5LJJObn57G0tCRJaLvdRr1eFwWAWq2GcDgsr5NOpyUs2tjYQDwex6OPPooXXnjBKgPcJexnWn8RAx3EWQB9AF/TWv/bgxb9JH0WAObm5pBMJpFIJJBKpQYXOqyyRKNRzM3NIRQKoVQqSYUlFoshmUyi2WzKbmBKOTOEocemB2foQ7664zjyXCa6XGTcFZLJpCzKhYUFNBoNZLNZFAoFVCoVtFotNBoNJBIJqbiYQ9qBQAAf/OAHcf78edsZvQvYjyfvAvhnWutXlVIJAK8opZ4C8Os4QNFPAOJ1w+Ewtre3sb6+Dr/fD9d1RZs8EokIM5BGy0EJdjQJ6rAQDD045sYWP8Mh1uFbrZZUSYBd8pgpIx2NRqUMeebMGeHGRKNRlMtllEolVKtV2S263S5isZgs5OnpaWQyGTz77LPW0A8Z+xH83ABA3cOKUuoCBvqGByr6CUDKhMvLy6hWqwiFQmJ00WgUlUpFJJ79fj8ikQii0agYKb2zyWFhgtputyUppaFzZyA6nY4833Ecj246+efMGziA0ev1kEqlsLi4iHA4jGQyie3tbTiOg0qlIq39YDAoCa3P50OxWMTs7Cx++qd/Gi+++KINXQ4RdxSTD9VtHwXwY7xP0U9T8JMGymNV+v2+JHCcyNna2vKo3lIzhQ0cJnjRaFQSTXZF2Rnl4ASNmEbL3cC4NpkV5fVUKhVhMHLRsPPZbreRSCQQjUal28oFWi6XhffCfIPNrvX1dUxMTODs2bN466237EkZh4R9G7lSKg7guwB+Q2tdfpezN/cl+mkKfkYiEc2xtr0xMzDw8JubmzI9T+1xVjhYC2dMXavV0Ol0EI/H4ff7USqVxEDpzaempoS2a2oqMnRIpVISm5uy0VQNcF1XJDFIB3YcB4FAAI1GQxZSKBRCLpeThUZDZ9m02WzCdV088sgjOHfunD004BCwLyNXSgUxMPBvaa2/N7z7wEQ/uWBY66b0MsMH8laKxSJarRYmJiaEYlutVpFIJJBIJMRDuq6LdDqNUCgkIYMpNdHr9dBut5HJZGSB0AhZVaHsc7fblUVDY6aCVyaTwdLSknh97gbJZFISVia0uVxOQiTyYDgf2u/3MT8/j16vh/Pnz9sB6QPGfqorCsC/B3BBa/27xq8OTPTT1DE0p/LpRWkw5uFZTPKi0SharZanowkMWIVzc3OYmJhANBqV8mK1WgUw6GjW63VEIhExOu4cZu2cnpoLIxKJQCmFcrns4caYnVjK25Hmy+czESX5ixWlmZkZ5HI5kai7efPm/v56FvvCfjz5pwH8XQBvKqVeH973P+OART9ZCqxUKgB2G0Tm9s3QgsMOTBKDwaC058kzLxQKqNVqcvoEQyGWGem5WalhMkuvXi6XpVLDhcbyIsOYWq2GaDSKbDaLUCgkOQGviQ2rdruNqakpCWWUUh7hJMbshUIBn/jEJ9Dv9+3M6AFiP9WV53DrOBsAfuEdnvM7AH5nvxdBI2I5jz8z/KDxkWlID99sNrGzsyPGQlJVp9MRUSCK8cfjcSkpMglk15Gc81gsJtdQrVbRaDQQiUSEi6KUkrJjOp1GoVCA4zgSwpjdWVZeHMdBq9WSUKVYLKJSqXjESVn+pIbMz//8z+MHP/iBaK1bvD+MRMeTbfy9Mm+moZsxrskHbzQacoTK5OQklFLY3NyUE+KCwaDHc9PQqdrFoWXzIAAAUsVhHE9KQCaTEUPPZDJotVpIpVK4cuWKGDNVAHg7FotJaGOeaAdABj7IiSmXy4hGo3j00UfxV3/1V0IFtnjvGAkjB3YlJ8gpYfhiJmckcLECYtbF2+02crmc1NdZ5jNH6Fg2NLuZ3BFMxiMTTMb89M5sIpmclLW1NQSDQXzqU5/C0tKSUG7N841MpiQ9fyAQ8CwqLhDW35VS+NCHPoRXXnnFKgC8T4yEkZuah6Yh722Q0EAZtnBBmKCxcmYzn8/LWUJkMvK22TgiTZasxr0C/7VaTXYMJpQ0XMbZH/3oR3H9+nUZ6Gg0GiiXy555Ve5IqVRKBEwBSNWH00ikNjz00EO4cOHCXfgrjC9GwsiBXemJcDgs3UmTdku6LW/zOYzhzZMlgIHRMKb1+XyIx+OIRqOIRCJIpVJIJpNi8Fw0DGk4PMHXNFmRHGKuVCoIhUJIJBKyKBuNBqanp9FqtYQoxokjlkLb7TZc1xXC2NLSkoQkbCBVKhURI11cXES5XMba2trd+UOMIUbCyM2SoWmYDCfMw2pNWixb96bHp6GbKlisYDC+brVaQg/gWUKRSEQ8N2m2nPYHICFRrVaTujsPxXUcB9VqVfIEzoKSC8OdgtRg/t7v9yOXy6FQKEgdnypihUJBrun06dMol8tSebK4M4yEkQO75UHz6EJO8ZghiXlsCmNegp6fPBNgUDmZnJwUz8g4vd1uo1AoIB6PixR0LBbzlADppbmgeARjvV5Ho9GQJhE9P8uObObwczD5JC2B2oqhUAj33XefsBZ5XaaIUrPZRCQSwZkzZyxr8T1iZIzc9NxsrNBzA7uJpykOCuwmn/yduQBYliwWi9KIocgn691MAm/cuIFEIoFkMolMJoN0Oo3Z2VlEo1Gh/cZiManotFot7OzsyGR+KBTC6uoq0um0DDabjEfWzUkJTqVSUrVpt9u4fv26LKButytHu5h03/vuuw/Xrl2zHJc7xEgZuSmWT/ahaeT8nenBWcVot9seZVrT8Gk4pvScOXkEQEqR2WxWDD4SiWB2dlbUbCcnJyWW5yhduVxGPp9HPB5HsVgUdiEAqc6YR8QwvOG8aC6XQ71eR7PZxPb2thDCWFJkGMf4f25uzjaK7hAjYeTmwIRZZ95blWClYy9xisZrCoWaySgF9GnYfE1TUxGAdFhJ5uLiyOfzCIfDmJiYgOu6WFxcFFrBzs4Otra2UK1WkUqlsLa2hu3tbWFRArsjfLxNPRhe6/T0NObm5uS9TCIXAFEs6Ha7yGQyqNVqEsdb3B4jYeQ0WrOiYSaQZqy+FwxpWMtmK51EKHMcDoCMs+1NUPkzX4+enzyZeDyOVquFzc1NVKtVnDx5UnYZ1tkjkYhMA3HBMFThoiPJjNIbgUAA6XQa8/PzqNVqwnEh95wipfF4XIx6amoKvV7PzonuEyNh5AA8qlimx2W3E9gVIDK7oQxdTAk5lgJJruLUD18zGo3K7sH7yQGnp2V4AUBovaQGbGxsIJ1Oo9/vixZirVZDsViUXIKJpNmd5YxqvV6XmDsQGJxdND8/j0qlIobMejkATyeUXBuGRNbQb4+RMHIzRDGrLDQ0Gqk5qWN6ZzPcYKJnCnOaJC96VlN+jg0mck1o7K1WS8hgWmuJx6vVKq5cuYLJyUkpMzYaDSlTApDbjuMI34ZlSFaBKIsRj8cxMzPzNoHQXC4n78334eLtdrtYWFi4ZdPMwouRMHLA27an8ZGrQuMHIJ7bfDywO4cZDoelBm1KyDEUoqc1h5MBeCoyTExZhmTrP5/PS0hETZZkMimhiqm7aJ48xx2Hx7MAkCFqnnfkOA7m5+c943LAwNAZOpFVydCn2WzixIkTuHHjhh22eBeMjJGbRsit3kwQzcfRoE3Dp/c2z+sEdo8Z32uApOWamon1et0jN8fk8Y9eeglzR+Qt10MhfHp+XjjsXFSUpkulUpiamrISdO+CkTByk3Vo1siZkLLBwm0a2A1RAHjKjKa4J8MgPp7xOkOGWCwmWoZ8jWq1KtNGrusOKh+tFv6Ln/s5dLtdJJNJTE9PS7Xj5MmTiMfjAIBisYh6vY5arSan0RGUuet2u5idnZV6eywWQyaTgc/nk/es1+tYXl5GNpvFH37nO0gmk8JzMQ8o4PVOTk56hqYtvPDd/iGHD9Nrm0bHcAHwxt2mRybtldRZ1pzZtt9bU2fowkYRdwAmgOSV8PX5XrOzs5IsZrNZNJtNOI6Dra0t4dpMTk6KJiM/Dzu3JgenWCyiVCqhVquh0WhIfb1er3uaR+l0GgBw8uRJKYGSHswEnZSFxcVFT55isYuR+FZohDSoYDAoZUDTWOjNGZ7QWPnzrWJ1xs6m1jgTOTPhNP8Hds8BpW5ho9HwCA8x/GHsbIZMbOSws8pdhKVIn8/nETVlqbNer2NrawvFYhGxWEyoCNFoFAsLC0IV3ku9bbVaiMVimJqaOuS/1L2JkQhX6JlMxVme/ECj41AzsBtnm80gludoxDQgjrWZ86MsQ7K7aEo/A16KARmC9XpdJvCpbRiJRPDII48gkUhIk4riSD6fzyNeRPpAu91GuVyWayAzMZFIIJ/Py7SRybfRWiMej2NyclIYj3sNvVar4cEHH8TOzo49QHcPRsbITWoqAKkhMzbnIIFZATF1DfmPr8FWOh+718Pz8fV6XeJ25gJ8jtlICofD8jr01iRdccaUrfdarYaJiQnU63VhObK7SQ46hyto6KyJc/EFAgFMTk4CGBzAVa1WkUwmEY/Hsby8LAMXZn+h1+thfn7eDkLvwcgY+d7bNHyz2bO3XGg+hj/TC3Map9fr4a3/9J/QmZ9/7xeoFJ74sz+77cMmymX8zre/LfXvEydOCDuRoYvjOFJBosetVqtSR6cujFn7Zj2e0tG1Wg3lctlDAOt2u9je3sbMzAxWV1ftNJGB/UhSOACeARAePv6Ptdb/Uh2w4CeNkzG32cQx/5AMN2jwbLawE0gPzxCh2+2iMz+Pj3384574mB7YVLYKBoNynIrrugiFQhKu/L0vfUkEh1qtFn77t7+Jycmq97tyNf7RP/ryPr72d8fmpoNf//WfF8F+v9+PdDotu86pU6fkVGiGL1QKSyQSmJiYsOcUGdiPJ28BeFxrXR2KDD2nlPozAF/EAQl+0jOb4YTZ3WSYYBoxf8/Ez2Qgmp6ciEQinq4pO5t8T3OogovN1D0HgJ2dHQAYVlKq+OIX/ys56Nbn8wF/APz2b/8rETbq9XooFArIZrNStpydncXKyorox1BkiOEVALz00ssSuwPwnH5XqVSQTqfhuq4cDMD+gM/nQ6FQwOTkJHK5nCVwDXHb6ooegC4rOPynMRD2/Prw/q8D+FvD2yL4qbW+DoCCn+8IelgOK3BL5/ZtJpgMW0zvzj+wmYCyBGmSumjQjNmZWDJBNEWEmJRyEXGYAoDEw/1+H/l83nNyxNbWFnZ2dkQZwHVdYS+Sd+44jqgL8LPws9LQ6/W6TALVajWRuuOI4OLi4i0PLmDXlxNNFvuXifMDeAXAgwD+N631j5VSByr4aSaKZhWFIYXZxaSnNsMXngrXaDSEe8KQBYBQW4FdZQA2T0z6LqsdyWQSACQpZQUlGo0ik8kAAObn50Ww37y+RqOBfD4vxyf6/X6Uy2Xp6nI6aGpqClprqYiYFSSzQlKpVFCtVhEIBJBMJlEoFJBKpZBIJKTlzyoS6QoUUbLYp5EPQ42PKKVSAL6vlPpr7/LwOxb8jMViWiklTRU2VBifDx/vibf3DjPTQBnKMPyhsZg1d/7PhcBYnJUOLpjNzU0Ui0UAwOrqqucsIgDS+qf+CgA518hkJLK+zbE5VooCgQAmJiYkgTaTTTakCCoVkDC2uLiITCYju4gZivl8PqRSKWSzWRuy4A6rK1rrolLqaQCfxwEKfgK7HpNhB2vbrVZLRsfotc3fMQ5nSZDGwNtmt9OkArAUGIlEEAqFhGdeLpdRr9fRbrc9g8NXrlxBKBSStj4ACXNmZ2flvm63KyVCeniOzeXzeZn64eKKx+NSg08mk/Ke5sKhIFKtVhN5ukgkIhz0vVwfYCA6SlrvccdtY3Kl1NTQg0MpFQHwiwAuYlfwE3i74OevKqXCSqn7sQ/BT7mY4R+KIYNSSlSwWEkw9QkDgYBHqZa1aC4CM543qQLmVNHwc8lrmhP2jJcBiD5iNpvF6uoqgF01LEq/ARAqAiXnqtUqCoWCCI0y9GLuMDc3h4cffhjT09PSTAIgc6YAhH1ItQASzqLRKE6dOiUD2FyobPnzKJrjjv148jkAXx/G5T4Af6S1/lOl1As4QMFPs2LC0IQcDv7e5JGz5szYnGU0hjQAZNYTgHhQ07PzfhqFUkpmMGlUNFqGTt1uVwYVeKKb2TSanZ0Vz03+TKvVEkWuTqcjTR7G4FprzMzMAIDE0UCysB8AACAASURBVPF4XIyUDSRSHSKRCJLJJGq1mkeYyWykua4rHdPjjv0Ifr6BwekSe+/P44AEP4FdJiKTQjZ3SCvllm3GncBundxxHAkfzFPcaNQMReih6VG5MDhulkwmZQfg2UEApOZuHg7AQ6/8fr/UpZPJpBizSQ/gVH4ulxOOeKFQADBIaqkMYM51TkxMANjdHVKplCjp+v1+oddSB8asJJFhSWdxnDESHU9gN0beq7PC3zFJpJelMZiGRq9uaioS/EOzimG21GkgzWZTzuncy+hjSdOcD2WHlWxHYHfHYalzYmJCkt9cLifnIfFzcBfq9XpyZCKwu6gAyHQQh6cjkQgKhYIsFH4ehlmJRAJTU1Mi/G+NfERAo9g7UGzOd5LeanJXgN2pHo6UMW5nE4XPNxmL3B3YMTRFPxkWmN6RTRdzYoiGlU6nJQSZm5sTSi61ExuNBl599VUsLS2J1os5Z+rz+VCpVGRcDhgkm/TUZpmUMnrkvHNxskMbjUaRSqUwNzeHXq+Hc+fOeYRFjyNGwshNT8MQg55475CEWRvndmyGENVqVbZqshIByBQQY3wmaMBuyMNpeZPHbpYbCZb3tNYy4MyE8caNG0in01hYWEC3OzjQq16vS7eUqgJcrJzbbLfbHpnmyclJbGxsyPUxvgcgAxnhcFikpHmt7L62Wi2cOHFCJCyOM0bCyIHdereZRLEKwRDB7HAC3qEGE+wQsjHC1+dzAHhiX3Y/WYo0Z0r5+mQGkiQFDJJMx3E8FZcXXngBc3Nz6Pf7KJVKkqiarX6+Nr02CVxcuADwxhtveI5+4cLmz8AupYED0rzuQCCAlZUVnDlzBjMzM1heXj6Av9C9i5EwchqgKahjdieZSJqNHj7O5KCbNFuGOuZEDnkwNBazuwoMQgSGLVxAe+dMZ2ZmRA6i2WyiUCh4GIWlUgmO4+AnP/kJZmZmxHB5VDrZg9xVWCVi6ZKv8/LLL4uMNI16byWHi77f70ulxefzSb29WCwinU57vofjiJEwcgAe782tGIDnZ3MO1JzLBLwa50zUSIoC4JFcM7VcGIfzuSRt0ePzPTKZDE6dOgW/3y9hxOrqqgh0mtWfUqmETqfjmW4KBALIZDKoVqtyfApLnEwWHcdBLpcDMODKMGzi9bARxO+lWCyKkVO8n8KlTMJd1/XkEccRI2fkjJ3NGBiAlAaZdJpxvFkJYUxqhjR8jHn/Xhovf28mwNz6gUFo0u/3US6XxdBI4jKPN+cABeNwcsN5nOLs7Cxu3ryJdruNUqkk3U/W6FlWZCfUBAehKaXBY2MYQpmlVo7ascZujXxEYE7Um7xyLoC9MhV7DZSVE4YZ5h+WBsMaOUMgs7ZsHldOQ6eR8wQ5dld5Dfzf7MTyf4ZS3W5XOCasiW9sbIhOOY9+2d7efpsgP1+LOxrr+dFoVFQCzPfnruS6LlKplJxqcZy1zUfCyGmse7uaZnhCnW+GIOa8Jg2VvHM+1yRyUayIvzMTOXM750IDBg2Vhx56CHjuORSLRfR6Pezs7IjBMBGlGL95zaFQSIhmPFWCzEee3MzxOU7r5/N5icMnJibkOpiwcpfhwiblgd9Nt9sVSrDJmJyZmTnWSrgjYeQApNECQAzDjLvpmc2GkTmMzG0cgKeCQdCj8rmMxblAGKrE43E5Udnv90vZzoyHmS/wWszH8UyhRCIhjEq+B0OTdrstp13wGHUOMHPnoKoWADn1gouW7+n3+5FIJN4mJdfpdFAqlVCv1+WAgOOMkfj0/AOZ2zIA8UjErYySkm0kdZmenEnkrV6LNfZoNCpDGjRKeth2uy1U22QyiWAwKOU+ACINZ1IJXNdFPB5HOBxGqVTyTCzRQMltr1QqaDabiMViMrlk0gj2nnnE5JN8GE4/md8XyVzcCbLZ7LE/D3RkjJxa4MCuMZvzmHvZe9ym6aXMZNFsf7Mkx6kc05OTUst2frfbRSwWk5yAQ8UAhHNerVY9qlwMGbiYWCosFosyP0qPXK/XRWELgMgv5/N5TE9PS6jC9+Nr1mo1xGIxTw5gDknwc3MBxGIxBINB5PN51Ot1qbcfV4yEkZvlP9MzmYkfY3P+QanJYiaXbMOzrBcMBqUTaRKgGFcXi0UhdfE92OI3KbvAgE/OFrrP58PmpoNnn33O+zkA/P7v/8H7/j5yubgndEmn09K0ohARsFtt4WRTr9eTw7R46FetVpNu63HFSBg54FXRMuvf9OAmN9w0bBo+R+VYYgQGHpAJod/vx87OjiwMNmlMRSqzdr63TBmJRIQmUK1W8aUv/XUZKOZ14/eBf/yPvyylRrb8mTBubm7KQVc0YJ4BGo/HEYlERKTf7696aAfmFBONmLouZijEnILKvo1G423l2OOGkTBylv7MFjxjbMA72sWDYuv1usS6TPLIRKRHzufzEo++8MILMrrGkAfwnlXE92L1wqy0cCqfCy4YDGJ6elpq0KZEBk+TM0fpTDEgDiy7rithGas0sVjsbVrnpVIJlUpFJqQ4qM0FZ5Y+mXTze6pUKse6Rg6MiJEDXn1wYLeCwQFnc8aT5Tgmn6lUSuLvfr8vRsWFElhdxVvnzr33i1MK3/zGN277MHdnR6gBFB1lSdGUq6PRkUnISpE5dW/mG+Sfs1lmLmRWVvg8AJ6cIZfLHeuWPjBCRm7WyflHZsOHXjUej4u4D7Ar5rm6uipJmBm7sxv4gc9/3lO5oedkiZLJrSl9wR2C+MzjjyMQCOCBBx4QKeVarQa/34+pqSkx0M4wJyDll5M8nBRiBYi1b8bVrHFzuAOATAYx2SUFIBaLSf7CgQlWZ1qtFsrlMuLxuBj5ccfIGLnZsaRHZ2zN7p9Jvd3r2SkYSsOiV+NiMTukBAWE2GAhQYzdSvMMn1qtJkepcLyNFF0OIrMsCAyqOSxB0tt2u10ZhSM/Pp/Pe6i+AEQSg1URXht3BdMZcKCbMTiVAgB4bh9njIyRM3HiZA//gOb5PkwaGb8Hg0HPsEOv15PKCLDLMjS3crM0qbVGIpHwlCM5KLy37e+6roRQ2WwW0WjUU3/m9SUSCamG8BqKxaKH986FRJFQsyOqtcbU1BQmJiY8cTwAESRil5QkMGBXWpp8+Ha7LXX8446RMHKTo7JXMWvv4/Yy/ujd6L05fLDXsNnoMaf1GdporUXf23VdD5ux2+0CL7+MWCwmlRQK8fOamHhSR7HRaEjszaPKmYT2ej3UajUPzRaAxOX9fh9zc3PY2dmRgWnez+SUzSvSAYBdeQyzn8BE+bhj30Y+nNZ/GcCa1vqX1AEKfjKMMOvfLAkSNByWz/jHBOB5nknmMrufpjQFY2AKe05OToqoJtvyrOA0Gg3k4nH8P3/yJ2+/8LuA7FBtq9vtyrgbHYFpxKy8cLeYn5+XhXjccSee/J8AuACAbbmv4AAFP7ntmhUCU0HL5JczvjXLb/KB9pCyGAaYg8Hkdp89exbdbhdLS0uyiPiPnU+tNf7ZF7+IRCKBUqmEzc1N2SkYMnFu0ww7WMZjCMUQKZ/Po1KpeOrac3NziEajaLVamJ+fx8rKCgqFAlqtlvDEyVMnvYCLFdgdw2OYxZq+qdF4nLFfLcQFAH8TA5mJfzq8+1cAPDa8/XUATwP4TRiCnwCuK6Uo+PnCO72+mWhyOMKsOgDwhBZy8YHd0yX2xp7c2klYYgc0Go1icnISCwsLWF9fx5UrVzyjbfTi5XJZjNxsjTNEYgeVhshKCc/+5AnKrVZLDN0U9ORnMvsBExMTyGaz6Pf70sbf2dnxNIO2trZkgDoajUpyzPyEMX6z2bRJ5xD79eS/B+CfAzDVag5U8HOvdzIPrSJYQ2e5zjzn0+S9MKzhQVXks0xOTiKTyaDZbGJ9fR3ValU8MCs3WmuZ3mFIxKYMy4rJZBJKKdTrdWm1M1fgjmDG4TRQ6rhEo1GRiKYhZzIZYQ9y1ykWizLWRuOlYherS2Q5mkMljuNgY2Pj2DeBiP2I8P8SgKzW+hWl1GP7eM07FvxMJBKahsFQxOSfMAFljXkvH5yJFkuBwWBQWHys1ESjURSLRVy+fFmmj2igTBzj8bgcJchEzixl+v1+UbsloYw7i+M4KBQKIkfBkIWfg4kxFzRZhcwFlFIyNBEIBFAqlUTJlskmdzzzGkkJYFWKn31zc3Mff6rjgf148k8D+GWl1BcAOABcpdQ3cYCCnyQ+MS6n5yUdlqELy4Ec+GUoAcBTSWFyytqx67rIZrMolUqyCMxZUT7P9NimYhbj3FgsJl7UHJVjm50LptlsCnOSC4V1bu5YsVgMsVgMJ06cQLvdxtLSkvBWGo2GeH2eAsdwhMc4cpfYS0kIhUKoVCrHnl5rYj8ycb8F4LcAYOjJ/yet9d9RSv0bDIQ+v4q3C35+Wyn1uxgknrcV/KQ3NBMytrG57dPrARCRIdbMWXMeXqPEqTQwUk7NhJSvZZ5LxFiaBsgwhPRZ6ohzgVBk3xyL297elkXEHadarUpLnporjuPg5MmTiEQiOH/+vIQXzWYT5XJZTn8zW/3MLwDvXCt3lGAwiEQigUuXLh171SwT76dO/lUckOAny3VMHs1hB3O2kyQkSjgAu6xF06jo8Un04uuax4zzf8dx4LqucF2A3QkeemsuKBo1X3Nqagr9fh/ValXq82bDh2VN83oYYoXDYTiOg/X1dWxubnqUeRmiMISjDB2npegAWE7kd5RKpVCtVo89tXYv7lSf/GkMqijQByj4yRo5sDsjaSac5JYwgaORMcnk8/dWWBiCmFu6SQ1gPZnVDGqhM9lkssjOI0MQn8+HyclJUbVi6ECjM7VhTNYilWrT6TROnjyJ1dVVZLNZz4wmPTbb/zRifv5arSahGh1Br9dDIpFAKpXCa6+9Zr34HoxMx5NbLcfJzLgXgGz3rGrQa5tVEFNuwhyVo+emEQKDIWUeHUhdF1ZieBoEva45lMzr2Nzc9EwQmd1Zk2Nihhps91Ngf2VlRbRTAEitm4uMBC3W47PZLOr1uuQa3NlisRjS6TRyuZyNxW+BkTDySCSCn/3Zn0UwGJTtm9Jo3L7Zsme1Yi+jkBUW/t4cguYsJxdDKpUS0XueoMbX5KADRe4dx5HjE8l03NjYEGMyZ065S/A+xvo+30DVKhwOY3JyUho+xWLRU6NnXkAxUPLNe70estksCoWCx+P7fD7EYjGkUil0Oh1cu3btaP6AI46RMPJer4f19XWhjZphiWnELDHurZ2TpWgen8IYmBUNzoXymBFO1mg9OAGOi4aP5/gb43DOci4tLclsKLArYUHPyqSZ781ubjgcxuzsrAh0bmxseDqWzC1IEaZoKWNsLjSWRDlgwWt76aWXLBnrHTASRt5ut3HlyhXZ3kl/JQPRrLSY3BN6XzaDmCSyzEhSluu6mJ+fx/z8PCqVCi5fvoyNjQ2J61lf53vVajUpDfLEBq21GBI9qEnj5a5hks04PeQ4Ds6ePSsx/c2bN7G6uirNJDZ1mIzyOrLZLFqtFtrttsg9U6aZVN9YLIabN2+K8pbF2zESRg7sNnRI1jLnOenBTfLWrTgr5tQMG0ALCwtwHAdra2s4f/687BhmFcOc8WQjh4QoGvPFixc90zpU+jJ3H5MbEwgEZAZ0cXFRPku5XMaVK1dQqVTE6/Ok5Xg8LkeVA/BILpu1eiIUCmFzcxNXr149xL/MvY+RMHJ6MtN7dzod8ZpmwsjEj2C4YMbuDDeCwSAuXryIra0tifEBCJHKnA4C4NE55xhbrVZDrVZDvV4XNVtWXBj7c6chHMdBOp1GKpXy6JTXajUUi0WUSiXx9JxVNdUFmLRSAJTlSPO801AohEKhgGvXrnkGvy3ejpExcnO4mGdeAl6hTm7n3W5XKiBmEmZOwBeLRTnSmwkgAAlnWHpstVqeOJ7CQNQa5Dk/ADzTN9xVTLYkFa0efPBBTE9Py9lBy8vLWFlZ8UjEsYHFw7qCwSBOnDiBqakpIVdVq1WUy2XPGaMczMjn81haWjr2k/j7wUgYOQCRTzAPbCVfg5wM/pFpzPV6HaFQCIlEwjOZznCDyR+wKwQUiUREpo2jYxybi8VimJiY8JCrqD+ulPIMJpuJsXm98/PzcrhWr9fD6uoqVlZWZFHy7FHW48PhsCTEp0+fls/FYxN5ejO7nel0GvV6HdevX/ec2mzxzhgZI2d7nV6OzMBKpSIGyW15YmICZ8+elQNly+Wy1NdNohbwdokJkpto2KxhRyIRUYklm3Bra8tz+gNhttTN8uXU1BQ+9KEPSQd3eXlZWuycSjJ56P3+4NhFMhu11igUClhdXRWxfnY+WXGp1+t44403LMPwDjASRm4SmRhasM1tDirzxIbp6WnE43Gsr6/LKWikAphxsqku5fP5hELLRdDv9xGPx2VImgZFhVk+n4sH8J5OwRCLterFxUVJLjc3N1GpVOSkB85ckvRFEhp1EH0+H7a2tnD9+nUpUZpncQaDQeRyOVy5csUa+B1iJIwc2I29CdJSaVxMOHu9Hm7cuIGrV69KfdqsqnDAgSVGJqU0enMihycZh8Nh1Go1rK6uCk+EiSCNEtjVL6fgPct+8/PzOH36NMLhsJy+TMOn92Zpkh1L5iFU6t3e3kaxWPQcVZ5KpeQ7WV5exs2bN+3M5nvAyBi5edQ3S4mmBDMn6sm4o/HTUCgFweoDa+a8j0brOI5UVyjA2Wq1UK/XJQE1T6vgIuJxJ6QAJxIJhMNhuc04OpfLyW7EM0G5YLnIGEvToHO5nFSS6OEXFhYQiUSQy+Vw48YNrK+vW07Ke8RIGDm9I0fHbiUJwWoLGz9m3L13aJnxNxeJ2X0EgJ2dHc/UPn9Pz0/vyyoM43ZgV5uRFAPSY/P5vByfYuoPsizI1+N7UZyIi4l88VQqhQcffBCu62J5eRlvvvmm5aO8T4yEkVPWgbfNeBrwSjLwMaa4Dh/H+jGwOz7Hx5mjbOa0zq3AhJT1ew4ucLdheS8ajaLX62FtbQ29Xg+xWEzmOc0dhYfQ9no9oeMyj6CHb7fbCAaDePjhhzE3N4cXX3wRr732mqfaZPHeMBJGzvjU5KaYTEL+z+TTNGwzZjYHhRmumKN09OambAXB35m/dxwHmUwGs7OzIrtG3XGe7UOlLSakpPyawvz1el3O8eSgB2c9KTUXiUTwMz/zM0ilUvjhD3+IS5cu2fj7gDASRg7sMvbYEDLnOgnG66xt8z4uEtIBOH0TCARQLpfFiFjlMBeDWUc3dwvHcZBMJjE9PQ2fzyejaZVKRUhehUJBJuTZnueC4tge8wBWkNipZMgTj8cxMTGBD3/4w+j3+/jmN7+J9fV3nRa0uEOMjJGbLDwae61WE0MBdg+3MlVbTcUtemdWTMjJNkWJWG/nrkFNFvLFE4mEhCY8kY0GS9mJfD4vi4RdS7MJxWSVNXCWJc3HkBdz6tQpLC4uYnNzE9/73vds/H0IGBkjp9gma+PmESAmaYthDA3JnPIxp/cZVjCONsfoTH0XlgI58tbtdpHL5VCr1aQ+br6HOYnPUiSHJzjQzA4lX3NnZwc7OztQSsF1XczMzMDn8+Hs2bNIJpN47rnn8MILL1gOyiFhJIyciefeQWNTv8RMFPk4VmMI8zEMO6hitXesLhQKIZlMSihRqVSE68L3ZOeTi4+lSE4yRaNRzM/PS7KbyWTkiBbW7PP5PLa3t6Uun0qlEA6H8fDDD8Pv9+N73/seLl++fHe+6GOKkTFycsA5zkVwe2eN2RQRMkuIPp9PhHbMigqNkrd5eGsqlZL2ey6XQ6PR8BzzzZiZ708yl9ZaRIomJyfl8fTopOuyAsNJpEQigYmJCUQiETz00EO4fv06nnjiCSvldhewX5m4GwAqGAh4drXWH1cHKPgJ7PLB2V7ncSL05qbClcljYVze6XQ8khamqhSTU5b82u22TNswNmcdnIMbwC7dgDnCwsIC5ufnEQwGUa1WUalU5D17vR7K5bIwJDlGR7lldlczmQxefPFFPPPMM7Y9f5dwJ578M1pr89iCAxP8ZPOGHHKOegG7k/omldZs9rCyQph8F8bt5LUwaS0UClKS5OtzBzBr8zRU8mUmJyfR7XZx9epVmbKn4A/r2eTXMPmdm5vD7OyscGe++93vYnl52XYv7yLeT7hyoIKfADyVD8bhZlPINFpTP8WUj2M5D4C04dmI4T96bYYW8mUMPT5ZgqFQCPPz80ilUqjX63j99dexubkpuQDflzV5HveSTqdlOv/06dNoNps4f/48nn/+eencWtw97NfINYD/VymlAfzvQx3DAxX8JEwV2+Freyi0rIqYZT3G5D6fT1iBrIfz4FoSt5jkmlQAcwjZbEAFAgFks1mZLjIpBxTg5E7CxRSLxfDQQw8hGo3ivvvuw6uvvopnn30Wq6urtrlzRNivkX9aa70+NOSnlFIX3+Wxdyz4GY/HNZND1rdpPGY4obWWuUcaKY2WZCweD0gNxE6nI6enmS19wHvqsTlhQ/2TXC6HnZ0dT7LL/x3HkQHnfn9wOkQ6ncb9998v4dOTTz6JZ599VrqjFkeDfRm51np9+H9WKfV9DMKPAxP8BHaPPGHVhKGKqRQFQJiIwG4H1PxnylgAEA45PT25MYzBWa40h5o5hEHhIho3E2PmBO12G9PT05idnUU4HMbi4iK01vjxj3+Mc+fOYXNz08beI4D9SDfHAPi01pXh7f8SwL/CQNjzQAQ/gd243OShMBllQsgqhzm8zFo2yU+M2yORiEg+kJxlctPZrOGCabfbKJVKEmebxk8jZ6gEQGZBZ2ZmcObMGcRiMWxubuLJJ5/ElStX7OzlCGE/nnwGwPeHyV8AwLe11k8opV7CAQl+hkIhTE1NSUmP52JyoNk8GpxGx8OhyO+uVCoi58byIY0fgPDJTcotuSfA7gkSZumRnpvjeBzaiMViiMfjePjhh/HhD38YW1tbeOKJJ/Dyyy97OrUWowE1CttpJpPRn/vc5zwSzbVaDZVKxSMiZLIPmSRSu5selmI8Zt3crLEzLOJRhnw9aqrQe5tDxlprGVNbWFjA2bNnMT8/j06ng6eeego/+tGP5KQ2i6NBoVB4RWv98Vv9biQ6nr1eD8ViUcqBnMOkFospFcfaOL0tT4tga56lPT7HPOfTTGbJUjRb+Iy1ufDj8Th6vR5c18WpU6dw4sQJfOADH0ClUsGTTz6Jp59+Gtvb20f87VncDiNh5ORgA7u6gD6fT8bEWLozCVpmu52cclNbkKU+arWYcb3ZCaVR8/0ov+a6LmKxGO677z488MADcF0X6+vr+MY3voGXXnpJxu0sRh8jYeQ0OKpcMWwxNb8ZsrDWTEM2mzmmlje7nqxnmx1Jvi5jei4wHp61uLiIkydPYmFhAY1GA2+++SaeeeYZXL161bbi70GMhJGzW5lKpaT5w8SRxCzOZNLQafTUJiEvhZTdveNt8XhcqiQ8vIrCnZlMBidOnMDc3BympqYQCoWwsrKCb33rWzh37hy2t7dtI+cexkgYea/XQz6f95z7w5CFZUPSYl3XRaVSEcYfJ+HJDTHF9jmJw+STtXeewhwKhfDAAw8glUqh1+thaWkJzz//PN566y1sb29bhaoxwUgYObBLnWVcTfIUAPG65GhTadbkm5Oqa8pQsMwYCoUwMTGBYDCIubk5TE5OotfrYWdnB9euXcOlS5dw8eJFERSyGC+MhJGT8cfjTBiK0JOarX3Gzxxro+SD3+/H3NwcXNdFMpkUg2fDp1wuI5vNYmlpCX/xF3+BCxcuyICDDUXGGyNj5IlEwkOdpVYhJ+YjkQimpqak00g6K8ldDFkok7y5uYmNjQ1sbW3h8uXLyGazqFQqkpBaHB+MhJFHIhF85jOfQSqVkrZ9MplEKpWSc3PYsQQgnPPV1VVsbm7i4sWL2NzclHN4GHbYSogFMCJGPjU1hV/7tV8DsKtQ1Wq1UK1WsbW1he3tbaysrODmzZtYWVnB1tYWdnZ2UCqVPAKa7F5aWJgYCSOv1+v4/ve/j7W1NWxubiKXy2F7e1tG1NjVfLcwwxq4xTthJLgrwWBQJ5NJmwBavGe8G3fl1mKAdxls11tYHAZGwsgtLA4T1sgtxh7WyC3GHtbILcYe1sgtxh7WyC3GHtbILcYe+zJypVRKKfXHSqmLSqkLSqlPKaUmlFJPKaWuDP9PG4//LaXUVaXUJaXU5w7v8i0sbo/9evJ/C+AJrfVZAD8F4AJ2BT/PAPjR8GfsEfz8PIDfV0r5D/rCLSz2i9sauVLKBfDXAfx7ANBat7XWRQyEPb8+fNjXAfyt4W0R/NRaXwdAwU8LiyPBfjz5aQDbAP4PpdRrSql/N1TS8gh+AjAFP1eM57+j4KdS6mWl1MujwJ+xGF/sx8gDAD4K4A+01o8CqGEYmrwD9i34qbX++FDQf18Xa2HxXrAfI18FsKq1/vHw5z/GwOi3hkKfOAjBTwuLw8JtjVxrvQlgRSn1geFdv4CBziEFP4G3C37+qlIqrJS6H/sU/LSwOCzsd2jifwDwLaVUCMASgL+PwQI5EMFPC4vDxEgMTQQCAe267lFfhsU9jJEfmrCwOExYI7cYe1gjtxh7WCO3GHtYI7cYe1gjtxh7WCO3GHtYI7cYe1gjtxh7WCO3GHtYI7cYe1gjtxh7WCO3GHtYI7cYe1gjtxh7WCO3GHtYI7cYe1gjtxh7WCO3GHtYI7cYe+xHJu4DSqnXjX9lpdRvWMFPi3sF+9FduaS1/ojW+iMAPgagDuD7sIKfFvcI7jRc+QUA17TWN2EFPy3uEdypkf8qgD8c3raCnxb3BPZt5EP1rF8G8H/f7qG3uM8KflocGe7Ek/8NAK9qrbeGP1vBT4t7Andi5H8bu6EKYAU/Le4R7EvwUykVBfBZAP+tcfdXYQU/Le4BWMFPi7GAFfy0ONawRm4x9rBGbjH2sEZuMfawRm4x9rBGbjH2sEZuMfawRm4x9rBGbjH2GImOWYkrDAAAAjRJREFUp1KqAuDSUV/HXcAkgNxRX8RdwFF8zvu01lO3+sV+D6s9bFx6p5bsOGHInbef8y7DhisWYw9r5BZjj1Ex8q8d9QXcJdjPeQQYicTTwuIwMSqe3MLi0HDkRq6U+vxQhOiqUuorR3097wdKqUWl1P+nlLqglHpLKfVPhvePnRCTUsqvlHpNKfWnw59H9zNqrY/sHwA/gGsATgMIAfgJgEeO8pre5+eZA/DR4e0EgMsAHgHwrwF8ZXj/VwD8L8Pbjww/cxjA/cPvwn/Un2Ofn/WfAvg2gD8d/jyyn/GoPfknAVzVWi9prdsAvoOBONE9Ca31htb61eHtCoALGGjOjJUQk1JqAcDfBPDvjLtH9jMetZHvS4joXoRS6hSARwH8GO9TiGkE8XsA/jmAvnHfyH7GozbyfQkR3WtQSsUBfBfAb2ity+/20FvcN9KfXyn1SwCyWutX9vuUW9x3Vz/jUbf1x06ISCkVxMDAv6W1/t7w7i2l1JzWemMMhJg+DeCXlVJfAOAAcJVS38Qof8YjTl4CAJYwSEiYeH7wqJOq9/F5FID/COD39tz/b+BNyv718PYH4U3KlnCPJJ7D638Mu4nnyH7GUfiivoBBFeIagH9x1NfzPj/Lz2GwFb8B4PXhvy8AyGAgb31l+P+E8Zx/MfzslwD8jaP+DHf4eU0jH9nPaDueFmOPo048LSwOHdbILcYe1sgtxh7WyC3GHtbILcYe1sgtxh7WyC3GHtbILcYe/z9+8oae2vO/xwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] Code to compare the ground truth and predicted mask\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "index        = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c = valid_dataset[index]\n",
    "\n",
    "print('Ground Truth ', b['boxes'].data.cpu().numpy())\n",
    "\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "ax   = plt.gca()\n",
    "\n",
    "if(len(all_target1[index]) > 0):\n",
    "    print(all_target1[index])\n",
    "    print(all_scores1[index])\n",
    "    \n",
    "    temp  = all_target1[index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='yellow', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 1')\n",
    "\n",
    "if(len(all_target[index]) > 0):\n",
    "    print(all_target[index])\n",
    "    print(all_scores[index])\n",
    "    \n",
    "    temp  = all_target[index]\n",
    "    index = 0\n",
    "    rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "    ax.add_patch(rect)\n",
    "else:\n",
    "    print('Not found 2')\n",
    "\n",
    "temp  = b['boxes'].data.cpu().numpy()#all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='red', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For doing the inference on the test images\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Some code to do visualization\n",
    "\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "fp    = \"/home/yu-hao/Downloads/coco_sample.png\"\n",
    "image = cv2.imread(fp)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Plot image 1\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#train_dataset1     = DBTDataset(train_set=1)\n",
    "#index = random.randint(0, len(train_dataset)-1)\n",
    "#image, b, c = train_dataset[index]\n",
    "\n",
    "case_index       = random.randint(0, len(train_dataset)-1)\n",
    "image, b, c = train_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "# transform = A.Compose(\n",
    "#     [A.HorizontalFlip(p=0.95)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "# )\n",
    "\n",
    "# temp_box = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "# temp_box[0][0] = temp_box[0][0]\n",
    "# temp_box[0][2] = temp_box[0][2]\n",
    "# temp_box[0][3] = temp_box[0][3]\n",
    "# temp_box[0][1] = temp_box[0][1]\n",
    "\n",
    "# random.seed(7)\n",
    "# transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "\n",
    "plt.imshow(image)\n",
    "ax   = plt.gca()\n",
    "\n",
    "temp  = b['boxes']#b[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Some code to test the augmentation\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#case_index  = random.randint(0, len(valid_dataset)-1)\n",
    "image, b, c = valid_dataset[case_index]\n",
    "image       = np.moveaxis(image, 0, -1)\n",
    "\n",
    "transform = A.Compose(\n",
    "    #[A.HorizontalFlip(p=0.99)],\n",
    "    [A.VerticalFlip(p=0.99)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    ")\n",
    "\n",
    "temp_box       = b['boxes'].data.cpu().numpy()\n",
    "\n",
    "temp_box[0][0] = temp_box[0][0]\n",
    "temp_box[0][2] = temp_box[0][2]\n",
    "temp_box[0][3] = temp_box[0][3]\n",
    "temp_box[0][1] = temp_box[0][1]\n",
    "temp           = temp_box\n",
    "\n",
    "if(0):\n",
    "    transformed = transform(image=image, bboxes=temp_box, labels=b['labels'])\n",
    "    image    = transformed['image']\n",
    "    temp     = transformed['bboxes']\n",
    "\n",
    "plt.imshow(image)\n",
    "ax    = plt.gca()\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images  = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes  = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the images\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = DBTDataset()\n",
    "#valid_dataset = DBTDataset()\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.5)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #50 loss: nan\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #100 loss: nan\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #150 loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-402f63afabfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "#train_dataset = td1\n",
    "#model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for k in range(len(train_dataset)-1):\n",
    "        images, targets, image_ids = train_dataset[k]\n",
    "        #for images, targets, image_ids in train_data_loader:\n",
    "        #print(images, targets)\n",
    "        \n",
    "        images  = [images]\n",
    "        targets = [targets]\n",
    "        #images  = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        #losses = loss_dict['loss_rpn_box_reg'] + loss_dict['loss_objectness']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(loss_dict)\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[  76., 1665.,  254., 1791.]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([156]), 'area': tensor([22428.], device='cuda:0'), 'iscrowd': tensor([0], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "images, targets, image_ids = train_dataset[k]\n",
    "images  = [images]\n",
    "targets = [targets]\n",
    "\n",
    "loss_dict = model(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-80a206e9139a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#cpu_device = torch.device(\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moriginal_image_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "images  = [images]\n",
    "#cpu_device = torch.device(\"cpu\")\n",
    "outputs = model(images)\n",
    "#outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_dict[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3000, 2000])\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4f50cab3d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
