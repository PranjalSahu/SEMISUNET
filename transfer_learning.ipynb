{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the Imports\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [OLD] Code for classification\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/media/yu-hao/WindowsData/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)\n",
    "\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     4,
     37,
     72,
     128,
     192,
     198,
     203
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] All the functions for reading the data for Wheat Dataset\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def expand_bbox(x):\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n",
    "    r1 = [float(x) for x in r]\n",
    "    r = r1\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "train_df = pd.read_csv('/media/yu-hao/WindowsData/WheatDataset/train.csv')\n",
    "train_df.shape\n",
    "\n",
    "train_df['x'] = -1\n",
    "train_df['y'] = -1\n",
    "train_df['w'] = -1\n",
    "train_df['h'] = -1\n",
    "\n",
    "temp = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "#train_df[['x', 'y', 'w', 'h']] = \n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df['x'] = temp[:, 0]#train_df['x'].astype(np.float)\n",
    "train_df['y'] = temp[:, 1]#train_df['y'].astype(np.float)\n",
    "train_df['w'] = temp[:, 2]#train_df['w'].astype(np.float)\n",
    "train_df['h'] = temp[:, 3]#train_df['h'].astype(np.float)\n",
    "\n",
    "# df['bbox'] = df['bbox'].apply(lambda x: np.array(x))\n",
    "# x = np.array(list(df['bbox']))\n",
    "# print(x)\n",
    "# for i, dim in enumerate(['x', 'y', 'w', 'h']):\n",
    "#     df[dim] = x[:, i]\n",
    "\n",
    "# # df.drop('bbox', axis=1, inplace=True)\n",
    "# #df.head()\n",
    "\n",
    "class WheatDatasetOld(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.image_ids  = self.df['image_id'].unique()\n",
    "        self.image_dir  = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        records  = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        im_name = image_id + '.jpg'\n",
    "        img = Image.open(self.image_dir/im_name).convert(\"RGB\")\n",
    "        img = T.ToTensor()(img)\n",
    "        \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']  = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        \n",
    "        return img, target, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "class DBTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_set = 1, transforms = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if train_set == 1:\n",
    "            self.train_start  = 0\n",
    "            self.train_end    = 150\n",
    "        else:\n",
    "            self.train_start  = 150\n",
    "            self.train_end    = 200\n",
    "        \n",
    "        self.trainx = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainx.npy')[self.train_start:self.train_end]#.astype('float16')/60000.0\n",
    "        self.trainy = np.load('/media/yu-hao/WindowsData/DBT_numpy/trainy.npy')[self.train_start:self.train_end]\n",
    "        self.coordx = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordx.npy')[self.train_start:self.train_end]\n",
    "        self.coordy = np.load('/media/yu-hao/WindowsData/DBT_numpy/coordy.npy')[self.train_start:self.train_end]\n",
    "        self.width_arr  = np.load('/media/yu-hao/WindowsData/DBT_numpy/width_arr.npy')[self.train_start:self.train_end]\n",
    "        self.height_arr = np.load('/media/yu-hao/WindowsData/DBT_numpy/height_arr.npy')[self.train_start:self.train_end]\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.trainx[idx, 0].astype('float32')/60000.0\n",
    "        img[img > 1] = 1\n",
    "        img = ndimage.interpolation.zoom(img, 0.25)\n",
    "        \n",
    "        img = np.expand_dims(img, 0)\n",
    "        img = np.concatenate([img, img, img], axis=0)\n",
    "        \n",
    "        #print(img.shape, 'img shape is')\n",
    "        #img = torch.Tensor(img).to(device)\n",
    "        #img = T.ToTensor()(img).to(device)\n",
    "        \n",
    "        boxes = np.array([self.coordx[idx]/4, self.coordy[idx]/4, self.width_arr[idx]/4, self.height_arr[idx]/4])#records[['x', 'y', 'w', 'h']].values\n",
    "        #print(boxes)\n",
    "        boxes = np.expand_dims(boxes, axis=0)\n",
    "        boxes[:, 2] = boxes[:, 0]+boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1]+boxes[:, 3]\n",
    "        #boxes = boxes\n",
    "        #print('boxes shape is ',boxes.shape)\n",
    "        #boxes = torch.Tensor(boxes).to(device)#, device='cuda:0')#dtype=torch.int64)\n",
    "        \n",
    "        #labels = torch.ones((1,), dtype=torch.int64)#torch.Tensor(self.trainy[idx]).to(device)\n",
    "        \n",
    "        area = self.width_arr[idx] * self.height_arr[idx]\n",
    "        area = torch.Tensor(area)#.to(device)\n",
    "\n",
    "        # there is only one class\n",
    "        labels =  torch.ones((1,)).type(torch.int64)#.to(device)#torch.Tensor(np.array([1])).type(torch.int64).to(device)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.Tensor(np.array([0])).type(torch.int64)#.to(device)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes']     = torch.Tensor(boxes)\n",
    "        target['labels']    = labels\n",
    "        target['image_id']  = torch.tensor([idx])\n",
    "        target['area']      = area\n",
    "        target['iscrowd']   = iscrowd\n",
    "        \n",
    "        return img, target, idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.trainx.shape[0]\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] Wheat Dataset and Model Creation\n",
    "\n",
    "image_ids = train_df['image_id'].unique()\n",
    "valid_ids = image_ids[-665:]\n",
    "train_ids = image_ids[:-665]\n",
    "\n",
    "valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "train_df = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(valid_df.shape, train_df.shape)\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DIR_INPUT = '/media/yu-hao/WindowsData/WheatDataset'\n",
    "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
    "DIR_TEST  = f'{DIR_INPUT}/test'\n",
    "\n",
    "train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] DBT Dataset and Model Creation\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset     = DBTDataset(train_set=1)\n",
    "valid_dataset     = DBTDataset(train_set=0)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "model       = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params       = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer    = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.0001)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.4085538983345032\n",
      "Epoch #0 Train loss: 0.636813370020766\n",
      "Epoch #0 Val   loss: 0.44265748216555667\n",
      "Validation Iteration #50 loss: 0.2827727794647217\n",
      "Epoch #1 Train loss: 0.3782736674735421\n",
      "Epoch #1 Val   loss: 0.3844927709836226\n",
      "Validation Iteration #50 loss: 0.24969077110290527\n",
      "Epoch #2 Train loss: 0.3050282656362182\n",
      "Epoch #2 Val   loss: 0.35385303084666914\n",
      "Validation Iteration #50 loss: 0.23800887167453766\n",
      "Epoch #3 Train loss: 0.27857378870248795\n",
      "Epoch #3 Val   loss: 0.33550997279011285\n",
      "Validation Iteration #50 loss: 0.23004253208637238\n",
      "Epoch #4 Train loss: 0.2650080503601777\n",
      "Epoch #4 Val   loss: 0.32332662573227516\n",
      "Validation Iteration #50 loss: 0.21421855688095093\n",
      "Epoch #5 Train loss: 0.25437537659155696\n",
      "Epoch #5 Val   loss: 0.31280608819081235\n",
      "Validation Iteration #50 loss: 0.19781391322612762\n",
      "Epoch #6 Train loss: 0.24924509815479579\n",
      "Epoch #6 Val   loss: 0.30464147178681344\n",
      "Validation Iteration #50 loss: 0.20302604138851166\n",
      "Epoch #7 Train loss: 0.24244388036037745\n",
      "Epoch #7 Val   loss: 0.29844766654647314\n",
      "Validation Iteration #50 loss: 0.20642581582069397\n",
      "Epoch #8 Train loss: 0.239055582959401\n",
      "Epoch #8 Val   loss: 0.2936458681893145\n",
      "Validation Iteration #50 loss: 0.2020455151796341\n",
      "Epoch #9 Train loss: 0.2355140235863234\n",
      "Epoch #9 Val   loss: 0.28900505911845425\n",
      "Validation Iteration #50 loss: 0.20636144280433655\n",
      "Epoch #10 Train loss: 0.23137797925033068\n",
      "Epoch #10 Val   loss: 0.2852724360210912\n",
      "Validation Iteration #50 loss: 0.21573598682880402\n",
      "Epoch #11 Train loss: 0.2292720283332624\n",
      "Epoch #11 Val   loss: 0.28176024594368076\n",
      "Validation Iteration #50 loss: 0.21579612791538239\n",
      "Epoch #12 Train loss: 0.22755137517264015\n",
      "Epoch #12 Val   loss: 0.27875235218268174\n",
      "Validation Iteration #50 loss: 0.20577125251293182\n",
      "Epoch #13 Train loss: 0.22349970081919118\n",
      "Epoch #13 Val   loss: 0.27594386417787153\n",
      "Validation Iteration #50 loss: 0.23171238601207733\n",
      "Epoch #14 Train loss: 0.22007048090821818\n",
      "Epoch #14 Val   loss: 0.27348634875737704\n",
      "Validation Iteration #50 loss: 0.22755320370197296\n",
      "Epoch #15 Train loss: 0.21837817563822395\n",
      "Epoch #15 Val   loss: 0.2713191421845785\n",
      "Validation Iteration #50 loss: 0.2277972251176834\n",
      "Epoch #16 Train loss: 0.2176605141476581\n",
      "Epoch #16 Val   loss: 0.2693342751237602\n",
      "Validation Iteration #50 loss: 0.2572876214981079\n",
      "Epoch #17 Train loss: 0.21577083319425583\n",
      "Epoch #17 Val   loss: 0.26760361935847843\n",
      "Validation Iteration #50 loss: 0.2564212679862976\n",
      "Epoch #18 Train loss: 0.21569340558428512\n",
      "Epoch #18 Val   loss: 0.26596878034624494\n",
      "Validation Iteration #50 loss: 0.2461395412683487\n",
      "Epoch #19 Train loss: 0.2148082770015064\n",
      "Epoch #19 Val   loss: 0.26427234629025825\n",
      "Validation Iteration #50 loss: 0.25111114978790283\n",
      "Epoch #20 Train loss: 0.21304729816160703\n",
      "Epoch #20 Val   loss: 0.26257604387871947\n",
      "Validation Iteration #50 loss: 0.26015931367874146\n",
      "Epoch #21 Train loss: 0.21172676313864558\n",
      "Epoch #21 Val   loss: 0.26096565245123177\n",
      "Validation Iteration #50 loss: 0.25610652565956116\n",
      "Epoch #22 Train loss: 0.21136532919971565\n",
      "Epoch #22 Val   loss: 0.25957113594115777\n",
      "Validation Iteration #50 loss: 0.2710365653038025\n",
      "Epoch #23 Train loss: 0.212077586274398\n",
      "Epoch #23 Val   loss: 0.25846718294689286\n",
      "Validation Iteration #50 loss: 0.25633642077445984\n",
      "Epoch #24 Train loss: 0.21138259572418114\n",
      "Epoch #24 Val   loss: 0.25727420994868644\n",
      "Validation Iteration #50 loss: 0.25342896580696106\n",
      "Epoch #25 Train loss: 0.21122806362415614\n",
      "Epoch #25 Val   loss: 0.2561832493138031\n",
      "Validation Iteration #50 loss: 0.24489079415798187\n",
      "Epoch #26 Train loss: 0.2118032190360521\n",
      "Epoch #26 Val   loss: 0.25512357170765215\n",
      "Validation Iteration #50 loss: 0.25096288323402405\n",
      "Epoch #27 Train loss: 0.20979187559140355\n",
      "Epoch #27 Val   loss: 0.25409787700890185\n",
      "Validation Iteration #50 loss: 0.2408740222454071\n",
      "Epoch #28 Train loss: 0.21033842351875806\n",
      "Epoch #28 Val   loss: 0.25322116721688276\n",
      "Validation Iteration #50 loss: 0.24611660838127136\n",
      "Epoch #29 Train loss: 0.21482808495822706\n",
      "Epoch #29 Val   loss: 0.2523461799591016\n",
      "Validation Iteration #50 loss: 0.24848371744155884\n",
      "Epoch #30 Train loss: 0.21234266636402985\n",
      "Epoch #30 Val   loss: 0.2516698301873787\n",
      "Validation Iteration #50 loss: 0.2400166094303131\n",
      "Epoch #31 Train loss: 0.21497791378121628\n",
      "Epoch #31 Val   loss: 0.25095735801956975\n",
      "Validation Iteration #50 loss: 0.24732717871665955\n",
      "Epoch #32 Train loss: 0.21421770516194796\n",
      "Epoch #32 Val   loss: 0.2502683212498685\n",
      "Validation Iteration #50 loss: 0.24427121877670288\n",
      "Epoch #33 Train loss: 0.21527468138619474\n",
      "Epoch #33 Val   loss: 0.24964869757313535\n",
      "Validation Iteration #50 loss: 0.2454068511724472\n",
      "Epoch #34 Train loss: 0.2153840472823695\n",
      "Epoch #34 Val   loss: 0.24901469873858023\n",
      "Validation Iteration #50 loss: 0.24307598173618317\n",
      "Epoch #35 Train loss: 0.2163604162633419\n",
      "Epoch #35 Val   loss: 0.24847510765887734\n",
      "Validation Iteration #50 loss: 0.23927631974220276\n",
      "Epoch #36 Train loss: 0.21701778000906893\n",
      "Epoch #36 Val   loss: 0.24793514293345492\n",
      "Validation Iteration #50 loss: 0.24122969806194305\n",
      "Epoch #37 Train loss: 0.21704406879450144\n",
      "Epoch #37 Val   loss: 0.24743561078662332\n",
      "Validation Iteration #50 loss: 0.24364587664604187\n",
      "Epoch #38 Train loss: 0.21656279305094167\n",
      "Epoch #38 Val   loss: 0.24700473251897673\n",
      "Validation Iteration #50 loss: 0.24365684390068054\n",
      "Epoch #39 Train loss: 0.21540161379073797\n",
      "Epoch #39 Val   loss: 0.24665625728666782\n",
      "Validation Iteration #50 loss: 0.24181699752807617\n",
      "Epoch #40 Train loss: 0.21678140524186587\n",
      "Epoch #40 Val   loss: 0.24630153394438759\n",
      "Validation Iteration #50 loss: 0.2410474717617035\n",
      "Epoch #41 Train loss: 0.21760014130880959\n",
      "Epoch #41 Val   loss: 0.24596252147749667\n",
      "Validation Iteration #50 loss: 0.23308105766773224\n",
      "Epoch #42 Train loss: 0.21921222539324509\n",
      "Epoch #42 Val   loss: 0.24562913499279398\n",
      "Validation Iteration #50 loss: 0.23626945912837982\n",
      "Epoch #43 Train loss: 0.21873424241417333\n",
      "Epoch #43 Val   loss: 0.24528957439677698\n",
      "Validation Iteration #50 loss: 0.2339990884065628\n",
      "Epoch #44 Train loss: 0.2213318983190938\n",
      "Epoch #44 Val   loss: 0.24495225255815392\n",
      "Validation Iteration #50 loss: 0.23083357512950897\n",
      "Epoch #45 Train loss: 0.22116050163381978\n",
      "Epoch #45 Val   loss: 0.2445756464100203\n",
      "Validation Iteration #50 loss: 0.23757489025592804\n",
      "Epoch #46 Train loss: 0.22062017180417715\n",
      "Epoch #46 Val   loss: 0.244275201922938\n",
      "Validation Iteration #50 loss: 0.23478035628795624\n",
      "Epoch #47 Train loss: 0.22131984641677455\n",
      "Epoch #47 Val   loss: 0.24398407457062068\n",
      "Validation Iteration #50 loss: 0.23636630177497864\n",
      "Epoch #48 Train loss: 0.21949288680365212\n",
      "Epoch #48 Val   loss: 0.24370780255019758\n",
      "Validation Iteration #50 loss: 0.2313513457775116\n",
      "Epoch #49 Train loss: 0.22196111945729508\n",
      "Epoch #49 Val   loss: 0.24347197835262005\n",
      "Validation Iteration #50 loss: 0.22857512533664703\n",
      "Epoch #50 Train loss: 0.2190119145732177\n",
      "Epoch #50 Val   loss: 0.2432477857642224\n",
      "Validation Iteration #50 loss: 0.22795598208904266\n",
      "Epoch #51 Train loss: 0.21890234986418172\n",
      "Epoch #51 Val   loss: 0.24303028091082912\n",
      "Validation Iteration #50 loss: 0.22377276420593262\n",
      "Epoch #52 Train loss: 0.21939100678029813\n",
      "Epoch #52 Val   loss: 0.24284011711662498\n",
      "Validation Iteration #50 loss: 0.2326786071062088\n",
      "Epoch #53 Train loss: 0.2195091816155534\n",
      "Epoch #53 Val   loss: 0.2426408760228388\n",
      "Validation Iteration #50 loss: 0.22554880380630493\n",
      "Epoch #54 Train loss: 0.21965155005455017\n",
      "Epoch #54 Val   loss: 0.242437979954106\n",
      "Validation Iteration #50 loss: 0.2276037633419037\n",
      "Epoch #55 Train loss: 0.22077348279325584\n",
      "Epoch #55 Val   loss: 0.24228401027701713\n",
      "Validation Iteration #50 loss: 0.2242293357849121\n",
      "Epoch #56 Train loss: 0.21843916805166946\n",
      "Epoch #56 Val   loss: 0.2421292349716269\n",
      "Validation Iteration #50 loss: 0.22482843697071075\n",
      "Epoch #57 Train loss: 0.22017904802372582\n",
      "Epoch #57 Val   loss: 0.24202676293309233\n",
      "Validation Iteration #50 loss: 0.22811849415302277\n",
      "Epoch #58 Train loss: 0.2207133040616387\n",
      "Epoch #58 Val   loss: 0.2418722534039807\n",
      "Validation Iteration #50 loss: 0.2264695018529892\n",
      "Epoch #59 Train loss: 0.21972373324005226\n",
      "Epoch #59 Val   loss: 0.24171647710295824\n",
      "Validation Iteration #50 loss: 0.22853471338748932\n",
      "Epoch #60 Train loss: 0.21943345156155133\n",
      "Epoch #60 Val   loss: 0.24157954271398907\n",
      "Validation Iteration #50 loss: 0.2246096283197403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61 Train loss: 0.2196163386106491\n",
      "Epoch #61 Val   loss: 0.24146022756697227\n",
      "Validation Iteration #50 loss: 0.2291772961616516\n",
      "Epoch #62 Train loss: 0.22310244448875127\n",
      "Epoch #62 Val   loss: 0.24132444157341315\n",
      "Validation Iteration #50 loss: 0.22860245406627655\n",
      "Epoch #63 Train loss: 0.22328561779699826\n",
      "Epoch #63 Val   loss: 0.2411917495684555\n",
      "Validation Iteration #50 loss: 0.22194786369800568\n",
      "Epoch #64 Train loss: 0.22309334929052152\n",
      "Epoch #64 Val   loss: 0.2410555232382385\n",
      "Validation Iteration #50 loss: 0.21912020444869995\n",
      "Epoch #65 Train loss: 0.22530981231676905\n",
      "Epoch #65 Val   loss: 0.2409363484709135\n",
      "Validation Iteration #50 loss: 0.2235022634267807\n",
      "Epoch #66 Train loss: 0.22318214687861895\n",
      "Epoch #66 Val   loss: 0.2407721482414055\n",
      "Validation Iteration #50 loss: 0.22217078506946564\n",
      "Epoch #67 Train loss: 0.21994693694930328\n",
      "Epoch #67 Val   loss: 0.24066066575063838\n",
      "Validation Iteration #50 loss: 0.22713570296764374\n",
      "Epoch #68 Train loss: 0.22115268871972435\n",
      "Epoch #68 Val   loss: 0.24055170559364816\n",
      "Validation Iteration #50 loss: 0.22198505699634552\n",
      "Epoch #69 Train loss: 0.22248377572549016\n",
      "Epoch #69 Val   loss: 0.24043426664320977\n",
      "Validation Iteration #50 loss: 0.2299438714981079\n",
      "Epoch #70 Train loss: 0.22255377275379082\n",
      "Epoch #70 Val   loss: 0.2403347382275539\n",
      "Validation Iteration #50 loss: 0.23481211066246033\n",
      "Epoch #71 Train loss: 0.22215998800177322\n",
      "Epoch #71 Val   loss: 0.24023518681080422\n",
      "Validation Iteration #50 loss: 0.23947738111019135\n",
      "Epoch #72 Train loss: 0.2240104243943566\n",
      "Epoch #72 Val   loss: 0.24011613196203657\n",
      "Validation Iteration #50 loss: 0.2396564483642578\n",
      "Epoch #73 Train loss: 0.2239435966077604\n",
      "Epoch #73 Val   loss: 0.240022980730028\n",
      "Validation Iteration #50 loss: 0.23901835083961487\n",
      "Epoch #74 Train loss: 0.22469771496559443\n",
      "Epoch #74 Val   loss: 0.23995052221493843\n",
      "Validation Iteration #50 loss: 0.2382107973098755\n",
      "Epoch #75 Train loss: 0.22426008511530726\n",
      "Epoch #75 Val   loss: 0.23986716778111844\n",
      "Validation Iteration #50 loss: 0.23866510391235352\n",
      "Epoch #76 Train loss: 0.22459876694177328\n",
      "Epoch #76 Val   loss: 0.23982356022764276\n",
      "Validation Iteration #50 loss: 0.24246415495872498\n",
      "Epoch #77 Train loss: 0.22513275868014285\n",
      "Epoch #77 Val   loss: 0.2397699382530867\n",
      "Validation Iteration #50 loss: 0.24072401225566864\n",
      "Epoch #78 Train loss: 0.22343024140910098\n",
      "Epoch #78 Val   loss: 0.23972131461340357\n",
      "Validation Iteration #50 loss: 0.2402941882610321\n",
      "Epoch #79 Train loss: 0.2234105083503221\n",
      "Epoch #79 Val   loss: 0.2396686747813454\n",
      "Validation Iteration #50 loss: 0.24177083373069763\n",
      "Epoch #80 Train loss: 0.22331845328996056\n",
      "Epoch #80 Val   loss: 0.2395967964970941\n",
      "Validation Iteration #50 loss: 0.2420429140329361\n",
      "Epoch #81 Train loss: 0.22395828758415423\n",
      "Epoch #81 Val   loss: 0.23954346387404216\n",
      "Validation Iteration #50 loss: 0.24118703603744507\n",
      "Epoch #82 Train loss: 0.22317768947074287\n",
      "Epoch #82 Val   loss: 0.2394980635628643\n",
      "Validation Iteration #50 loss: 0.24526402354240417\n",
      "Epoch #83 Train loss: 0.2228380563227754\n",
      "Epoch #83 Val   loss: 0.23947141117556192\n",
      "Validation Iteration #50 loss: 0.23977543413639069\n",
      "Epoch #84 Train loss: 0.22415908818182192\n",
      "Epoch #84 Val   loss: 0.23944113010464751\n",
      "Validation Iteration #50 loss: 0.2445085048675537\n",
      "Epoch #85 Train loss: 0.22369303946432315\n",
      "Epoch #85 Val   loss: 0.2394325017049402\n",
      "Validation Iteration #50 loss: 0.24666057527065277\n",
      "Epoch #86 Train loss: 0.22406723350286484\n",
      "Epoch #86 Val   loss: 0.23942130972704562\n",
      "Validation Iteration #50 loss: 0.23863829672336578\n",
      "Epoch #87 Train loss: 0.22388238342184769\n",
      "Epoch #87 Val   loss: 0.23943147078885899\n",
      "Validation Iteration #50 loss: 0.23938670754432678\n",
      "Epoch #88 Train loss: 0.22515612487730227\n",
      "Epoch #88 Val   loss: 0.23941576846632961\n",
      "Validation Iteration #50 loss: 0.23770970106124878\n",
      "Epoch #89 Train loss: 0.225199588819554\n",
      "Epoch #89 Val   loss: 0.23941302212894472\n",
      "Validation Iteration #50 loss: 0.23550546169281006\n",
      "Epoch #90 Train loss: 0.22532227674597188\n",
      "Epoch #90 Val   loss: 0.23939723085016895\n",
      "Validation Iteration #50 loss: 0.23821921646595\n",
      "Epoch #91 Train loss: 0.22691060916373604\n",
      "Epoch #91 Val   loss: 0.23941708886902469\n",
      "Validation Iteration #50 loss: 0.2401888072490692\n",
      "Epoch #92 Train loss: 0.22612813389615008\n",
      "Epoch #92 Val   loss: 0.23945270974198288\n",
      "Validation Iteration #50 loss: 0.23278720676898956\n",
      "Epoch #93 Train loss: 0.22834575921297073\n",
      "Epoch #93 Val   loss: 0.23946772092639326\n",
      "Validation Iteration #50 loss: 0.23242871463298798\n",
      "Epoch #94 Train loss: 0.2269800395557755\n",
      "Epoch #94 Val   loss: 0.23945742617493215\n",
      "Validation Iteration #50 loss: 0.22927330434322357\n",
      "Epoch #95 Train loss: 0.22931743609277824\n",
      "Epoch #95 Val   loss: 0.2394726517944573\n",
      "Validation Iteration #50 loss: 0.2331041395664215\n",
      "Epoch #96 Train loss: 0.22674832767561862\n",
      "Epoch #96 Val   loss: 0.2394783853373766\n",
      "Validation Iteration #50 loss: 0.23498199880123138\n",
      "Epoch #97 Train loss: 0.22744358997595937\n",
      "Epoch #97 Val   loss: 0.2394930141593448\n",
      "Validation Iteration #50 loss: 0.22715477645397186\n",
      "Epoch #98 Train loss: 0.22778660764819697\n",
      "Epoch #98 Val   loss: 0.23950844700936672\n",
      "Validation Iteration #50 loss: 0.22910383343696594\n",
      "Epoch #99 Train loss: 0.23077384264845596\n",
      "Epoch #99 Val   loss: 0.23952977240085602\n",
      "Validation Iteration #50 loss: 0.22562026977539062\n",
      "Epoch #100 Train loss: 0.22809526285058573\n",
      "Epoch #100 Val   loss: 0.23956825783347138\n",
      "Validation Iteration #50 loss: 0.2234768271446228\n",
      "Epoch #101 Train loss: 0.22923133483058528\n",
      "Epoch #101 Val   loss: 0.2395848615227062\n",
      "Validation Iteration #50 loss: 0.22865179181098938\n",
      "Epoch #102 Train loss: 0.22831835087976957\n",
      "Epoch #102 Val   loss: 0.23959991254630242\n",
      "Validation Iteration #50 loss: 0.22072266042232513\n",
      "Epoch #103 Train loss: 0.22842000718963773\n",
      "Epoch #103 Val   loss: 0.23963736752654322\n",
      "Validation Iteration #50 loss: 0.22856950759887695\n",
      "Epoch #104 Train loss: 0.22922270196048836\n",
      "Epoch #104 Val   loss: 0.2396886821740713\n",
      "Validation Iteration #50 loss: 0.2278224527835846\n",
      "Epoch #105 Train loss: 0.22914114594459534\n",
      "Epoch #105 Val   loss: 0.23972927993068846\n",
      "Validation Iteration #50 loss: 0.2315860241651535\n",
      "Epoch #106 Train loss: 0.22852529624575063\n",
      "Epoch #106 Val   loss: 0.2397689801231723\n",
      "Validation Iteration #50 loss: 0.23044827580451965\n",
      "Epoch #107 Train loss: 0.22805403133756236\n",
      "Epoch #107 Val   loss: 0.23981023047045086\n",
      "Validation Iteration #50 loss: 0.2364090234041214\n",
      "Epoch #108 Train loss: 0.22659079201127352\n",
      "Epoch #108 Val   loss: 0.23984780710981862\n",
      "Validation Iteration #50 loss: 0.232761412858963\n",
      "Epoch #109 Train loss: 0.22782161635787865\n",
      "Epoch #109 Val   loss: 0.23988304167360694\n",
      "Validation Iteration #50 loss: 0.23455990850925446\n",
      "Epoch #110 Train loss: 0.2278134466001862\n",
      "Epoch #110 Val   loss: 0.23989209209517812\n",
      "Validation Iteration #50 loss: 0.2320575714111328\n",
      "Epoch #111 Train loss: 0.22876931530864617\n",
      "Epoch #111 Val   loss: 0.23990516970445822\n",
      "Validation Iteration #50 loss: 0.2370876520872116\n",
      "Epoch #112 Train loss: 0.22754086593264028\n",
      "Epoch #112 Val   loss: 0.23991294410092398\n",
      "Validation Iteration #50 loss: 0.22869038581848145\n",
      "Epoch #113 Train loss: 0.22816119695964612\n",
      "Epoch #113 Val   loss: 0.2399090639191118\n",
      "Validation Iteration #50 loss: 0.23045282065868378\n",
      "Epoch #114 Train loss: 0.22886557367287183\n",
      "Epoch #114 Val   loss: 0.23990876967691657\n",
      "Validation Iteration #50 loss: 0.22583547234535217\n",
      "Epoch #115 Train loss: 0.2277666753844211\n",
      "Epoch #115 Val   loss: 0.23990845918378717\n",
      "Validation Iteration #50 loss: 0.2247796207666397\n",
      "Epoch #116 Train loss: 0.22711249872257835\n",
      "Epoch #116 Val   loss: 0.2399062105184476\n",
      "Validation Iteration #50 loss: 0.2210054099559784\n",
      "Epoch #117 Train loss: 0.22623944792308306\n",
      "Epoch #117 Val   loss: 0.23990154707369624\n",
      "Validation Iteration #50 loss: 0.22337153553962708\n",
      "Epoch #118 Train loss: 0.22601303850349627\n",
      "Epoch #118 Val   loss: 0.23990723048592663\n",
      "Validation Iteration #50 loss: 0.2206750363111496\n",
      "Epoch #119 Train loss: 0.22710517244903664\n",
      "Epoch #119 Val   loss: 0.23989160778239751\n",
      "Validation Iteration #50 loss: 0.22198006510734558\n",
      "Epoch #120 Train loss: 0.22723515253317983\n",
      "Epoch #120 Val   loss: 0.23989583072545645\n",
      "Validation Iteration #50 loss: 0.22382497787475586\n",
      "Epoch #121 Train loss: 0.22591985369983472\n",
      "Epoch #121 Val   loss: 0.23988877026385347\n",
      "Validation Iteration #50 loss: 0.21942399442195892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #122 Train loss: 0.22778350193249552\n",
      "Epoch #122 Val   loss: 0.2398919446290173\n",
      "Validation Iteration #50 loss: 0.2212560921907425\n",
      "Epoch #123 Train loss: 0.22808528299394407\n",
      "Epoch #123 Val   loss: 0.23989919952992766\n",
      "Validation Iteration #50 loss: 0.219659224152565\n",
      "Epoch #124 Train loss: 0.22791755081791626\n",
      "Epoch #124 Val   loss: 0.23990420662439788\n",
      "Validation Iteration #50 loss: 0.2207489013671875\n",
      "Epoch #125 Train loss: 0.22848699045808693\n",
      "Epoch #125 Val   loss: 0.2399053082452298\n",
      "Validation Iteration #50 loss: 0.21881623566150665\n",
      "Epoch #126 Train loss: 0.22659223173793994\n",
      "Epoch #126 Val   loss: 0.23991879426407714\n",
      "Validation Iteration #50 loss: 0.22093413770198822\n",
      "Epoch #127 Train loss: 0.228463183108129\n",
      "Epoch #127 Val   loss: 0.2399243435356766\n",
      "Validation Iteration #50 loss: 0.2199174463748932\n",
      "Epoch #128 Train loss: 0.22893632634689934\n",
      "Epoch #128 Val   loss: 0.23994013748854204\n",
      "Validation Iteration #50 loss: 0.22183161973953247\n",
      "Epoch #129 Train loss: 0.2274404987692833\n",
      "Epoch #129 Val   loss: 0.239935501131433\n",
      "Validation Iteration #50 loss: 0.22032082080841064\n",
      "Epoch #130 Train loss: 0.22761399259692744\n",
      "Epoch #130 Val   loss: 0.23994667359960828\n",
      "Validation Iteration #50 loss: 0.21931853890419006\n",
      "Epoch #131 Train loss: 0.22561480222563995\n",
      "Epoch #131 Val   loss: 0.23994737369440375\n",
      "Validation Iteration #50 loss: 0.21877768635749817\n",
      "Epoch #132 Train loss: 0.22533223974077324\n",
      "Epoch #132 Val   loss: 0.23994943060890106\n",
      "Validation Iteration #50 loss: 0.21781504154205322\n",
      "Epoch #133 Train loss: 0.22567811765168844\n",
      "Epoch #133 Val   loss: 0.23995260952466116\n",
      "Validation Iteration #50 loss: 0.22107164561748505\n",
      "Epoch #134 Train loss: 0.2256887135537047\n",
      "Epoch #134 Val   loss: 0.23995849226954316\n",
      "Validation Iteration #50 loss: 0.21571704745292664\n",
      "Epoch #135 Train loss: 0.22733498835249952\n",
      "Epoch #135 Val   loss: 0.23996044874056432\n",
      "Validation Iteration #50 loss: 0.21921010315418243\n",
      "Epoch #136 Train loss: 0.22627260457528264\n",
      "Epoch #136 Val   loss: 0.2399762952715945\n",
      "Validation Iteration #50 loss: 0.2143186628818512\n",
      "Epoch #137 Train loss: 0.22547673865368492\n",
      "Epoch #137 Val   loss: 0.23998973732077025\n",
      "Validation Iteration #50 loss: 0.21420009434223175\n",
      "Epoch #138 Train loss: 0.22687796071956032\n",
      "Epoch #138 Val   loss: 0.24001595421627733\n",
      "Validation Iteration #50 loss: 0.2192465215921402\n",
      "Epoch #139 Train loss: 0.22554113206110502\n",
      "Epoch #139 Val   loss: 0.24004027850025303\n",
      "Validation Iteration #50 loss: 0.2219245582818985\n",
      "Epoch #140 Train loss: 0.22503038456565455\n",
      "Epoch #140 Val   loss: 0.24007923606797377\n",
      "Validation Iteration #50 loss: 0.22143401205539703\n",
      "Epoch #141 Train loss: 0.22641636235149284\n",
      "Epoch #141 Val   loss: 0.24009797220014414\n",
      "Validation Iteration #50 loss: 0.22869542241096497\n",
      "Epoch #142 Train loss: 0.22549520040813245\n",
      "Epoch #142 Val   loss: 0.24011754874999708\n",
      "Validation Iteration #50 loss: 0.2220565527677536\n",
      "Epoch #143 Train loss: 0.2255838705520881\n",
      "Epoch #143 Val   loss: 0.2401447603558628\n",
      "Validation Iteration #50 loss: 0.22136381268501282\n",
      "Epoch #144 Train loss: 0.22546200964011645\n",
      "Epoch #144 Val   loss: 0.2401710903217685\n",
      "Validation Iteration #50 loss: 0.2122819423675537\n",
      "Epoch #145 Train loss: 0.2256332137867024\n",
      "Epoch #145 Val   loss: 0.24019372862841232\n",
      "Validation Iteration #50 loss: 0.21781794726848602\n",
      "Epoch #146 Train loss: 0.22487023590426697\n",
      "Epoch #146 Val   loss: 0.24022518892926986\n",
      "Validation Iteration #50 loss: 0.21789595484733582\n",
      "Epoch #147 Train loss: 0.2234996263133852\n",
      "Epoch #147 Val   loss: 0.24025325168876788\n",
      "Validation Iteration #50 loss: 0.21944504976272583\n",
      "Epoch #148 Train loss: 0.22349510145814797\n",
      "Epoch #148 Val   loss: 0.24027229610890166\n",
      "Validation Iteration #50 loss: 0.23071029782295227\n",
      "Epoch #149 Train loss: 0.22235968199215436\n",
      "Epoch #149 Val   loss: 0.24029943335514803\n",
      "Validation Iteration #50 loss: 0.22385108470916748\n",
      "Epoch #150 Train loss: 0.22409381482161975\n",
      "Epoch #150 Val   loss: 0.24032375139814824\n",
      "Validation Iteration #50 loss: 0.22859269380569458\n",
      "Epoch #151 Train loss: 0.22227380365917557\n",
      "Epoch #151 Val   loss: 0.24034599634075937\n",
      "Validation Iteration #50 loss: 0.237265944480896\n",
      "Epoch #152 Train loss: 0.22383341781402888\n",
      "Epoch #152 Val   loss: 0.24036755573515797\n",
      "Validation Iteration #50 loss: 0.23520545661449432\n",
      "Epoch #153 Train loss: 0.22505320843897367\n",
      "Epoch #153 Val   loss: 0.240384744776117\n",
      "Validation Iteration #50 loss: 0.23420780897140503\n",
      "Epoch #154 Train loss: 0.22384073153922432\n",
      "Epoch #154 Val   loss: 0.24040794058266113\n",
      "Validation Iteration #50 loss: 0.23586532473564148\n",
      "Epoch #155 Train loss: 0.22183819742579208\n",
      "Epoch #155 Val   loss: 0.2404295045742857\n",
      "Validation Iteration #50 loss: 0.23403583467006683\n",
      "Epoch #156 Train loss: 0.22206620989661469\n",
      "Epoch #156 Val   loss: 0.24044750999551612\n",
      "Validation Iteration #50 loss: 0.23760324716567993\n",
      "Epoch #157 Train loss: 0.22154174705869273\n",
      "Epoch #157 Val   loss: 0.24045806029174902\n",
      "Validation Iteration #50 loss: 0.22663787007331848\n",
      "Epoch #158 Train loss: 0.21955479191322075\n",
      "Epoch #158 Val   loss: 0.2404635765728999\n",
      "Validation Iteration #50 loss: 0.23006345331668854\n",
      "Epoch #159 Train loss: 0.22222643345594406\n",
      "Epoch #159 Val   loss: 0.24045213570531745\n",
      "Validation Iteration #50 loss: 0.22759070992469788\n",
      "Epoch #160 Train loss: 0.22172995695942327\n",
      "Epoch #160 Val   loss: 0.24045454935342914\n",
      "Validation Iteration #50 loss: 0.2274274080991745\n",
      "Epoch #161 Train loss: 0.22224609004823784\n",
      "Epoch #161 Val   loss: 0.24044673228309138\n",
      "Validation Iteration #50 loss: 0.23020866513252258\n",
      "Epoch #162 Train loss: 0.22032547153924642\n",
      "Epoch #162 Val   loss: 0.24044117790705305\n",
      "Validation Iteration #50 loss: 0.23225265741348267\n",
      "Epoch #163 Train loss: 0.22052809733309245\n",
      "Epoch #163 Val   loss: 0.2404376278521308\n",
      "Validation Iteration #50 loss: 0.23237380385398865\n",
      "Epoch #164 Train loss: 0.21927873868691294\n",
      "Epoch #164 Val   loss: 0.2404228216354108\n",
      "Validation Iteration #50 loss: 0.23117364943027496\n",
      "Epoch #165 Train loss: 0.21922897429842697\n",
      "Epoch #165 Val   loss: 0.2404202500960692\n",
      "Validation Iteration #50 loss: 0.22894194722175598\n",
      "Epoch #166 Train loss: 0.21812301285957036\n",
      "Epoch #166 Val   loss: 0.24041204553960602\n",
      "Validation Iteration #50 loss: 0.22897234559059143\n",
      "Epoch #167 Train loss: 0.2172658545406241\n",
      "Epoch #167 Val   loss: 0.2404032227320549\n",
      "Validation Iteration #50 loss: 0.2328842133283615\n",
      "Epoch #168 Train loss: 0.21700074209978706\n",
      "Epoch #168 Val   loss: 0.2403996436770634\n",
      "Validation Iteration #50 loss: 0.2330034077167511\n",
      "Epoch #169 Train loss: 0.21713017829154668\n",
      "Epoch #169 Val   loss: 0.24040115413622618\n",
      "Validation Iteration #50 loss: 0.23252592980861664\n",
      "Epoch #170 Train loss: 0.21688263000626312\n",
      "Epoch #170 Val   loss: 0.24041057021109521\n",
      "Validation Iteration #50 loss: 0.24283893406391144\n",
      "Epoch #171 Train loss: 0.21661430048315147\n",
      "Epoch #171 Val   loss: 0.24041962191755742\n",
      "Validation Iteration #50 loss: 0.24485553801059723\n",
      "Epoch #172 Train loss: 0.21628982573747635\n",
      "Epoch #172 Val   loss: 0.24044123806784873\n",
      "Validation Iteration #50 loss: 0.23830251395702362\n",
      "Epoch #173 Train loss: 0.21640073390383469\n",
      "Epoch #173 Val   loss: 0.24044979006631725\n",
      "Validation Iteration #50 loss: 0.23534870147705078\n",
      "Epoch #174 Train loss: 0.21696308255195618\n",
      "Epoch #174 Val   loss: 0.24046258363094958\n",
      "Validation Iteration #50 loss: 0.24138326942920685\n",
      "Epoch #175 Train loss: 0.2159687127721937\n",
      "Epoch #175 Val   loss: 0.24048017024655233\n",
      "Validation Iteration #50 loss: 0.23539648950099945\n",
      "Epoch #176 Train loss: 0.21619517395370885\n",
      "Epoch #176 Val   loss: 0.2405021326508744\n",
      "Validation Iteration #50 loss: 0.2396729439496994\n",
      "Epoch #177 Train loss: 0.2150385344499036\n",
      "Epoch #177 Val   loss: 0.24052294104158004\n",
      "Validation Iteration #50 loss: 0.23712670803070068\n",
      "Epoch #178 Train loss: 0.21617134817336736\n",
      "Epoch #178 Val   loss: 0.24053890098944516\n",
      "Validation Iteration #50 loss: 0.23827771842479706\n",
      "Epoch #179 Train loss: 0.2163856202050259\n",
      "Epoch #179 Val   loss: 0.2405462690334544\n",
      "Validation Iteration #50 loss: 0.236517995595932\n",
      "Epoch #180 Train loss: 0.21567779465725548\n",
      "Epoch #180 Val   loss: 0.2405535504380237\n",
      "Validation Iteration #50 loss: 0.23533985018730164\n",
      "Epoch #181 Train loss: 0.2153276726603508\n",
      "Epoch #181 Val   loss: 0.2405642168824691\n",
      "Validation Iteration #50 loss: 0.23698224127292633\n",
      "Epoch #182 Train loss: 0.21665411952294802\n",
      "Epoch #182 Val   loss: 0.24056770460552507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.2386174201965332\n",
      "Epoch #183 Train loss: 0.21431085643799683\n",
      "Epoch #183 Val   loss: 0.24057053038881573\n",
      "Validation Iteration #50 loss: 0.23742817342281342\n",
      "Epoch #184 Train loss: 0.21453114206853666\n",
      "Epoch #184 Val   loss: 0.2405720954315578\n",
      "Validation Iteration #50 loss: 0.23707926273345947\n",
      "Epoch #185 Train loss: 0.21502011622253217\n",
      "Epoch #185 Val   loss: 0.24057247290638875\n",
      "Validation Iteration #50 loss: 0.24295388162136078\n",
      "Epoch #186 Train loss: 0.21444671640270635\n",
      "Epoch #186 Val   loss: 0.24056553899481775\n",
      "Validation Iteration #50 loss: 0.23907342553138733\n",
      "Epoch #187 Train loss: 0.2133789360523224\n",
      "Epoch #187 Val   loss: 0.2405639162040186\n",
      "Validation Iteration #50 loss: 0.2381870597600937\n",
      "Epoch #188 Train loss: 0.213935130128735\n",
      "Epoch #188 Val   loss: 0.24055931903649308\n",
      "Validation Iteration #50 loss: 0.23840107023715973\n",
      "Epoch #189 Train loss: 0.2125599733308742\n",
      "Epoch #189 Val   loss: 0.24055536901540603\n",
      "Validation Iteration #50 loss: 0.23804478347301483\n",
      "Epoch #190 Train loss: 0.21246403651802162\n",
      "Epoch #190 Val   loss: 0.24054831057558893\n",
      "Validation Iteration #50 loss: 0.22938476502895355\n",
      "Epoch #191 Train loss: 0.2132480889558792\n",
      "Epoch #191 Val   loss: 0.2405366002020832\n",
      "Validation Iteration #50 loss: 0.23404556512832642\n",
      "Epoch #192 Train loss: 0.2130943387746811\n",
      "Epoch #192 Val   loss: 0.24053271469389503\n",
      "Validation Iteration #50 loss: 0.23338361084461212\n",
      "Epoch #193 Train loss: 0.21236080323394976\n",
      "Epoch #193 Val   loss: 0.2405311808427301\n",
      "Validation Iteration #50 loss: 0.23250561952590942\n",
      "Epoch #194 Train loss: 0.21276435491285825\n",
      "Epoch #194 Val   loss: 0.24052438206926605\n",
      "Validation Iteration #50 loss: 0.2347278594970703\n",
      "Epoch #195 Train loss: 0.21255025463668922\n",
      "Epoch #195 Val   loss: 0.2405080334626992\n",
      "Validation Iteration #50 loss: 0.22804349660873413\n",
      "Epoch #196 Train loss: 0.21455668109027962\n",
      "Epoch #196 Val   loss: 0.2404993916223497\n",
      "Validation Iteration #50 loss: 0.2350298911333084\n",
      "Epoch #197 Train loss: 0.2121200439961333\n",
      "Epoch #197 Val   loss: 0.24048718990122708\n",
      "Validation Iteration #50 loss: 0.23220506310462952\n",
      "Epoch #198 Train loss: 0.21214305727105393\n",
      "Epoch #198 Val   loss: 0.24046900064893612\n",
      "Validation Iteration #50 loss: 0.2270021140575409\n",
      "Epoch #199 Train loss: 0.21268159268718018\n",
      "Epoch #199 Val   loss: 0.24045016405674127\n",
      "Validation Iteration #50 loss: 0.22972701489925385\n",
      "Epoch #200 Train loss: 0.21081396231525823\n",
      "Epoch #200 Val   loss: 0.2404367497687681\n",
      "Validation Iteration #50 loss: 0.22635464370250702\n",
      "Epoch #201 Train loss: 0.21211664613924527\n",
      "Epoch #201 Val   loss: 0.24042123267306503\n",
      "Validation Iteration #50 loss: 0.2282930314540863\n",
      "Epoch #202 Train loss: 0.21220107847138456\n",
      "Epoch #202 Val   loss: 0.24040461681972358\n",
      "Validation Iteration #50 loss: 0.23648479580879211\n",
      "Epoch #203 Train loss: 0.213214924853099\n",
      "Epoch #203 Val   loss: 0.2403922560673943\n",
      "Validation Iteration #50 loss: 0.23466844856739044\n",
      "Epoch #204 Train loss: 0.21468034621916318\n",
      "Epoch #204 Val   loss: 0.24038147511446453\n",
      "Validation Iteration #50 loss: 0.23441295325756073\n",
      "Epoch #205 Train loss: 0.21228959411382675\n",
      "Epoch #205 Val   loss: 0.2403711956481243\n",
      "Validation Iteration #50 loss: 0.23010419309139252\n",
      "Epoch #206 Train loss: 0.21236423089316017\n",
      "Epoch #206 Val   loss: 0.24035307918946922\n",
      "Validation Iteration #50 loss: 0.2345200628042221\n",
      "Epoch #207 Train loss: 0.21229386015942223\n",
      "Epoch #207 Val   loss: 0.2403422137005971\n",
      "Validation Iteration #50 loss: 0.23847104609012604\n",
      "Epoch #208 Train loss: 0.2121540888359672\n",
      "Epoch #208 Val   loss: 0.240331007779895\n",
      "Validation Iteration #50 loss: 0.23386995494365692\n",
      "Epoch #209 Train loss: 0.2117250969535426\n",
      "Epoch #209 Val   loss: 0.2403150553013379\n",
      "Validation Iteration #50 loss: 0.23342035710811615\n",
      "Epoch #210 Train loss: 0.2125536007316489\n",
      "Epoch #210 Val   loss: 0.24030066392107077\n",
      "Validation Iteration #50 loss: 0.23432037234306335\n",
      "Epoch #211 Train loss: 0.21265709674672076\n",
      "Epoch #211 Val   loss: 0.24029981590407334\n",
      "Validation Iteration #50 loss: 0.23623472452163696\n",
      "Epoch #212 Train loss: 0.21311896215928228\n",
      "Epoch #212 Val   loss: 0.24028798758596148\n",
      "Validation Iteration #50 loss: 0.23375016450881958\n",
      "Epoch #213 Train loss: 0.21194220922495188\n",
      "Epoch #213 Val   loss: 0.24028488544788745\n",
      "Validation Iteration #50 loss: 0.24112750589847565\n",
      "Epoch #214 Train loss: 0.21282364937819934\n",
      "Epoch #214 Val   loss: 0.24028648888169118\n",
      "Validation Iteration #50 loss: 0.23200155794620514\n",
      "Epoch #215 Train loss: 0.21087700834399775\n",
      "Epoch #215 Val   loss: 0.24027535263608155\n",
      "Validation Iteration #50 loss: 0.23500002920627594\n",
      "Epoch #216 Train loss: 0.2105678143469911\n",
      "Epoch #216 Val   loss: 0.24026837124654649\n",
      "Validation Iteration #50 loss: 0.23697654902935028\n",
      "Epoch #217 Train loss: 0.21076435164401405\n",
      "Epoch #217 Val   loss: 0.24025952518932736\n",
      "Validation Iteration #50 loss: 0.23291251063346863\n",
      "Epoch #218 Train loss: 0.21118588431885368\n",
      "Epoch #218 Val   loss: 0.24025990719497098\n",
      "Validation Iteration #50 loss: 0.23659376800060272\n",
      "Epoch #219 Train loss: 0.20997768640518188\n",
      "Epoch #219 Val   loss: 0.24024988735889222\n",
      "Validation Iteration #50 loss: 0.2348196804523468\n",
      "Epoch #220 Train loss: 0.2088587189975538\n",
      "Epoch #220 Val   loss: 0.24024234235992092\n",
      "Validation Iteration #50 loss: 0.2363865226507187\n",
      "Epoch #221 Train loss: 0.20749455376675255\n",
      "Epoch #221 Val   loss: 0.2402379076130177\n",
      "Validation Iteration #50 loss: 0.2360401749610901\n",
      "Epoch #222 Train loss: 0.20911651261542974\n",
      "Epoch #222 Val   loss: 0.240227466448499\n",
      "Validation Iteration #50 loss: 0.2348426878452301\n",
      "Epoch #223 Train loss: 0.20976603737002925\n",
      "Epoch #223 Val   loss: 0.24021607135412293\n",
      "Validation Iteration #50 loss: 0.23774774372577667\n",
      "Epoch #224 Train loss: 0.20785154284615265\n",
      "Epoch #224 Val   loss: 0.24021208191529297\n",
      "Validation Iteration #50 loss: 0.23217278718948364\n",
      "Epoch #225 Train loss: 0.20929821580648422\n",
      "Epoch #225 Val   loss: 0.24020907074882514\n",
      "Validation Iteration #50 loss: 0.23505866527557373\n",
      "Epoch #226 Train loss: 0.2082520864511791\n",
      "Epoch #226 Val   loss: 0.2402041101912166\n",
      "Validation Iteration #50 loss: 0.23835979402065277\n",
      "Epoch #227 Train loss: 0.20656399428844452\n",
      "Epoch #227 Val   loss: 0.24017857160926992\n",
      "Validation Iteration #50 loss: 0.24904027581214905\n",
      "Epoch #228 Train loss: 0.20706264713877126\n",
      "Epoch #228 Val   loss: 0.24016214963230142\n",
      "Validation Iteration #50 loss: 0.2473149299621582\n",
      "Epoch #229 Train loss: 0.20640321350411364\n",
      "Epoch #229 Val   loss: 0.24014645342444096\n",
      "Validation Iteration #50 loss: 0.24712885916233063\n",
      "Epoch #230 Train loss: 0.20651973352620476\n",
      "Epoch #230 Val   loss: 0.24012902629125368\n",
      "Validation Iteration #50 loss: 0.23864173889160156\n",
      "Epoch #231 Train loss: 0.20687165072089747\n",
      "Epoch #231 Val   loss: 0.24010597752896323\n",
      "Validation Iteration #50 loss: 0.24565201997756958\n",
      "Epoch #232 Train loss: 0.20490747337278567\n",
      "Epoch #232 Val   loss: 0.24009393622435213\n",
      "Validation Iteration #50 loss: 0.2542899549007416\n",
      "Epoch #233 Train loss: 0.20649125858357079\n",
      "Epoch #233 Val   loss: 0.2400817778120693\n",
      "Validation Iteration #50 loss: 0.24818013608455658\n",
      "Epoch #234 Train loss: 0.207749038150436\n",
      "Epoch #234 Val   loss: 0.24007377462086624\n",
      "Validation Iteration #50 loss: 0.2514643967151642\n",
      "Epoch #235 Train loss: 0.20635814258926793\n",
      "Epoch #235 Val   loss: 0.24005993407078796\n",
      "Validation Iteration #50 loss: 0.2451414316892624\n",
      "Epoch #236 Train loss: 0.20618737528198644\n",
      "Epoch #236 Val   loss: 0.24005113032316086\n",
      "Validation Iteration #50 loss: 0.2398732453584671\n",
      "Epoch #237 Train loss: 0.20572154263132497\n",
      "Epoch #237 Val   loss: 0.2400328070406923\n",
      "Validation Iteration #50 loss: 0.24181413650512695\n",
      "Epoch #238 Train loss: 0.20557388780932678\n",
      "Epoch #238 Val   loss: 0.24001931113505387\n",
      "Validation Iteration #50 loss: 0.2456599622964859\n",
      "Epoch #239 Train loss: 0.20460225916222521\n",
      "Epoch #239 Val   loss: 0.23999619163477268\n",
      "Validation Iteration #50 loss: 0.23356719315052032\n",
      "Epoch #240 Train loss: 0.20478194873583944\n",
      "Epoch #240 Val   loss: 0.23996876247234325\n",
      "Validation Iteration #50 loss: 0.24178318679332733\n",
      "Epoch #241 Train loss: 0.20451839385848297\n",
      "Epoch #241 Val   loss: 0.23994361290451227\n",
      "Validation Iteration #50 loss: 0.23414738476276398\n",
      "Epoch #242 Train loss: 0.20428833130158877\n",
      "Epoch #242 Val   loss: 0.23991011637851156\n",
      "Validation Iteration #50 loss: 0.24029819667339325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #243 Train loss: 0.20299391722992846\n",
      "Epoch #243 Val   loss: 0.23988062584911288\n",
      "Validation Iteration #50 loss: 0.2409779280424118\n",
      "Epoch #244 Train loss: 0.20327961189966454\n",
      "Epoch #244 Val   loss: 0.23985448878264315\n",
      "Validation Iteration #50 loss: 0.24394544959068298\n",
      "Epoch #245 Train loss: 0.2021455407926911\n",
      "Epoch #245 Val   loss: 0.23982374570923645\n",
      "Validation Iteration #50 loss: 0.23652292788028717\n",
      "Epoch #246 Train loss: 0.20114816842894806\n",
      "Epoch #246 Val   loss: 0.2397881960922667\n",
      "Validation Iteration #50 loss: 0.2404906153678894\n",
      "Epoch #247 Train loss: 0.2017893685322059\n",
      "Epoch #247 Val   loss: 0.23974820127521201\n",
      "Validation Iteration #50 loss: 0.24176645278930664\n",
      "Epoch #248 Train loss: 0.1995934036217238\n",
      "Epoch #248 Val   loss: 0.2397141982448812\n",
      "Validation Iteration #50 loss: 0.24132059514522552\n",
      "Epoch #249 Train loss: 0.20007660043867012\n",
      "Epoch #249 Val   loss: 0.2396765936429684\n",
      "Validation Iteration #50 loss: 0.2402980625629425\n",
      "Epoch #250 Train loss: 0.19951288951070686\n",
      "Epoch #250 Val   loss: 0.23964062038097797\n",
      "Validation Iteration #50 loss: 0.245260551571846\n",
      "Epoch #251 Train loss: 0.20004288499292575\n",
      "Epoch #251 Val   loss: 0.23959792815649175\n",
      "Validation Iteration #50 loss: 0.23943910002708435\n",
      "Epoch #252 Train loss: 0.19886519799107\n",
      "Epoch #252 Val   loss: 0.23954973005349603\n",
      "Validation Iteration #50 loss: 0.2394300401210785\n",
      "Epoch #253 Train loss: 0.20039519040208115\n",
      "Epoch #253 Val   loss: 0.23951170718052545\n",
      "Validation Iteration #50 loss: 0.23329806327819824\n",
      "Epoch #254 Train loss: 0.19865559904198898\n",
      "Epoch #254 Val   loss: 0.23947179341298244\n",
      "Validation Iteration #50 loss: 0.2356729507446289\n",
      "Epoch #255 Train loss: 0.19856299695215726\n",
      "Epoch #255 Val   loss: 0.23943088693060696\n",
      "Validation Iteration #50 loss: 0.23136180639266968\n",
      "Epoch #256 Train loss: 0.19988766234172017\n",
      "Epoch #256 Val   loss: 0.23938191077089066\n",
      "Validation Iteration #50 loss: 0.24144220352172852\n",
      "Epoch #257 Train loss: 0.2005675109593492\n",
      "Epoch #257 Val   loss: 0.23933722978912086\n",
      "Validation Iteration #50 loss: 0.23278187215328217\n",
      "Epoch #258 Train loss: 0.1997226241387819\n",
      "Epoch #258 Val   loss: 0.2392882127143312\n",
      "Validation Iteration #50 loss: 0.23291490972042084\n",
      "Epoch #259 Train loss: 0.1991494180340516\n",
      "Epoch #259 Val   loss: 0.23923744169565347\n",
      "Validation Iteration #50 loss: 0.23659448325634003\n",
      "Epoch #260 Train loss: 0.19928937797483645\n",
      "Epoch #260 Val   loss: 0.2391845302668919\n",
      "Validation Iteration #50 loss: 0.23629871010780334\n",
      "Epoch #261 Train loss: 0.19765925093701012\n",
      "Epoch #261 Val   loss: 0.23913797300420506\n",
      "Validation Iteration #50 loss: 0.24003605544567108\n",
      "Epoch #262 Train loss: 0.19779500796606667\n",
      "Epoch #262 Val   loss: 0.23909113891956646\n",
      "Validation Iteration #50 loss: 0.2317848950624466\n",
      "Epoch #263 Train loss: 0.19791521249633087\n",
      "Epoch #263 Val   loss: 0.23904054285849863\n",
      "Validation Iteration #50 loss: 0.23287701606750488\n",
      "Epoch #264 Train loss: 0.19756569517286202\n",
      "Epoch #264 Val   loss: 0.23898634346112804\n",
      "Validation Iteration #50 loss: 0.23232153058052063\n",
      "Epoch #265 Train loss: 0.1968068417749907\n",
      "Epoch #265 Val   loss: 0.23893388459419088\n",
      "Validation Iteration #50 loss: 0.23297494649887085\n",
      "Epoch #266 Train loss: 0.19718633043138603\n",
      "Epoch #266 Val   loss: 0.23887645328072366\n",
      "Validation Iteration #50 loss: 0.23417392373085022\n",
      "Epoch #267 Train loss: 0.19655168919186844\n",
      "Epoch #267 Val   loss: 0.23881745399601423\n",
      "Validation Iteration #50 loss: 0.23089076578617096\n",
      "Epoch #268 Train loss: 0.1950525243423487\n",
      "Epoch #268 Val   loss: 0.23875166227775266\n",
      "Validation Iteration #50 loss: 0.23054584860801697\n",
      "Epoch #269 Train loss: 0.1955400940619017\n",
      "Epoch #269 Val   loss: 0.23869269519534886\n",
      "Validation Iteration #50 loss: 0.23400841653347015\n",
      "Epoch #270 Train loss: 0.19561552138705002\n",
      "Epoch #270 Val   loss: 0.23863730757472665\n",
      "Validation Iteration #50 loss: 0.23369193077087402\n",
      "Epoch #271 Train loss: 0.1942777123890425\n",
      "Epoch #271 Val   loss: 0.2385767318426349\n",
      "Validation Iteration #50 loss: 0.2349139302968979\n",
      "Epoch #272 Train loss: 0.1967188221843619\n",
      "Epoch #272 Val   loss: 0.23851152672990003\n",
      "Validation Iteration #50 loss: 0.24084794521331787\n",
      "Epoch #273 Train loss: 0.1938514834956119\n",
      "Epoch #273 Val   loss: 0.23845307995836632\n",
      "Validation Iteration #50 loss: 0.24101968109607697\n",
      "Epoch #274 Train loss: 0.1950007810404426\n",
      "Epoch #274 Val   loss: 0.23839669897006108\n",
      "Validation Iteration #50 loss: 0.24124661087989807\n",
      "Epoch #275 Train loss: 0.19327433642588163\n",
      "Epoch #275 Val   loss: 0.23834026737788583\n",
      "Validation Iteration #50 loss: 0.23199006915092468\n",
      "Epoch #276 Train loss: 0.19342242357762238\n",
      "Epoch #276 Val   loss: 0.23828613949679428\n",
      "Validation Iteration #50 loss: 0.24216438829898834\n",
      "Epoch #277 Train loss: 0.1929769966947405\n",
      "Epoch #277 Val   loss: 0.23822925096727168\n",
      "Validation Iteration #50 loss: 0.23989476263523102\n",
      "Epoch #278 Train loss: 0.19339950931699654\n",
      "Epoch #278 Val   loss: 0.23816398199445327\n",
      "Validation Iteration #50 loss: 0.24552756547927856\n",
      "Epoch #279 Train loss: 0.19141843914985657\n",
      "Epoch #279 Val   loss: 0.2381040532141924\n",
      "Validation Iteration #50 loss: 0.23975281417369843\n",
      "Epoch #280 Train loss: 0.19193731013097262\n",
      "Epoch #280 Val   loss: 0.23804939238218423\n",
      "Validation Iteration #50 loss: 0.2446228563785553\n",
      "Epoch #281 Train loss: 0.1909393672096102\n",
      "Epoch #281 Val   loss: 0.2379926280552939\n",
      "Validation Iteration #50 loss: 0.24964205920696259\n",
      "Epoch #282 Train loss: 0.19070032827163996\n",
      "Epoch #282 Val   loss: 0.23793903228121305\n",
      "Validation Iteration #50 loss: 0.24135570228099823\n",
      "Epoch #283 Train loss: 0.19032471038793264\n",
      "Epoch #283 Val   loss: 0.23787907270042635\n",
      "Validation Iteration #50 loss: 0.22892777621746063\n",
      "Epoch #284 Train loss: 0.19186687469482422\n",
      "Epoch #284 Val   loss: 0.23781567949112933\n",
      "Validation Iteration #50 loss: 0.2280244678258896\n",
      "Epoch #285 Train loss: 0.18972197292666687\n",
      "Epoch #285 Val   loss: 0.23775803650047525\n",
      "Validation Iteration #50 loss: 0.23509809374809265\n",
      "Epoch #286 Train loss: 0.1892458222022182\n",
      "Epoch #286 Val   loss: 0.23769710953211343\n",
      "Validation Iteration #50 loss: 0.22876589000225067\n",
      "Epoch #287 Train loss: 0.19014714266124524\n",
      "Epoch #287 Val   loss: 0.23763633161608097\n",
      "Validation Iteration #50 loss: 0.22702409327030182\n",
      "Epoch #288 Train loss: 0.18807191166438555\n",
      "Epoch #288 Val   loss: 0.2375741665481347\n",
      "Validation Iteration #50 loss: 0.22598615288734436\n",
      "Epoch #289 Train loss: 0.18746800406983025\n",
      "Epoch #289 Val   loss: 0.23751223643436356\n",
      "Validation Iteration #50 loss: 0.22365623712539673\n",
      "Epoch #290 Train loss: 0.1871967331359261\n",
      "Epoch #290 Val   loss: 0.23744631738471128\n",
      "Validation Iteration #50 loss: 0.22390246391296387\n",
      "Epoch #291 Train loss: 0.18615182646011053\n",
      "Epoch #291 Val   loss: 0.23738565758749983\n",
      "Validation Iteration #50 loss: 0.22782893478870392\n",
      "Epoch #292 Train loss: 0.1857965247411477\n",
      "Epoch #292 Val   loss: 0.2373277045166314\n",
      "Validation Iteration #50 loss: 0.22368153929710388\n",
      "Epoch #293 Train loss: 0.18559081656368157\n",
      "Epoch #293 Val   loss: 0.23725721021711607\n",
      "Validation Iteration #50 loss: 0.22814269363880157\n",
      "Epoch #294 Train loss: 0.18633281400329188\n",
      "Epoch #294 Val   loss: 0.23719078584475808\n",
      "Validation Iteration #50 loss: 0.228590726852417\n",
      "Epoch #295 Train loss: 0.18606554756039068\n",
      "Epoch #295 Val   loss: 0.237122004468327\n",
      "Validation Iteration #50 loss: 0.22429010272026062\n",
      "Epoch #296 Train loss: 0.18604611370124316\n",
      "Epoch #296 Val   loss: 0.23705294567341942\n",
      "Validation Iteration #50 loss: 0.22383855283260345\n",
      "Epoch #297 Train loss: 0.18587528482863777\n",
      "Epoch #297 Val   loss: 0.2369778619508923\n",
      "Validation Iteration #50 loss: 0.22588419914245605\n",
      "Epoch #298 Train loss: 0.18530555696863876\n",
      "Epoch #298 Val   loss: 0.2369037435351959\n",
      "Validation Iteration #50 loss: 0.22850240767002106\n",
      "Epoch #299 Train loss: 0.1846716541208719\n",
      "Epoch #299 Val   loss: 0.23682709341247876\n",
      "Validation Iteration #50 loss: 0.22899338603019714\n",
      "Epoch #300 Train loss: 0.18476754740664833\n",
      "Epoch #300 Val   loss: 0.2367518721617246\n",
      "Validation Iteration #50 loss: 0.22701396048069\n",
      "Epoch #301 Train loss: 0.18379069472614087\n",
      "Epoch #301 Val   loss: 0.23667800598040079\n",
      "Validation Iteration #50 loss: 0.2258019894361496\n",
      "Epoch #302 Train loss: 0.18261183366963737\n",
      "Epoch #302 Val   loss: 0.23660332096518827\n",
      "Validation Iteration #50 loss: 0.22311151027679443\n",
      "Epoch #303 Train loss: 0.18217325347818827\n",
      "Epoch #303 Val   loss: 0.2365329346133147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.22076554596424103\n",
      "Epoch #304 Train loss: 0.18208581405250648\n",
      "Epoch #304 Val   loss: 0.23645762349061977\n",
      "Validation Iteration #50 loss: 0.22077609598636627\n",
      "Epoch #305 Train loss: 0.18263771463381617\n",
      "Epoch #305 Val   loss: 0.23639046843924555\n",
      "Validation Iteration #50 loss: 0.22234946489334106\n",
      "Epoch #306 Train loss: 0.18097372705999173\n",
      "Epoch #306 Val   loss: 0.2363232484708541\n",
      "Validation Iteration #50 loss: 0.22088199853897095\n",
      "Epoch #307 Train loss: 0.180458034339704\n",
      "Epoch #307 Val   loss: 0.23625340367054248\n",
      "Validation Iteration #50 loss: 0.2210901379585266\n",
      "Epoch #308 Train loss: 0.1802751672895331\n",
      "Epoch #308 Val   loss: 0.23618606086806454\n",
      "Validation Iteration #50 loss: 0.22044488787651062\n",
      "Epoch #309 Train loss: 0.1808671798360975\n",
      "Epoch #309 Val   loss: 0.23611743048979686\n",
      "Validation Iteration #50 loss: 0.22390691936016083\n",
      "Epoch #310 Train loss: 0.1802146426941219\n",
      "Epoch #310 Val   loss: 0.23605483895314675\n",
      "Validation Iteration #50 loss: 0.22452785074710846\n",
      "Epoch #311 Train loss: 0.1791753235616182\n",
      "Epoch #311 Val   loss: 0.2359924491575719\n",
      "Validation Iteration #50 loss: 0.22647635638713837\n",
      "Epoch #312 Train loss: 0.17968396489557467\n",
      "Epoch #312 Val   loss: 0.23592288115739998\n",
      "Validation Iteration #50 loss: 0.22563546895980835\n",
      "Epoch #313 Train loss: 0.17854448427495204\n",
      "Epoch #313 Val   loss: 0.23584939677054714\n",
      "Validation Iteration #50 loss: 0.2261800318956375\n",
      "Epoch #314 Train loss: 0.17923555444729955\n",
      "Epoch #314 Val   loss: 0.2357806951171458\n",
      "Validation Iteration #50 loss: 0.22686313092708588\n",
      "Epoch #315 Train loss: 0.17852116885938143\n",
      "Epoch #315 Val   loss: 0.2357057234908961\n",
      "Validation Iteration #50 loss: 0.2220965474843979\n",
      "Epoch #316 Train loss: 0.17932959449918648\n",
      "Epoch #316 Val   loss: 0.2356307158349238\n",
      "Validation Iteration #50 loss: 0.22535006701946259\n",
      "Epoch #317 Train loss: 0.1793484107444161\n",
      "Epoch #317 Val   loss: 0.23555497492086386\n",
      "Validation Iteration #50 loss: 0.22647599875926971\n",
      "Epoch #318 Train loss: 0.17738212409772372\n",
      "Epoch #318 Val   loss: 0.23548064870822277\n",
      "Validation Iteration #50 loss: 0.2255801111459732\n",
      "Epoch #319 Train loss: 0.17676271223708204\n",
      "Epoch #319 Val   loss: 0.2354100128528304\n",
      "Validation Iteration #50 loss: 0.22754058241844177\n",
      "Epoch #320 Train loss: 0.17693991468925224\n",
      "Epoch #320 Val   loss: 0.2353411129830468\n",
      "Validation Iteration #50 loss: 0.22301389276981354\n",
      "Epoch #321 Train loss: 0.17709405328098096\n",
      "Epoch #321 Val   loss: 0.23526656011816308\n",
      "Validation Iteration #50 loss: 0.22676919400691986\n",
      "Epoch #322 Train loss: 0.17704614840055766\n",
      "Epoch #322 Val   loss: 0.23519681771686174\n",
      "Validation Iteration #50 loss: 0.23391872644424438\n",
      "Epoch #323 Train loss: 0.17611568695620486\n",
      "Epoch #323 Val   loss: 0.23512500018263474\n",
      "Validation Iteration #50 loss: 0.2282826155424118\n",
      "Epoch #324 Train loss: 0.17686603884947927\n",
      "Epoch #324 Val   loss: 0.23505262887336797\n",
      "Validation Iteration #50 loss: 0.23484688997268677\n",
      "Epoch #325 Train loss: 0.17575260918391378\n",
      "Epoch #325 Val   loss: 0.2349786325874279\n",
      "Validation Iteration #50 loss: 0.23486225306987762\n",
      "Epoch #326 Train loss: 0.17501994771392723\n",
      "Epoch #326 Val   loss: 0.2349110108295684\n",
      "Validation Iteration #50 loss: 0.23831605911254883\n",
      "Epoch #327 Train loss: 0.17442776419614492\n",
      "Epoch #327 Val   loss: 0.23484182768408735\n",
      "Validation Iteration #50 loss: 0.23530399799346924\n",
      "Epoch #328 Train loss: 0.17401004778711418\n",
      "Epoch #328 Val   loss: 0.23476759434749736\n",
      "Validation Iteration #50 loss: 0.23612704873085022\n",
      "Epoch #329 Train loss: 0.17438366538599917\n",
      "Epoch #329 Val   loss: 0.23469032893230865\n",
      "Validation Iteration #50 loss: 0.23509375751018524\n",
      "Epoch #330 Train loss: 0.17382985039761192\n",
      "Epoch #330 Val   loss: 0.2346148116974229\n",
      "Validation Iteration #50 loss: 0.23521925508975983\n",
      "Epoch #331 Train loss: 0.1732591723925189\n",
      "Epoch #331 Val   loss: 0.23453887263045364\n",
      "Validation Iteration #50 loss: 0.2273266315460205\n",
      "Epoch #332 Train loss: 0.17383766919374466\n",
      "Epoch #332 Val   loss: 0.23446344316941978\n",
      "Validation Iteration #50 loss: 0.23509293794631958\n",
      "Epoch #333 Train loss: 0.17269805740368993\n",
      "Epoch #333 Val   loss: 0.2343908612771223\n",
      "Validation Iteration #50 loss: 0.2320861965417862\n",
      "Epoch #334 Train loss: 0.17198609207805834\n",
      "Epoch #334 Val   loss: 0.23431499695531535\n",
      "Validation Iteration #50 loss: 0.2335166335105896\n",
      "Epoch #335 Train loss: 0.17150735423753136\n",
      "Epoch #335 Val   loss: 0.2342446016304659\n",
      "Validation Iteration #50 loss: 0.22877204418182373\n",
      "Epoch #336 Train loss: 0.17149204074552185\n",
      "Epoch #336 Val   loss: 0.2341767118772234\n",
      "Validation Iteration #50 loss: 0.22845673561096191\n",
      "Epoch #337 Train loss: 0.1696092962826553\n",
      "Epoch #337 Val   loss: 0.23411011302815712\n",
      "Validation Iteration #50 loss: 0.2297954112291336\n",
      "Epoch #338 Train loss: 0.17008257892570997\n",
      "Epoch #338 Val   loss: 0.23404504427745929\n",
      "Validation Iteration #50 loss: 0.23269912600517273\n",
      "Epoch #339 Train loss: 0.16845982639413132\n",
      "Epoch #339 Val   loss: 0.23398308819707703\n",
      "Validation Iteration #50 loss: 0.2292272448539734\n",
      "Epoch #340 Train loss: 0.16906457237507166\n",
      "Epoch #340 Val   loss: 0.2339197973143616\n",
      "Validation Iteration #50 loss: 0.22909724712371826\n",
      "Epoch #341 Train loss: 0.16923267355090693\n",
      "Epoch #341 Val   loss: 0.23385804609717736\n",
      "Validation Iteration #50 loss: 0.2321050763130188\n",
      "Epoch #342 Train loss: 0.1668034864491538\n",
      "Epoch #342 Val   loss: 0.2337973777068573\n",
      "Validation Iteration #50 loss: 0.2275659292936325\n",
      "Epoch #343 Train loss: 0.16797285997553876\n",
      "Epoch #343 Val   loss: 0.23373526803256148\n",
      "Validation Iteration #50 loss: 0.2296081781387329\n",
      "Epoch #344 Train loss: 0.16732456476280563\n",
      "Epoch #344 Val   loss: 0.2336767883969257\n",
      "Validation Iteration #50 loss: 0.22743761539459229\n",
      "Epoch #345 Train loss: 0.16634729641832804\n",
      "Epoch #345 Val   loss: 0.233615545858246\n",
      "Validation Iteration #50 loss: 0.23037466406822205\n",
      "Epoch #346 Train loss: 0.16528717232377907\n",
      "Epoch #346 Val   loss: 0.23355838646185265\n",
      "Validation Iteration #50 loss: 0.2297893911600113\n",
      "Epoch #347 Train loss: 0.16594969128307543\n",
      "Epoch #347 Val   loss: 0.23349847744699076\n",
      "Validation Iteration #50 loss: 0.22874417901039124\n",
      "Epoch #348 Train loss: 0.16550517474350177\n",
      "Epoch #348 Val   loss: 0.23344089565086343\n",
      "Validation Iteration #50 loss: 0.23663702607154846\n",
      "Epoch #349 Train loss: 0.16558730229735374\n",
      "Epoch #349 Val   loss: 0.23338446959361925\n",
      "Validation Iteration #50 loss: 0.2397403120994568\n",
      "Epoch #350 Train loss: 0.16504322482567085\n",
      "Epoch #350 Val   loss: 0.2333290825026249\n",
      "Validation Iteration #50 loss: 0.2404118925333023\n",
      "Epoch #351 Train loss: 0.1649511195719242\n",
      "Epoch #351 Val   loss: 0.23327183306959648\n",
      "Validation Iteration #50 loss: 0.24736233055591583\n",
      "Epoch #352 Train loss: 0.16567073076179153\n",
      "Epoch #352 Val   loss: 0.23321247759026933\n",
      "Validation Iteration #50 loss: 0.24201977252960205\n",
      "Epoch #353 Train loss: 0.1644579879939556\n",
      "Epoch #353 Val   loss: 0.2331572465375163\n",
      "Validation Iteration #50 loss: 0.24085910618305206\n",
      "Epoch #354 Train loss: 0.1633112261954107\n",
      "Epoch #354 Val   loss: 0.23309930931708148\n",
      "Validation Iteration #50 loss: 0.23993413150310516\n",
      "Epoch #355 Train loss: 0.16374278146969645\n",
      "Epoch #355 Val   loss: 0.23304051916817703\n",
      "Validation Iteration #50 loss: 0.24783502519130707\n",
      "Epoch #356 Train loss: 0.16177096884501607\n",
      "Epoch #356 Val   loss: 0.2329842285673517\n",
      "Validation Iteration #50 loss: 0.2466251254081726\n",
      "Epoch #357 Train loss: 0.16248078408994174\n",
      "Epoch #357 Val   loss: 0.2329275880151416\n",
      "Validation Iteration #50 loss: 0.2468739002943039\n",
      "Epoch #358 Train loss: 0.16084274826081177\n",
      "Epoch #358 Val   loss: 0.23286875603807952\n",
      "Validation Iteration #50 loss: 0.2503407597541809\n",
      "Epoch #359 Train loss: 0.15983323674452932\n",
      "Epoch #359 Val   loss: 0.2328127310380467\n",
      "Validation Iteration #50 loss: 0.2500706911087036\n",
      "Epoch #360 Train loss: 0.1597566981064646\n",
      "Epoch #360 Val   loss: 0.23275400733747081\n",
      "Validation Iteration #50 loss: 0.2538554072380066\n",
      "Epoch #361 Train loss: 0.16002609639575607\n",
      "Epoch #361 Val   loss: 0.23269879051056305\n",
      "Validation Iteration #50 loss: 0.25311926007270813\n",
      "Epoch #362 Train loss: 0.15807221712250458\n",
      "Epoch #362 Val   loss: 0.2326428104625412\n",
      "Validation Iteration #50 loss: 0.253505676984787\n",
      "Epoch #363 Train loss: 0.1583943751297499\n",
      "Epoch #363 Val   loss: 0.23258493106972838\n",
      "Validation Iteration #50 loss: 0.25671088695526123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #364 Train loss: 0.1580626533219689\n",
      "Epoch #364 Val   loss: 0.23252749040455914\n",
      "Validation Iteration #50 loss: 0.24790075421333313\n",
      "Epoch #365 Train loss: 0.15781829408124873\n",
      "Epoch #365 Val   loss: 0.23246610668237694\n",
      "Validation Iteration #50 loss: 0.25726598501205444\n",
      "Epoch #366 Train loss: 0.1566423526719997\n",
      "Epoch #366 Val   loss: 0.23241084478006901\n",
      "Validation Iteration #50 loss: 0.25585150718688965\n",
      "Epoch #367 Train loss: 0.1548733356359758\n",
      "Epoch #367 Val   loss: 0.23235503080796238\n",
      "Validation Iteration #50 loss: 0.25032633543014526\n",
      "Epoch #368 Train loss: 0.15480172614517965\n",
      "Epoch #368 Val   loss: 0.23229794744118715\n",
      "Validation Iteration #50 loss: 0.24960002303123474\n",
      "Epoch #369 Train loss: 0.1533893270320014\n",
      "Epoch #369 Val   loss: 0.2322397361797999\n",
      "Validation Iteration #50 loss: 0.2548479437828064\n",
      "Epoch #370 Train loss: 0.1531638426608161\n",
      "Epoch #370 Val   loss: 0.23218070202328076\n",
      "Validation Iteration #50 loss: 0.24954955279827118\n",
      "Epoch #371 Train loss: 0.15281496157771662\n",
      "Epoch #371 Val   loss: 0.23212350651350663\n",
      "Validation Iteration #50 loss: 0.25749853253364563\n",
      "Epoch #372 Train loss: 0.1528738319481674\n",
      "Epoch #372 Val   loss: 0.23206126651497913\n",
      "Validation Iteration #50 loss: 0.25464028120040894\n",
      "Epoch #373 Train loss: 0.1528671644628048\n",
      "Epoch #373 Val   loss: 0.2320001517265042\n",
      "Validation Iteration #50 loss: 0.25583380460739136\n",
      "Epoch #374 Train loss: 0.15203927320085073\n",
      "Epoch #374 Val   loss: 0.23193978231992476\n",
      "Validation Iteration #50 loss: 0.2543192505836487\n",
      "Epoch #375 Train loss: 0.15166500759752174\n",
      "Epoch #375 Val   loss: 0.23187825070623974\n",
      "Validation Iteration #50 loss: 0.2541147768497467\n",
      "Epoch #376 Train loss: 0.1515922909112353\n",
      "Epoch #376 Val   loss: 0.23181941750474766\n",
      "Validation Iteration #50 loss: 0.25233137607574463\n",
      "Epoch #377 Train loss: 0.15297529611148333\n",
      "Epoch #377 Val   loss: 0.2317603389494876\n",
      "Validation Iteration #50 loss: 0.25474631786346436\n",
      "Epoch #378 Train loss: 0.15205190664059237\n",
      "Epoch #378 Val   loss: 0.23170072310578638\n",
      "Validation Iteration #50 loss: 0.2536814212799072\n",
      "Epoch #379 Train loss: 0.1530695700723874\n",
      "Epoch #379 Val   loss: 0.23164418983133697\n",
      "Validation Iteration #50 loss: 0.25678548216819763\n",
      "Epoch #380 Train loss: 0.15254604757616394\n",
      "Epoch #380 Val   loss: 0.23158462398249094\n",
      "Validation Iteration #50 loss: 0.25246378779411316\n",
      "Epoch #381 Train loss: 0.15096461851345866\n",
      "Epoch #381 Val   loss: 0.2315232834410773\n",
      "Validation Iteration #50 loss: 0.24572984874248505\n",
      "Epoch #382 Train loss: 0.15005673840641975\n",
      "Epoch #382 Val   loss: 0.23146409425098127\n",
      "Validation Iteration #50 loss: 0.24560919404029846\n",
      "Epoch #383 Train loss: 0.1516326387461863\n",
      "Epoch #383 Val   loss: 0.23140400621550491\n",
      "Validation Iteration #50 loss: 0.24913398921489716\n",
      "Epoch #384 Train loss: 0.15030356516179286\n",
      "Epoch #384 Val   loss: 0.2313402365792643\n",
      "Validation Iteration #50 loss: 0.2475612461566925\n",
      "Epoch #385 Train loss: 0.1496427319943905\n",
      "Epoch #385 Val   loss: 0.23127845765324218\n",
      "Validation Iteration #50 loss: 0.2470792829990387\n",
      "Epoch #386 Train loss: 0.14934093936493523\n",
      "Epoch #386 Val   loss: 0.23121621942359016\n",
      "Validation Iteration #50 loss: 0.24642536044120789\n",
      "Epoch #387 Train loss: 0.149974196561073\n",
      "Epoch #387 Val   loss: 0.23115627794666105\n",
      "Validation Iteration #50 loss: 0.24684123694896698\n",
      "Epoch #388 Train loss: 0.14935304126457163\n",
      "Epoch #388 Val   loss: 0.23109683545889392\n",
      "Validation Iteration #50 loss: 0.2514890432357788\n",
      "Epoch #389 Train loss: 0.14871216114414365\n",
      "Epoch #389 Val   loss: 0.23104093996201747\n",
      "Validation Iteration #50 loss: 0.248337522149086\n",
      "Epoch #390 Train loss: 0.14900511130690575\n",
      "Epoch #390 Val   loss: 0.23097903599672\n",
      "Validation Iteration #50 loss: 0.25133398175239563\n",
      "Epoch #391 Train loss: 0.14859809687263087\n",
      "Epoch #391 Val   loss: 0.23092033315502586\n",
      "Validation Iteration #50 loss: 0.24757736921310425\n",
      "Epoch #392 Train loss: 0.14722080548342906\n",
      "Epoch #392 Val   loss: 0.2308661327258928\n",
      "Validation Iteration #50 loss: 0.24681121110916138\n",
      "Epoch #393 Train loss: 0.1467844792886784\n",
      "Epoch #393 Val   loss: 0.2308064955739433\n",
      "Validation Iteration #50 loss: 0.24902719259262085\n",
      "Epoch #394 Train loss: 0.14681766162577428\n",
      "Epoch #394 Val   loss: 0.23075137691175113\n",
      "Validation Iteration #50 loss: 0.25017452239990234\n",
      "Epoch #395 Train loss: 0.147551936342528\n",
      "Epoch #395 Val   loss: 0.2306937706492488\n",
      "Validation Iteration #50 loss: 0.24633461236953735\n",
      "Epoch #396 Train loss: 0.14625920884703336\n",
      "Epoch #396 Val   loss: 0.23063475159198085\n",
      "Validation Iteration #50 loss: 0.24487942457199097\n",
      "Epoch #397 Train loss: 0.1463953525220093\n",
      "Epoch #397 Val   loss: 0.23057717335157335\n",
      "Validation Iteration #50 loss: 0.23623254895210266\n",
      "Epoch #398 Train loss: 0.14477143711165377\n",
      "Epoch #398 Val   loss: 0.23051926237679135\n",
      "Validation Iteration #50 loss: 0.2371365875005722\n",
      "Epoch #399 Train loss: 0.14617724657842987\n",
      "Epoch #399 Val   loss: 0.23045624449562568\n",
      "Validation Iteration #50 loss: 0.2359342724084854\n",
      "Epoch #400 Train loss: 0.14423918469171776\n",
      "Epoch #400 Val   loss: 0.23039620763425883\n",
      "Validation Iteration #50 loss: 0.237653449177742\n",
      "Epoch #401 Train loss: 0.14412578901177958\n",
      "Epoch #401 Val   loss: 0.230331871876692\n",
      "Validation Iteration #50 loss: 0.23815320432186127\n",
      "Epoch #402 Train loss: 0.14462474478702797\n",
      "Epoch #402 Val   loss: 0.23026566695202214\n",
      "Validation Iteration #50 loss: 0.2498307079076767\n",
      "Epoch #403 Train loss: 0.14489351291405528\n",
      "Epoch #403 Val   loss: 0.23020382608256537\n",
      "Validation Iteration #50 loss: 0.24984526634216309\n",
      "Epoch #404 Train loss: 0.14380705827160886\n",
      "Epoch #404 Val   loss: 0.2301473827896408\n",
      "Validation Iteration #50 loss: 0.25133639574050903\n",
      "Epoch #405 Train loss: 0.14263919427206642\n",
      "Epoch #405 Val   loss: 0.23008867406560507\n",
      "Validation Iteration #50 loss: 0.24983108043670654\n",
      "Epoch #406 Train loss: 0.1423565127739781\n",
      "Epoch #406 Val   loss: 0.23003092907514178\n",
      "Validation Iteration #50 loss: 0.24812151491641998\n",
      "Epoch #407 Train loss: 0.14194788352439278\n",
      "Epoch #407 Val   loss: 0.22997171408742623\n",
      "Validation Iteration #50 loss: 0.2501257061958313\n",
      "Epoch #408 Train loss: 0.1415969821575441\n",
      "Epoch #408 Val   loss: 0.2299125777704276\n",
      "Validation Iteration #50 loss: 0.2391522228717804\n",
      "Epoch #409 Train loss: 0.14203147648980743\n",
      "Epoch #409 Val   loss: 0.22984866088371564\n",
      "Validation Iteration #50 loss: 0.2418336123228073\n",
      "Epoch #410 Train loss: 0.14257872516387388\n",
      "Epoch #410 Val   loss: 0.22978474424837145\n",
      "Validation Iteration #50 loss: 0.22824105620384216\n",
      "Epoch #411 Train loss: 0.14051585271954536\n",
      "Epoch #411 Val   loss: 0.22972050584613432\n",
      "Validation Iteration #50 loss: 0.23175056278705597\n",
      "Epoch #412 Train loss: 0.14069333221567304\n",
      "Epoch #412 Val   loss: 0.22965343517172734\n",
      "Validation Iteration #50 loss: 0.23138591647148132\n",
      "Epoch #413 Train loss: 0.14052275881955498\n",
      "Epoch #413 Val   loss: 0.22958971668811412\n",
      "Validation Iteration #50 loss: 0.22725337743759155\n",
      "Epoch #414 Train loss: 0.1398735424797786\n",
      "Epoch #414 Val   loss: 0.22952284987763855\n",
      "Validation Iteration #50 loss: 0.22987626492977142\n",
      "Epoch #415 Train loss: 0.13949836810168467\n",
      "Epoch #415 Val   loss: 0.22945733425984458\n",
      "Validation Iteration #50 loss: 0.241976797580719\n",
      "Epoch #416 Train loss: 0.13922453573659846\n",
      "Epoch #416 Val   loss: 0.22939491397042355\n",
      "Validation Iteration #50 loss: 0.22820182144641876\n",
      "Epoch #417 Train loss: 0.13841556561620613\n",
      "Epoch #417 Val   loss: 0.22932960290734627\n",
      "Validation Iteration #50 loss: 0.24041497707366943\n",
      "Epoch #418 Train loss: 0.1373446211218834\n",
      "Epoch #418 Val   loss: 0.2292676774327725\n",
      "Validation Iteration #50 loss: 0.22731012105941772\n",
      "Epoch #419 Train loss: 0.13720082589670232\n",
      "Epoch #419 Val   loss: 0.22920470022674883\n",
      "Validation Iteration #50 loss: 0.22348618507385254\n",
      "Epoch #420 Train loss: 0.13840479540981745\n",
      "Epoch #420 Val   loss: 0.22913841595003684\n",
      "Validation Iteration #50 loss: 0.2245260626077652\n",
      "Epoch #421 Train loss: 0.13682773670083598\n",
      "Epoch #421 Val   loss: 0.22907650086660694\n",
      "Validation Iteration #50 loss: 0.22497908771038055\n",
      "Epoch #422 Train loss: 0.13652177586367256\n",
      "Epoch #422 Val   loss: 0.22901049155520403\n",
      "Validation Iteration #50 loss: 0.21607515215873718\n",
      "Epoch #423 Train loss: 0.136478885223991\n",
      "Epoch #423 Val   loss: 0.228943646335312\n",
      "Validation Iteration #50 loss: 0.21514654159545898\n",
      "Epoch #424 Train loss: 0.13718739936226293\n",
      "Epoch #424 Val   loss: 0.22887829782736246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.2178821563720703\n",
      "Epoch #425 Train loss: 0.13633841825158974\n",
      "Epoch #425 Val   loss: 0.22880996851692478\n",
      "Validation Iteration #50 loss: 0.21651208400726318\n",
      "Epoch #426 Train loss: 0.13675992975109502\n",
      "Epoch #426 Val   loss: 0.22874220775937287\n",
      "Validation Iteration #50 loss: 0.21742652356624603\n",
      "Epoch #427 Train loss: 0.13576965524177803\n",
      "Epoch #427 Val   loss: 0.22867336178815767\n",
      "Validation Iteration #50 loss: 0.2162363976240158\n",
      "Epoch #428 Train loss: 0.13605046487952532\n",
      "Epoch #428 Val   loss: 0.2286032411051583\n",
      "Validation Iteration #50 loss: 0.2167111486196518\n",
      "Epoch #429 Train loss: 0.13469764805938067\n",
      "Epoch #429 Val   loss: 0.22853457700054633\n",
      "Validation Iteration #50 loss: 0.21609055995941162\n",
      "Epoch #430 Train loss: 0.1341715568774625\n",
      "Epoch #430 Val   loss: 0.22846370074029013\n",
      "Validation Iteration #50 loss: 0.21826007962226868\n",
      "Epoch #431 Train loss: 0.13373056367823952\n",
      "Epoch #431 Val   loss: 0.22839211859339661\n",
      "Validation Iteration #50 loss: 0.21529947221279144\n",
      "Epoch #432 Train loss: 0.13408761628364263\n",
      "Epoch #432 Val   loss: 0.2283235327280273\n",
      "Validation Iteration #50 loss: 0.2144785076379776\n",
      "Epoch #433 Train loss: 0.1329070429660772\n",
      "Epoch #433 Val   loss: 0.22825361845849126\n",
      "Validation Iteration #50 loss: 0.2252494990825653\n",
      "Epoch #434 Train loss: 0.13353832497408516\n",
      "Epoch #434 Val   loss: 0.2281804156319216\n",
      "Validation Iteration #50 loss: 0.22309675812721252\n",
      "Epoch #435 Train loss: 0.13200560702305092\n",
      "Epoch #435 Val   loss: 0.22810955060384352\n",
      "Validation Iteration #50 loss: 0.2102689892053604\n",
      "Epoch #436 Train loss: 0.1323273068195895\n",
      "Epoch #436 Val   loss: 0.2280368983897708\n",
      "Validation Iteration #50 loss: 0.21394991874694824\n",
      "Epoch #437 Train loss: 0.13122493852125972\n",
      "Epoch #437 Val   loss: 0.22796456596130735\n",
      "Validation Iteration #50 loss: 0.211757630109787\n",
      "Epoch #438 Train loss: 0.13141524713290365\n",
      "Epoch #438 Val   loss: 0.22789124621491894\n",
      "Validation Iteration #50 loss: 0.2096787989139557\n",
      "Epoch #439 Train loss: 0.13147613268933797\n",
      "Epoch #439 Val   loss: 0.2278155449048414\n",
      "Validation Iteration #50 loss: 0.21141093969345093\n",
      "Epoch #440 Train loss: 0.13120209190406298\n",
      "Epoch #440 Val   loss: 0.22773919180854335\n",
      "Validation Iteration #50 loss: 0.20707091689109802\n",
      "Epoch #441 Train loss: 0.13201338091963216\n",
      "Epoch #441 Val   loss: 0.2276626386129603\n",
      "Validation Iteration #50 loss: 0.2079065591096878\n",
      "Epoch #442 Train loss: 0.13125242685016833\n",
      "Epoch #442 Val   loss: 0.22758682248627152\n",
      "Validation Iteration #50 loss: 0.20763908326625824\n",
      "Epoch #443 Train loss: 0.12993332645610758\n",
      "Epoch #443 Val   loss: 0.22751123363401274\n",
      "Validation Iteration #50 loss: 0.21121326088905334\n",
      "Epoch #444 Train loss: 0.13024788918463806\n",
      "Epoch #444 Val   loss: 0.22743619005022733\n",
      "Validation Iteration #50 loss: 0.20970948040485382\n",
      "Epoch #445 Train loss: 0.13035431935598976\n",
      "Epoch #445 Val   loss: 0.22736110503548546\n",
      "Validation Iteration #50 loss: 0.21159417927265167\n",
      "Epoch #446 Train loss: 0.12999272581778074\n",
      "Epoch #446 Val   loss: 0.22728401178122593\n",
      "Validation Iteration #50 loss: 0.21242684125900269\n",
      "Epoch #447 Train loss: 0.12976622934404172\n",
      "Epoch #447 Val   loss: 0.22721091323470757\n",
      "Validation Iteration #50 loss: 0.21354663372039795\n",
      "Epoch #448 Train loss: 0.12978559183446983\n",
      "Epoch #448 Val   loss: 0.22714155391101173\n",
      "Validation Iteration #50 loss: 0.2129804790019989\n",
      "Epoch #449 Train loss: 0.1304643244335526\n",
      "Epoch #449 Val   loss: 0.2270694631873033\n",
      "Validation Iteration #50 loss: 0.21379508078098297\n",
      "Epoch #450 Train loss: 0.12775997621448418\n",
      "Epoch #450 Val   loss: 0.22699846652624409\n",
      "Validation Iteration #50 loss: 0.2146422266960144\n",
      "Epoch #451 Train loss: 0.12913766699401955\n",
      "Epoch #451 Val   loss: 0.22692521453319642\n",
      "Validation Iteration #50 loss: 0.21329577267169952\n",
      "Epoch #452 Train loss: 0.12815960516270838\n",
      "Epoch #452 Val   loss: 0.22685193383677502\n",
      "Validation Iteration #50 loss: 0.2175798863172531\n",
      "Epoch #453 Train loss: 0.1285308285763389\n",
      "Epoch #453 Val   loss: 0.22677998939325186\n",
      "Validation Iteration #50 loss: 0.21200960874557495\n",
      "Epoch #454 Train loss: 0.12764806790571465\n",
      "Epoch #454 Val   loss: 0.226705855022823\n",
      "Validation Iteration #50 loss: 0.21052061021327972\n",
      "Epoch #455 Train loss: 0.12703055162963114\n",
      "Epoch #455 Val   loss: 0.22663377999113157\n",
      "Validation Iteration #50 loss: 0.21119113266468048\n",
      "Epoch #456 Train loss: 0.12717186639967717\n",
      "Epoch #456 Val   loss: 0.22656084853447842\n",
      "Validation Iteration #50 loss: 0.21570435166358948\n",
      "Epoch #457 Train loss: 0.1263019224922908\n",
      "Epoch #457 Val   loss: 0.22649082786731764\n",
      "Validation Iteration #50 loss: 0.20965176820755005\n",
      "Epoch #458 Train loss: 0.12653340163983798\n",
      "Epoch #458 Val   loss: 0.22641740807395935\n",
      "Validation Iteration #50 loss: 0.21018794178962708\n",
      "Epoch #459 Train loss: 0.12604063435604698\n",
      "Epoch #459 Val   loss: 0.22634650498105333\n",
      "Validation Iteration #50 loss: 0.2040359079837799\n",
      "Epoch #460 Train loss: 0.12533412285541234\n",
      "Epoch #460 Val   loss: 0.22627545279666256\n",
      "Validation Iteration #50 loss: 0.20985770225524902\n",
      "Epoch #461 Train loss: 0.12518835283423724\n",
      "Epoch #461 Val   loss: 0.22620526012845765\n",
      "Validation Iteration #50 loss: 0.20194312930107117\n",
      "Epoch #462 Train loss: 0.12471006281281773\n",
      "Epoch #462 Val   loss: 0.22613459917487527\n",
      "Validation Iteration #50 loss: 0.20194309949874878\n",
      "Epoch #463 Train loss: 0.12489023706630657\n",
      "Epoch #463 Val   loss: 0.226064947183029\n",
      "Validation Iteration #50 loss: 0.20055735111236572\n",
      "Epoch #464 Train loss: 0.12385263313588343\n",
      "Epoch #464 Val   loss: 0.22599398088144607\n",
      "Validation Iteration #50 loss: 0.2045242339372635\n",
      "Epoch #465 Train loss: 0.12457230824388955\n",
      "Epoch #465 Val   loss: 0.22592426041376673\n",
      "Validation Iteration #50 loss: 0.20168434083461761\n",
      "Epoch #466 Train loss: 0.12365094611519262\n",
      "Epoch #466 Val   loss: 0.22585674055476612\n",
      "Validation Iteration #50 loss: 0.20216615498065948\n",
      "Epoch #467 Train loss: 0.12349110802537516\n",
      "Epoch #467 Val   loss: 0.2257881832568074\n",
      "Validation Iteration #50 loss: 0.19871091842651367\n",
      "Epoch #468 Train loss: 0.12272800916903898\n",
      "Epoch #468 Val   loss: 0.22571938953669282\n",
      "Validation Iteration #50 loss: 0.21289685368537903\n",
      "Epoch #469 Train loss: 0.12282759225682209\n",
      "Epoch #469 Val   loss: 0.22564935628721056\n",
      "Validation Iteration #50 loss: 0.1961810290813446\n",
      "Epoch #470 Train loss: 0.12182102548448663\n",
      "Epoch #470 Val   loss: 0.22557889252278066\n",
      "Validation Iteration #50 loss: 0.21463486552238464\n",
      "Epoch #471 Train loss: 0.12188994021792161\n",
      "Epoch #471 Val   loss: 0.2255123259519097\n",
      "Validation Iteration #50 loss: 0.20401015877723694\n",
      "Epoch #472 Train loss: 0.12088142982438992\n",
      "Epoch #472 Val   loss: 0.22544644606643663\n",
      "Validation Iteration #50 loss: 0.19910795986652374\n",
      "Epoch #473 Train loss: 0.1200708388105819\n",
      "Epoch #473 Val   loss: 0.22537966394789108\n",
      "Validation Iteration #50 loss: 0.20429210364818573\n",
      "Epoch #474 Train loss: 0.12007440391339753\n",
      "Epoch #474 Val   loss: 0.22530970031312603\n",
      "Validation Iteration #50 loss: 0.2046738713979721\n",
      "Epoch #475 Train loss: 0.1205593639690625\n",
      "Epoch #475 Val   loss: 0.22524419517341204\n",
      "Validation Iteration #50 loss: 0.2141648530960083\n",
      "Epoch #476 Train loss: 0.11978432496911601\n",
      "Epoch #476 Val   loss: 0.22517520355935097\n",
      "Validation Iteration #50 loss: 0.21766868233680725\n",
      "Epoch #477 Train loss: 0.12129964561838853\n",
      "Epoch #477 Val   loss: 0.2251123072100503\n",
      "Validation Iteration #50 loss: 0.2088184654712677\n",
      "Epoch #478 Train loss: 0.11938529383195073\n",
      "Epoch #478 Val   loss: 0.22504764483855436\n",
      "Validation Iteration #50 loss: 0.2107393443584442\n",
      "Epoch #479 Train loss: 0.11840747610518806\n",
      "Epoch #479 Val   loss: 0.22498154591124217\n",
      "Validation Iteration #50 loss: 0.21137475967407227\n",
      "Epoch #480 Train loss: 0.1180447869394955\n",
      "Epoch #480 Val   loss: 0.22491677431069468\n",
      "Validation Iteration #50 loss: 0.2137851119041443\n",
      "Epoch #481 Train loss: 0.11782527499293026\n",
      "Epoch #481 Val   loss: 0.22485225605001055\n",
      "Validation Iteration #50 loss: 0.21328040957450867\n",
      "Epoch #482 Train loss: 0.11761436983942986\n",
      "Epoch #482 Val   loss: 0.2247875977303405\n",
      "Validation Iteration #50 loss: 0.21593068540096283\n",
      "Epoch #483 Train loss: 0.1172482404661806\n",
      "Epoch #483 Val   loss: 0.22472621504518156\n",
      "Validation Iteration #50 loss: 0.21775424480438232\n",
      "Epoch #484 Train loss: 0.11646576225757599\n",
      "Epoch #484 Val   loss: 0.22466548274283746\n",
      "Validation Iteration #50 loss: 0.2171112447977066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #485 Train loss: 0.11640330796179019\n",
      "Epoch #485 Val   loss: 0.224604209172137\n",
      "Validation Iteration #50 loss: 0.21629686653614044\n",
      "Epoch #486 Train loss: 0.11698296097548384\n",
      "Epoch #486 Val   loss: 0.22454768347831497\n",
      "Validation Iteration #50 loss: 0.21965950727462769\n",
      "Epoch #487 Train loss: 0.11607071522035096\n",
      "Epoch #487 Val   loss: 0.2244878957672855\n",
      "Validation Iteration #50 loss: 0.2213863581418991\n",
      "Epoch #488 Train loss: 0.11636122355335637\n",
      "Epoch #488 Val   loss: 0.22443119532374328\n",
      "Validation Iteration #50 loss: 0.21995557844638824\n",
      "Epoch #489 Train loss: 0.11556214388263852\n",
      "Epoch #489 Val   loss: 0.22437058287311573\n",
      "Validation Iteration #50 loss: 0.21972814202308655\n",
      "Epoch #490 Train loss: 0.11530627349489614\n",
      "Epoch #490 Val   loss: 0.2243146781936421\n",
      "Validation Iteration #50 loss: 0.21821606159210205\n",
      "Epoch #491 Train loss: 0.11604370235612518\n",
      "Epoch #491 Val   loss: 0.22425870560901734\n",
      "Validation Iteration #50 loss: 0.21634991466999054\n",
      "Epoch #492 Train loss: 0.11500451047169535\n",
      "Epoch #492 Val   loss: 0.22420026072126448\n",
      "Validation Iteration #50 loss: 0.22034882009029388\n",
      "Epoch #493 Train loss: 0.11540557953872178\n",
      "Epoch #493 Val   loss: 0.22414062534261997\n",
      "Validation Iteration #50 loss: 0.2176365852355957\n",
      "Epoch #494 Train loss: 0.11469648131414463\n",
      "Epoch #494 Val   loss: 0.22408348348660348\n",
      "Validation Iteration #50 loss: 0.21587656438350677\n",
      "Epoch #495 Train loss: 0.11446443435392882\n",
      "Epoch #495 Val   loss: 0.22402723090002133\n",
      "Validation Iteration #50 loss: 0.21369193494319916\n",
      "Epoch #496 Train loss: 0.11461289148581655\n",
      "Epoch #496 Val   loss: 0.2239707940755531\n",
      "Validation Iteration #50 loss: 0.21397456526756287\n",
      "Epoch #497 Train loss: 0.1147985787768113\n",
      "Epoch #497 Val   loss: 0.2239177700547587\n",
      "Validation Iteration #50 loss: 0.2177158147096634\n",
      "Epoch #498 Train loss: 0.11490059702804215\n",
      "Epoch #498 Val   loss: 0.22386350893535104\n",
      "Validation Iteration #50 loss: 0.214054673910141\n",
      "Epoch #499 Train loss: 0.11491007985253084\n",
      "Epoch #499 Val   loss: 0.22381042623749145\n",
      "Validation Iteration #50 loss: 0.21323621273040771\n",
      "Epoch #500 Train loss: 0.11458969978909743\n",
      "Epoch #500 Val   loss: 0.2237568423815464\n",
      "Validation Iteration #50 loss: 0.21583178639411926\n",
      "Epoch #501 Train loss: 0.11373903033764739\n",
      "Epoch #501 Val   loss: 0.22370389825086487\n",
      "Validation Iteration #50 loss: 0.21361176669597626\n",
      "Epoch #502 Train loss: 0.11367710248420113\n",
      "Epoch #502 Val   loss: 0.22365030441003064\n",
      "Validation Iteration #50 loss: 0.21623800694942474\n",
      "Epoch #503 Train loss: 0.11276408971140259\n",
      "Epoch #503 Val   loss: 0.2236036788649466\n",
      "Validation Iteration #50 loss: 0.2147832214832306\n",
      "Epoch #504 Train loss: 0.11321571645768065\n",
      "Epoch #504 Val   loss: 0.22355152086751295\n",
      "Validation Iteration #50 loss: 0.21558289229869843\n",
      "Epoch #505 Train loss: 0.11296289453380987\n",
      "Epoch #505 Val   loss: 0.2234984481915057\n",
      "Validation Iteration #50 loss: 0.21493327617645264\n",
      "Epoch #506 Train loss: 0.11259965422122102\n",
      "Epoch #506 Val   loss: 0.22344582471581118\n",
      "Validation Iteration #50 loss: 0.21149098873138428\n",
      "Epoch #507 Train loss: 0.11211319836346727\n",
      "Epoch #507 Val   loss: 0.22339546174343466\n",
      "Validation Iteration #50 loss: 0.21148017048835754\n",
      "Epoch #508 Train loss: 0.11181021991528962\n",
      "Epoch #508 Val   loss: 0.22334764732304918\n",
      "Validation Iteration #50 loss: 0.21056166291236877\n",
      "Epoch #509 Train loss: 0.11197538536630179\n",
      "Epoch #509 Val   loss: 0.2233019159570179\n",
      "Validation Iteration #50 loss: 0.21281282603740692\n",
      "Epoch #510 Train loss: 0.11185197924312792\n",
      "Epoch #510 Val   loss: 0.2232554968259752\n",
      "Validation Iteration #50 loss: 0.21430006623268127\n",
      "Epoch #511 Train loss: 0.11253678524180462\n",
      "Epoch #511 Val   loss: 0.22321249922298683\n",
      "Validation Iteration #50 loss: 0.21273422241210938\n",
      "Epoch #512 Train loss: 0.11143785401394493\n",
      "Epoch #512 Val   loss: 0.22316600140119924\n",
      "Validation Iteration #50 loss: 0.21325363218784332\n",
      "Epoch #513 Train loss: 0.11115285499315512\n",
      "Epoch #513 Val   loss: 0.22311748585986257\n",
      "Validation Iteration #50 loss: 0.20922082662582397\n",
      "Epoch #514 Train loss: 0.11039550563222483\n",
      "Epoch #514 Val   loss: 0.22307288381058926\n",
      "Validation Iteration #50 loss: 0.21088404953479767\n",
      "Epoch #515 Train loss: 0.11065317062955153\n",
      "Epoch #515 Val   loss: 0.2230285010958667\n",
      "Validation Iteration #50 loss: 0.22008487582206726\n",
      "Epoch #516 Train loss: 0.11020964678180845\n",
      "Epoch #516 Val   loss: 0.22298706724426076\n",
      "Validation Iteration #50 loss: 0.21663598716259003\n",
      "Epoch #517 Train loss: 0.10907396438874696\n",
      "Epoch #517 Val   loss: 0.22294300119358\n",
      "Validation Iteration #50 loss: 0.21501611173152924\n",
      "Epoch #518 Train loss: 0.10907008302839179\n",
      "Epoch #518 Val   loss: 0.2229006245325932\n",
      "Validation Iteration #50 loss: 0.22695110738277435\n",
      "Epoch #519 Train loss: 0.10871098621895439\n",
      "Epoch #519 Val   loss: 0.2228583824627939\n",
      "Validation Iteration #50 loss: 0.22666485607624054\n",
      "Epoch #520 Train loss: 0.10805730756960417\n",
      "Epoch #520 Val   loss: 0.22281843704243945\n",
      "Validation Iteration #50 loss: 0.2268172949552536\n",
      "Epoch #521 Train loss: 0.10843319779163912\n",
      "Epoch #521 Val   loss: 0.22277890436840977\n",
      "Validation Iteration #50 loss: 0.22304344177246094\n",
      "Epoch #522 Train loss: 0.10773605657251258\n",
      "Epoch #522 Val   loss: 0.22273938727674283\n",
      "Validation Iteration #50 loss: 0.22823287546634674\n",
      "Epoch #523 Train loss: 0.107890850423198\n",
      "Epoch #523 Val   loss: 0.22270194777406213\n",
      "Validation Iteration #50 loss: 0.22928327322006226\n",
      "Epoch #524 Train loss: 0.10705739789103207\n",
      "Epoch #524 Val   loss: 0.22266360167946134\n",
      "Validation Iteration #50 loss: 0.22664345800876617\n",
      "Epoch #525 Train loss: 0.10715708254199278\n",
      "Epoch #525 Val   loss: 0.22262469253634354\n",
      "Validation Iteration #50 loss: 0.22753523290157318\n",
      "Epoch #526 Train loss: 0.10749274766758869\n",
      "Epoch #526 Val   loss: 0.22259000851918861\n",
      "Validation Iteration #50 loss: 0.22517113387584686\n",
      "Epoch #527 Train loss: 0.10670547324575876\n",
      "Epoch #527 Val   loss: 0.2225543326765542\n",
      "Validation Iteration #50 loss: 0.22369278967380524\n",
      "Epoch #528 Train loss: 0.10598117505249224\n",
      "Epoch #528 Val   loss: 0.22251823212330088\n",
      "Validation Iteration #50 loss: 0.22971436381340027\n",
      "Epoch #529 Train loss: 0.10585219452255651\n",
      "Epoch #529 Val   loss: 0.22248259331647127\n",
      "Validation Iteration #50 loss: 0.2386169135570526\n",
      "Epoch #530 Train loss: 0.10647226399496983\n",
      "Epoch #530 Val   loss: 0.22244881793876048\n",
      "Validation Iteration #50 loss: 0.2267390787601471\n",
      "Epoch #531 Train loss: 0.10538916074131664\n",
      "Epoch #531 Val   loss: 0.22241315342908205\n",
      "Validation Iteration #50 loss: 0.2415911704301834\n",
      "Epoch #532 Train loss: 0.10595362986388959\n",
      "Epoch #532 Val   loss: 0.22238306675054137\n",
      "Validation Iteration #50 loss: 0.2423037439584732\n",
      "Epoch #533 Train loss: 0.1048231654261288\n",
      "Epoch #533 Val   loss: 0.2223535740044525\n",
      "Validation Iteration #50 loss: 0.225880965590477\n",
      "Epoch #534 Train loss: 0.10637294069716804\n",
      "Epoch #534 Val   loss: 0.22231969535243673\n",
      "Validation Iteration #50 loss: 0.23652006685733795\n",
      "Epoch #535 Train loss: 0.10562962313231669\n",
      "Epoch #535 Val   loss: 0.2222852828654746\n",
      "Validation Iteration #50 loss: 0.24784024059772491\n",
      "Epoch #536 Train loss: 0.10460682527015083\n",
      "Epoch #536 Val   loss: 0.222252690995718\n",
      "Validation Iteration #50 loss: 0.23284612596035004\n",
      "Epoch #537 Train loss: 0.10475019757684909\n",
      "Epoch #537 Val   loss: 0.22222067425727912\n",
      "Validation Iteration #50 loss: 0.23793524503707886\n",
      "Epoch #538 Train loss: 0.10472199752142555\n",
      "Epoch #538 Val   loss: 0.22218718611430183\n",
      "Validation Iteration #50 loss: 0.23275943100452423\n",
      "Epoch #539 Train loss: 0.1042179235894429\n",
      "Epoch #539 Val   loss: 0.22215369380692132\n",
      "Validation Iteration #50 loss: 0.24152004718780518\n",
      "Epoch #540 Train loss: 0.10295806707520234\n",
      "Epoch #540 Val   loss: 0.22212179882943706\n",
      "Validation Iteration #50 loss: 0.2326371669769287\n",
      "Epoch #541 Train loss: 0.10360852413271603\n",
      "Epoch #541 Val   loss: 0.22208722264606642\n",
      "Validation Iteration #50 loss: 0.24106615781784058\n",
      "Epoch #542 Train loss: 0.10368612075322553\n",
      "Epoch #542 Val   loss: 0.2220535691748903\n",
      "Validation Iteration #50 loss: 0.24108923971652985\n",
      "Epoch #543 Train loss: 0.10359361316812665\n",
      "Epoch #543 Val   loss: 0.22201955787272812\n",
      "Validation Iteration #50 loss: 0.23328055441379547\n",
      "Epoch #544 Train loss: 0.10329334006497734\n",
      "Epoch #544 Val   loss: 0.22198414410304754\n",
      "Validation Iteration #50 loss: 0.24232043325901031\n",
      "Epoch #545 Train loss: 0.10261126490015733\n",
      "Epoch #545 Val   loss: 0.22195382013803605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.24021892249584198\n",
      "Epoch #546 Train loss: 0.10225194653398112\n",
      "Epoch #546 Val   loss: 0.221920099016685\n",
      "Validation Iteration #50 loss: 0.2404460906982422\n",
      "Epoch #547 Train loss: 0.10229636964045073\n",
      "Epoch #547 Val   loss: 0.221888038987092\n",
      "Validation Iteration #50 loss: 0.24217213690280914\n",
      "Epoch #548 Train loss: 0.10213144025520275\n",
      "Epoch #548 Val   loss: 0.22185636893138175\n",
      "Validation Iteration #50 loss: 0.23481112718582153\n",
      "Epoch #549 Train loss: 0.10266457066724174\n",
      "Epoch #549 Val   loss: 0.22182468076060702\n",
      "Validation Iteration #50 loss: 0.23692883551120758\n",
      "Epoch #550 Train loss: 0.10205559177618277\n",
      "Epoch #550 Val   loss: 0.2217920644688037\n",
      "Validation Iteration #50 loss: 0.23185712099075317\n",
      "Epoch #551 Train loss: 0.10234420119147551\n",
      "Epoch #551 Val   loss: 0.22175644401672692\n",
      "Validation Iteration #50 loss: 0.23632293939590454\n",
      "Epoch #552 Train loss: 0.10204969699445524\n",
      "Epoch #552 Val   loss: 0.22172333390072002\n",
      "Validation Iteration #50 loss: 0.2342221587896347\n",
      "Epoch #553 Train loss: 0.10079769045114517\n",
      "Epoch #553 Val   loss: 0.22168662171460166\n",
      "Validation Iteration #50 loss: 0.2343769073486328\n",
      "Epoch #554 Train loss: 0.10137167808256652\n",
      "Epoch #554 Val   loss: 0.22165317250801636\n",
      "Validation Iteration #50 loss: 0.2358008623123169\n",
      "Epoch #555 Train loss: 0.10072748147343334\n",
      "Epoch #555 Val   loss: 0.22161664241872311\n",
      "Validation Iteration #50 loss: 0.24255606532096863\n",
      "Epoch #556 Train loss: 0.10084340054737895\n",
      "Epoch #556 Val   loss: 0.22158337280847737\n",
      "Validation Iteration #50 loss: 0.23976238071918488\n",
      "Epoch #557 Train loss: 0.101473738684466\n",
      "Epoch #557 Val   loss: 0.22155188258017328\n",
      "Validation Iteration #50 loss: 0.24125626683235168\n",
      "Epoch #558 Train loss: 0.10070440977027542\n",
      "Epoch #558 Val   loss: 0.22152110440081113\n",
      "Validation Iteration #50 loss: 0.2411855161190033\n",
      "Epoch #559 Train loss: 0.10002741825423743\n",
      "Epoch #559 Val   loss: 0.22148708423988506\n",
      "Validation Iteration #50 loss: 0.24082978069782257\n",
      "Epoch #560 Train loss: 0.10092615826349509\n",
      "Epoch #560 Val   loss: 0.2214543458350676\n",
      "Validation Iteration #50 loss: 0.24175198376178741\n",
      "Epoch #561 Train loss: 0.1002546795888951\n",
      "Epoch #561 Val   loss: 0.22142157960013106\n",
      "Validation Iteration #50 loss: 0.24272389709949493\n",
      "Epoch #562 Train loss: 0.09986960907515727\n",
      "Epoch #562 Val   loss: 0.2213894979391272\n",
      "Validation Iteration #50 loss: 0.24182769656181335\n",
      "Epoch #563 Train loss: 0.0992911854072621\n",
      "Epoch #563 Val   loss: 0.2213584447500082\n",
      "Validation Iteration #50 loss: 0.24387511610984802\n",
      "Epoch #564 Train loss: 0.09924965155752082\n",
      "Epoch #564 Val   loss: 0.22133006322152604\n",
      "Validation Iteration #50 loss: 0.24358192086219788\n",
      "Epoch #565 Train loss: 0.10043963358590477\n",
      "Epoch #565 Val   loss: 0.22130254590157322\n",
      "Validation Iteration #50 loss: 0.2422739416360855\n",
      "Epoch #566 Train loss: 0.09923867508769035\n",
      "Epoch #566 Val   loss: 0.22127189126364527\n",
      "Validation Iteration #50 loss: 0.24453650414943695\n",
      "Epoch #567 Train loss: 0.09963238847098853\n",
      "Epoch #567 Val   loss: 0.22124486248778788\n",
      "Validation Iteration #50 loss: 0.24449700117111206\n",
      "Epoch #568 Train loss: 0.09953615520345538\n",
      "Epoch #568 Val   loss: 0.22122402767184554\n",
      "Validation Iteration #50 loss: 0.24065235257148743\n",
      "Epoch #569 Train loss: 0.09860501869728691\n",
      "Epoch #569 Val   loss: 0.22119557585129854\n",
      "Validation Iteration #50 loss: 0.2410745918750763\n",
      "Epoch #570 Train loss: 0.0986463519695558\n",
      "Epoch #570 Val   loss: 0.22117158166023337\n",
      "Validation Iteration #50 loss: 0.241145059466362\n",
      "Epoch #571 Train loss: 0.09880248888542778\n",
      "Epoch #571 Val   loss: 0.22115018184588858\n",
      "Validation Iteration #50 loss: 0.24276921153068542\n",
      "Epoch #572 Train loss: 0.09890749756442874\n",
      "Epoch #572 Val   loss: 0.22112405516486278\n",
      "Validation Iteration #50 loss: 0.2438879758119583\n",
      "Epoch #573 Train loss: 0.09849290647789051\n",
      "Epoch #573 Val   loss: 0.2211006536482726\n",
      "Validation Iteration #50 loss: 0.24083386361598969\n",
      "Epoch #574 Train loss: 0.09708357357272976\n",
      "Epoch #574 Val   loss: 0.22107877087931968\n",
      "Validation Iteration #50 loss: 0.24300158023834229\n",
      "Epoch #575 Train loss: 0.09769566337528982\n",
      "Epoch #575 Val   loss: 0.2210577614754677\n",
      "Validation Iteration #50 loss: 0.24249418079853058\n",
      "Epoch #576 Train loss: 0.096868752452888\n",
      "Epoch #576 Val   loss: 0.2210337373802637\n",
      "Validation Iteration #50 loss: 0.24396318197250366\n",
      "Epoch #577 Train loss: 0.09612406241266351\n",
      "Epoch #577 Val   loss: 0.22101105455807907\n",
      "Validation Iteration #50 loss: 0.24723459780216217\n",
      "Epoch #578 Train loss: 0.09616108238697052\n",
      "Epoch #578 Val   loss: 0.22098916110878242\n",
      "Validation Iteration #50 loss: 0.2420058399438858\n",
      "Epoch #579 Train loss: 0.09708074852824211\n",
      "Epoch #579 Val   loss: 0.22096499577461093\n",
      "Validation Iteration #50 loss: 0.2470742166042328\n",
      "Epoch #580 Train loss: 0.09866770513747868\n",
      "Epoch #580 Val   loss: 0.2209413090451228\n",
      "Validation Iteration #50 loss: 0.24720990657806396\n",
      "Epoch #581 Train loss: 0.0959224957776697\n",
      "Epoch #581 Val   loss: 0.22091813160964746\n",
      "Validation Iteration #50 loss: 0.24104076623916626\n",
      "Epoch #582 Train loss: 0.09628903238396895\n",
      "Epoch #582 Val   loss: 0.22089335789528225\n",
      "Validation Iteration #50 loss: 0.24310478568077087\n",
      "Epoch #583 Train loss: 0.09633516147732735\n",
      "Epoch #583 Val   loss: 0.22087012503190148\n",
      "Validation Iteration #50 loss: 0.24070937931537628\n",
      "Epoch #584 Train loss: 0.09634057099097654\n",
      "Epoch #584 Val   loss: 0.22084809892433085\n",
      "Validation Iteration #50 loss: 0.244029238820076\n",
      "Epoch #585 Train loss: 0.09517270140349865\n",
      "Epoch #585 Val   loss: 0.22082792317563701\n",
      "Validation Iteration #50 loss: 0.24114815890789032\n",
      "Epoch #586 Train loss: 0.09527883815922235\n",
      "Epoch #586 Val   loss: 0.22080824863448273\n",
      "Validation Iteration #50 loss: 0.23887580633163452\n",
      "Epoch #587 Train loss: 0.09537693407190473\n",
      "Epoch #587 Val   loss: 0.22078772025667268\n",
      "Validation Iteration #50 loss: 0.24494512379169464\n",
      "Epoch #588 Train loss: 0.09458557339875322\n",
      "Epoch #588 Val   loss: 0.22076945729233569\n",
      "Validation Iteration #50 loss: 0.23847095668315887\n",
      "Epoch #589 Train loss: 0.09358640740576543\n",
      "Epoch #589 Val   loss: 0.2207490980819431\n",
      "Validation Iteration #50 loss: 0.23985052108764648\n",
      "Epoch #590 Train loss: 0.09404702131685458\n",
      "Epoch #590 Val   loss: 0.22072906606359965\n",
      "Validation Iteration #50 loss: 0.2395586520433426\n",
      "Epoch #591 Train loss: 0.09350796984998803\n",
      "Epoch #591 Val   loss: 0.2207083994243875\n",
      "Validation Iteration #50 loss: 0.24223387241363525\n",
      "Epoch #592 Train loss: 0.09343192745980464\n",
      "Epoch #592 Val   loss: 0.22068846994475302\n",
      "Validation Iteration #50 loss: 0.24338749051094055\n",
      "Epoch #593 Train loss: 0.09343975782394409\n",
      "Epoch #593 Val   loss: 0.22067429100801772\n",
      "Validation Iteration #50 loss: 0.24309410154819489\n",
      "Epoch #594 Train loss: 0.0931739013053869\n",
      "Epoch #594 Val   loss: 0.22065833373730923\n",
      "Validation Iteration #50 loss: 0.24178552627563477\n",
      "Epoch #595 Train loss: 0.09314552910233799\n",
      "Epoch #595 Val   loss: 0.220641051770549\n",
      "Validation Iteration #50 loss: 0.24475863575935364\n",
      "Epoch #596 Train loss: 0.09360411723977641\n",
      "Epoch #596 Val   loss: 0.22062676631018852\n",
      "Validation Iteration #50 loss: 0.23694176971912384\n",
      "Epoch #597 Train loss: 0.09274247818087276\n",
      "Epoch #597 Val   loss: 0.22060920976261447\n",
      "Validation Iteration #50 loss: 0.2460203319787979\n",
      "Epoch #598 Train loss: 0.09196104050466888\n",
      "Epoch #598 Val   loss: 0.22058950953211393\n",
      "Validation Iteration #50 loss: 0.23750340938568115\n",
      "Epoch #599 Train loss: 0.09187746361682289\n",
      "Epoch #599 Val   loss: 0.22057223527286296\n",
      "Validation Iteration #50 loss: 0.23729832470417023\n",
      "Epoch #600 Train loss: 0.09226892045453976\n",
      "Epoch #600 Val   loss: 0.2205548423586651\n",
      "Validation Iteration #50 loss: 0.23759767413139343\n",
      "Epoch #601 Train loss: 0.09246597831186495\n",
      "Epoch #601 Val   loss: 0.22053715352463107\n",
      "Validation Iteration #50 loss: 0.24416029453277588\n",
      "Epoch #602 Train loss: 0.09105549105688145\n",
      "Epoch #602 Val   loss: 0.22052198306480708\n",
      "Validation Iteration #50 loss: 0.23760758340358734\n",
      "Epoch #603 Train loss: 0.09179945917505968\n",
      "Epoch #603 Val   loss: 0.2205050557745517\n",
      "Validation Iteration #50 loss: 0.2467949092388153\n",
      "Epoch #604 Train loss: 0.09142418676301052\n",
      "Epoch #604 Val   loss: 0.22049013572707724\n",
      "Validation Iteration #50 loss: 0.24265450239181519\n",
      "Epoch #605 Train loss: 0.09110099724248837\n",
      "Epoch #605 Val   loss: 0.2204764763502852\n",
      "Validation Iteration #50 loss: 0.24319274723529816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #606 Train loss: 0.0908280582608361\n",
      "Epoch #606 Val   loss: 0.22046364260904236\n",
      "Validation Iteration #50 loss: 0.24294514954090118\n",
      "Epoch #607 Train loss: 0.09027329136274363\n",
      "Epoch #607 Val   loss: 0.22044909136513105\n",
      "Validation Iteration #50 loss: 0.2422441989183426\n",
      "Epoch #608 Train loss: 0.09048298903201756\n",
      "Epoch #608 Val   loss: 0.22043310432169375\n",
      "Validation Iteration #50 loss: 0.24865739047527313\n",
      "Epoch #609 Train loss: 0.09069668923161532\n",
      "Epoch #609 Val   loss: 0.22041766759839665\n",
      "Validation Iteration #50 loss: 0.2428596317768097\n",
      "Epoch #610 Train loss: 0.09022321650072147\n",
      "Epoch #610 Val   loss: 0.2204018397969382\n",
      "Validation Iteration #50 loss: 0.25202620029449463\n",
      "Epoch #611 Train loss: 0.08965040841384937\n",
      "Epoch #611 Val   loss: 0.22038815158352354\n",
      "Validation Iteration #50 loss: 0.24409861862659454\n",
      "Epoch #612 Train loss: 0.08982436084433605\n",
      "Epoch #612 Val   loss: 0.22037269734524023\n",
      "Validation Iteration #50 loss: 0.24497520923614502\n",
      "Epoch #613 Train loss: 0.08946286975161026\n",
      "Epoch #613 Val   loss: 0.22035606884435135\n",
      "Validation Iteration #50 loss: 0.24614964425563812\n",
      "Epoch #614 Train loss: 0.08950111887564785\n",
      "Epoch #614 Val   loss: 0.22033963103343576\n",
      "Validation Iteration #50 loss: 0.2449687272310257\n",
      "Epoch #615 Train loss: 0.08924032139934991\n",
      "Epoch #615 Val   loss: 0.22032328731902354\n",
      "Validation Iteration #50 loss: 0.24969035387039185\n",
      "Epoch #616 Train loss: 0.08958691240925538\n",
      "Epoch #616 Val   loss: 0.22030843921974552\n",
      "Validation Iteration #50 loss: 0.24612723290920258\n",
      "Epoch #617 Train loss: 0.08894255678904683\n",
      "Epoch #617 Val   loss: 0.2202932216136171\n",
      "Validation Iteration #50 loss: 0.24988824129104614\n",
      "Epoch #618 Train loss: 0.08850327536071602\n",
      "Epoch #618 Val   loss: 0.22027896974076985\n",
      "Validation Iteration #50 loss: 0.24714744091033936\n",
      "Epoch #619 Train loss: 0.08896718036971595\n",
      "Epoch #619 Val   loss: 0.22026314125685184\n",
      "Validation Iteration #50 loss: 0.24634860455989838\n",
      "Epoch #620 Train loss: 0.08822810502820894\n",
      "Epoch #620 Val   loss: 0.22025134494634538\n",
      "Validation Iteration #50 loss: 0.2475924789905548\n",
      "Epoch #621 Train loss: 0.08736761502529446\n",
      "Epoch #621 Val   loss: 0.22024004858317955\n",
      "Validation Iteration #50 loss: 0.2538317143917084\n",
      "Epoch #622 Train loss: 0.08805001752549096\n",
      "Epoch #622 Val   loss: 0.22022452420041155\n",
      "Validation Iteration #50 loss: 0.24763232469558716\n",
      "Epoch #623 Train loss: 0.08695313442302377\n",
      "Epoch #623 Val   loss: 0.22021430754332383\n",
      "Validation Iteration #50 loss: 0.24960577487945557\n",
      "Epoch #624 Train loss: 0.08825076430251724\n",
      "Epoch #624 Val   loss: 0.2202024476821606\n",
      "Validation Iteration #50 loss: 0.23895590007305145\n",
      "Epoch #625 Train loss: 0.08710865833257374\n",
      "Epoch #625 Val   loss: 0.22019102458440867\n",
      "Validation Iteration #50 loss: 0.24860741198062897\n",
      "Epoch #626 Train loss: 0.0868858584251843\n",
      "Epoch #626 Val   loss: 0.2201808426200561\n",
      "Validation Iteration #50 loss: 0.2413884401321411\n",
      "Epoch #627 Train loss: 0.08727065287530422\n",
      "Epoch #627 Val   loss: 0.2201692091884244\n",
      "Validation Iteration #50 loss: 0.24613599479198456\n",
      "Epoch #628 Train loss: 0.08750796582745879\n",
      "Epoch #628 Val   loss: 0.22016002202443655\n",
      "Validation Iteration #50 loss: 0.24220679700374603\n",
      "Epoch #629 Train loss: 0.08636699979634661\n",
      "Epoch #629 Val   loss: 0.22014761636618146\n",
      "Validation Iteration #50 loss: 0.24677547812461853\n",
      "Epoch #630 Train loss: 0.08658320476350032\n",
      "Epoch #630 Val   loss: 0.2201372832622991\n",
      "Validation Iteration #50 loss: 0.2475573718547821\n",
      "Epoch #631 Train loss: 0.08666746926150824\n",
      "Epoch #631 Val   loss: 0.2201249820562016\n",
      "Validation Iteration #50 loss: 0.24737896025180817\n",
      "Epoch #632 Train loss: 0.08529047952278664\n",
      "Epoch #632 Val   loss: 0.22011219159534626\n",
      "Validation Iteration #50 loss: 0.2487146556377411\n",
      "Epoch #633 Train loss: 0.08546196296811104\n",
      "Epoch #633 Val   loss: 0.22010221828775942\n",
      "Validation Iteration #50 loss: 0.25298914313316345\n",
      "Epoch #634 Train loss: 0.08568152687267254\n",
      "Epoch #634 Val   loss: 0.220091278115451\n",
      "Validation Iteration #50 loss: 0.23789925873279572\n",
      "Epoch #635 Train loss: 0.08595685405950797\n",
      "Epoch #635 Val   loss: 0.220080864502646\n",
      "Validation Iteration #50 loss: 0.24588103592395782\n",
      "Epoch #636 Train loss: 0.08494755939433449\n",
      "Epoch #636 Val   loss: 0.22006928839951\n",
      "Validation Iteration #50 loss: 0.24381482601165771\n",
      "Epoch #637 Train loss: 0.0848025880557926\n",
      "Epoch #637 Val   loss: 0.22006028510265416\n",
      "Validation Iteration #50 loss: 0.2494971603155136\n",
      "Epoch #638 Train loss: 0.08459305861278583\n",
      "Epoch #638 Val   loss: 0.22005147234805442\n",
      "Validation Iteration #50 loss: 0.25101104378700256\n",
      "Epoch #639 Train loss: 0.0845943416811918\n",
      "Epoch #639 Val   loss: 0.22004332681222316\n",
      "Validation Iteration #50 loss: 0.25077033042907715\n",
      "Epoch #640 Train loss: 0.08399475552141666\n",
      "Epoch #640 Val   loss: 0.2200344814646792\n",
      "Validation Iteration #50 loss: 0.24279074370861053\n",
      "Epoch #641 Train loss: 0.08385938111888736\n",
      "Epoch #641 Val   loss: 0.2200266700813789\n",
      "Validation Iteration #50 loss: 0.24586743116378784\n",
      "Epoch #642 Train loss: 0.08335449095619352\n",
      "Epoch #642 Val   loss: 0.22001807167921103\n",
      "Validation Iteration #50 loss: 0.2440270334482193\n",
      "Epoch #643 Train loss: 0.08353746172628905\n",
      "Epoch #643 Val   loss: 0.22000837464168546\n",
      "Validation Iteration #50 loss: 0.246491938829422\n",
      "Epoch #644 Train loss: 0.08388797429047133\n",
      "Epoch #644 Val   loss: 0.22000141701992595\n",
      "Validation Iteration #50 loss: 0.25270164012908936\n",
      "Epoch #645 Train loss: 0.08341920964027706\n",
      "Epoch #645 Val   loss: 0.21999486708433805\n",
      "Validation Iteration #50 loss: 0.25280866026878357\n",
      "Epoch #646 Train loss: 0.0839367871613879\n",
      "Epoch #646 Val   loss: 0.21998784633960355\n",
      "Validation Iteration #50 loss: 0.2503735423088074\n",
      "Epoch #647 Train loss: 0.08323437671520208\n",
      "Epoch #647 Val   loss: 0.21998075872274064\n",
      "Validation Iteration #50 loss: 0.2534472644329071\n",
      "Epoch #648 Train loss: 0.08297620998009254\n",
      "Epoch #648 Val   loss: 0.2199737030950516\n",
      "Validation Iteration #50 loss: 0.25159117579460144\n",
      "Epoch #649 Train loss: 0.08204750157892704\n",
      "Epoch #649 Val   loss: 0.2199660821788057\n",
      "Validation Iteration #50 loss: 0.2530216574668884\n",
      "Epoch #650 Train loss: 0.0829475028930526\n",
      "Epoch #650 Val   loss: 0.2199593332035877\n",
      "Validation Iteration #50 loss: 0.24646534025669098\n",
      "Epoch #651 Train loss: 0.08298594592825363\n",
      "Epoch #651 Val   loss: 0.21995020494192008\n",
      "Validation Iteration #50 loss: 0.24899595975875854\n",
      "Epoch #652 Train loss: 0.08298315441137866\n",
      "Epoch #652 Val   loss: 0.2199435519709058\n",
      "Validation Iteration #50 loss: 0.24384565651416779\n",
      "Epoch #653 Train loss: 0.08329145894630959\n",
      "Epoch #653 Val   loss: 0.21993513864258463\n",
      "Validation Iteration #50 loss: 0.24926230311393738\n",
      "Epoch #654 Train loss: 0.08285811376806937\n",
      "Epoch #654 Val   loss: 0.21992795164366014\n",
      "Validation Iteration #50 loss: 0.24481675028800964\n",
      "Epoch #655 Train loss: 0.08197876399284915\n",
      "Epoch #655 Val   loss: 0.21992134173989547\n",
      "Validation Iteration #50 loss: 0.2424253225326538\n",
      "Epoch #656 Train loss: 0.08172311653432093\n",
      "Epoch #656 Val   loss: 0.2199154600776717\n",
      "Validation Iteration #50 loss: 0.24219173192977905\n",
      "Epoch #657 Train loss: 0.08218389112306268\n",
      "Epoch #657 Val   loss: 0.21990846148945753\n",
      "Validation Iteration #50 loss: 0.24517905712127686\n",
      "Epoch #658 Train loss: 0.08186708567173857\n",
      "Epoch #658 Val   loss: 0.2199010331772013\n",
      "Validation Iteration #50 loss: 0.24450138211250305\n",
      "Epoch #659 Train loss: 0.08095840531352319\n",
      "Epoch #659 Val   loss: 0.21989637547734872\n",
      "Validation Iteration #50 loss: 0.24431248009204865\n",
      "Epoch #660 Train loss: 0.081476525844712\n",
      "Epoch #660 Val   loss: 0.21989149630787652\n",
      "Validation Iteration #50 loss: 0.243434876203537\n",
      "Epoch #661 Train loss: 0.08120289660598103\n",
      "Epoch #661 Val   loss: 0.21988622185302534\n",
      "Validation Iteration #50 loss: 0.24831674993038177\n",
      "Epoch #662 Train loss: 0.08104208699966732\n",
      "Epoch #662 Val   loss: 0.21988272844227136\n",
      "Validation Iteration #50 loss: 0.24922825396060944\n",
      "Epoch #663 Train loss: 0.08147468398276128\n",
      "Epoch #663 Val   loss: 0.21988121490145277\n",
      "Validation Iteration #50 loss: 0.24919524788856506\n",
      "Epoch #664 Train loss: 0.08094309770355099\n",
      "Epoch #664 Val   loss: 0.21987766238342069\n",
      "Validation Iteration #50 loss: 0.2516010105609894\n",
      "Epoch #665 Train loss: 0.08014672720118572\n",
      "Epoch #665 Val   loss: 0.21987324516353693\n",
      "Validation Iteration #50 loss: 0.24096037447452545\n",
      "Epoch #666 Train loss: 0.08039871289541847\n",
      "Epoch #666 Val   loss: 0.21987064587552302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.24063453078269958\n",
      "Epoch #667 Train loss: 0.07978935971071846\n",
      "Epoch #667 Val   loss: 0.21986848848366974\n",
      "Validation Iteration #50 loss: 0.24132175743579865\n",
      "Epoch #668 Train loss: 0.07994497371347327\n",
      "Epoch #668 Val   loss: 0.21986469549410806\n",
      "Validation Iteration #50 loss: 0.24304385483264923\n",
      "Epoch #669 Train loss: 0.07989739028638915\n",
      "Epoch #669 Val   loss: 0.2198643748645257\n",
      "Validation Iteration #50 loss: 0.23906749486923218\n",
      "Epoch #670 Train loss: 0.07947243740291972\n",
      "Epoch #670 Val   loss: 0.2198603001535178\n",
      "Validation Iteration #50 loss: 0.24660907685756683\n",
      "Epoch #671 Train loss: 0.08014473907257381\n",
      "Epoch #671 Val   loss: 0.21986120254323296\n",
      "Validation Iteration #50 loss: 0.23942333459854126\n",
      "Epoch #672 Train loss: 0.07903827726840973\n",
      "Epoch #672 Val   loss: 0.21986056507614793\n",
      "Validation Iteration #50 loss: 0.2496080994606018\n",
      "Epoch #673 Train loss: 0.07954314831448228\n",
      "Epoch #673 Val   loss: 0.21986253880758558\n",
      "Validation Iteration #50 loss: 0.24004405736923218\n",
      "Epoch #674 Train loss: 0.07904462751589324\n",
      "Epoch #674 Val   loss: 0.21985858478692183\n",
      "Validation Iteration #50 loss: 0.23954105377197266\n",
      "Epoch #675 Train loss: 0.07868473584714689\n",
      "Epoch #675 Val   loss: 0.21985836585828483\n",
      "Validation Iteration #50 loss: 0.24020768702030182\n",
      "Epoch #676 Train loss: 0.07808959915449745\n",
      "Epoch #676 Val   loss: 0.21985588086699778\n",
      "Validation Iteration #50 loss: 0.24465569853782654\n",
      "Epoch #677 Train loss: 0.0781358776516036\n",
      "Epoch #677 Val   loss: 0.2198537545972247\n",
      "Validation Iteration #50 loss: 0.2403135895729065\n",
      "Epoch #678 Train loss: 0.07798648909910728\n",
      "Epoch #678 Val   loss: 0.2198513729734762\n",
      "Validation Iteration #50 loss: 0.24107523262500763\n",
      "Epoch #679 Train loss: 0.0783929355246456\n",
      "Epoch #679 Val   loss: 0.2198471294050168\n",
      "Validation Iteration #50 loss: 0.23165962100028992\n",
      "Epoch #680 Train loss: 0.07891431450843811\n",
      "Epoch #680 Val   loss: 0.21984292231655841\n",
      "Validation Iteration #50 loss: 0.23606476187705994\n",
      "Epoch #681 Train loss: 0.07796039696978896\n",
      "Epoch #681 Val   loss: 0.2198403256706021\n",
      "Validation Iteration #50 loss: 0.23387257754802704\n",
      "Epoch #682 Train loss: 0.07837312992073987\n",
      "Epoch #682 Val   loss: 0.21983701321291674\n",
      "Validation Iteration #50 loss: 0.2317611575126648\n",
      "Epoch #683 Train loss: 0.07725675423678599\n",
      "Epoch #683 Val   loss: 0.21983467270102766\n",
      "Validation Iteration #50 loss: 0.23299607634544373\n",
      "Epoch #684 Train loss: 0.0776422149256656\n",
      "Epoch #684 Val   loss: 0.21983464999037602\n",
      "Validation Iteration #50 loss: 0.23677723109722137\n",
      "Epoch #685 Train loss: 0.0767595966003443\n",
      "Epoch #685 Val   loss: 0.21983070268618818\n",
      "Validation Iteration #50 loss: 0.23109020292758942\n",
      "Epoch #686 Train loss: 0.07670178117328569\n",
      "Epoch #686 Val   loss: 0.21983035000204013\n",
      "Validation Iteration #50 loss: 0.23658816516399384\n",
      "Epoch #687 Train loss: 0.0772878127662759\n",
      "Epoch #687 Val   loss: 0.2198318488952539\n",
      "Validation Iteration #50 loss: 0.23546481132507324\n",
      "Epoch #688 Train loss: 0.07683343293243333\n",
      "Epoch #688 Val   loss: 0.2198326751345221\n",
      "Validation Iteration #50 loss: 0.2384302020072937\n",
      "Epoch #689 Train loss: 0.07610354209808927\n",
      "Epoch #689 Val   loss: 0.21983460031923244\n",
      "Validation Iteration #50 loss: 0.2342177778482437\n",
      "Epoch #690 Train loss: 0.07652447764810763\n",
      "Epoch #690 Val   loss: 0.21983619716562117\n",
      "Validation Iteration #50 loss: 0.2371017187833786\n",
      "Epoch #691 Train loss: 0.07636982771126848\n",
      "Epoch #691 Val   loss: 0.21983574350849425\n",
      "Validation Iteration #50 loss: 0.23314009606838226\n",
      "Epoch #692 Train loss: 0.07597307762817333\n",
      "Epoch #692 Val   loss: 0.21983788950780822\n",
      "Validation Iteration #50 loss: 0.2371009886264801\n",
      "Epoch #693 Train loss: 0.07584450570376296\n",
      "Epoch #693 Val   loss: 0.219840299199364\n",
      "Validation Iteration #50 loss: 0.23715314269065857\n",
      "Epoch #694 Train loss: 0.07612278018342822\n",
      "Epoch #694 Val   loss: 0.2198415737531102\n",
      "Validation Iteration #50 loss: 0.2397848665714264\n",
      "Epoch #695 Train loss: 0.07613616132814634\n",
      "Epoch #695 Val   loss: 0.21984375777689757\n",
      "Validation Iteration #50 loss: 0.24127458035945892\n",
      "Epoch #696 Train loss: 0.07627019109694581\n",
      "Epoch #696 Val   loss: 0.21984455882993154\n",
      "Validation Iteration #50 loss: 0.23750898241996765\n",
      "Epoch #697 Train loss: 0.07652163299682893\n",
      "Epoch #697 Val   loss: 0.21984511102059745\n",
      "Validation Iteration #50 loss: 0.23944753408432007\n",
      "Epoch #698 Train loss: 0.07522668748309738\n",
      "Epoch #698 Val   loss: 0.21984465675240134\n",
      "Validation Iteration #50 loss: 0.24113626778125763\n",
      "Epoch #699 Train loss: 0.07492327160741154\n",
      "Epoch #699 Val   loss: 0.2198469969351868\n",
      "Validation Iteration #50 loss: 0.24093326926231384\n",
      "Epoch #700 Train loss: 0.07476057671010494\n",
      "Epoch #700 Val   loss: 0.2198484733996715\n",
      "Validation Iteration #50 loss: 0.23798111081123352\n",
      "Epoch #701 Train loss: 0.07468020259157608\n",
      "Epoch #701 Val   loss: 0.21984917264649698\n",
      "Validation Iteration #50 loss: 0.24567310512065887\n",
      "Epoch #702 Train loss: 0.07429884462372253\n",
      "Epoch #702 Val   loss: 0.2198508939940072\n",
      "Validation Iteration #50 loss: 0.2439720779657364\n",
      "Epoch #703 Train loss: 0.07400700969523505\n",
      "Epoch #703 Val   loss: 0.21985306768349236\n",
      "Validation Iteration #50 loss: 0.24563480913639069\n",
      "Epoch #704 Train loss: 0.0747815758774155\n",
      "Epoch #704 Val   loss: 0.2198530295907443\n",
      "Validation Iteration #50 loss: 0.24419693648815155\n",
      "Epoch #705 Train loss: 0.07394973316082828\n",
      "Epoch #705 Val   loss: 0.2198545203427911\n",
      "Validation Iteration #50 loss: 0.24092164635658264\n",
      "Epoch #706 Train loss: 0.07451411532728296\n",
      "Epoch #706 Val   loss: 0.21985644781736072\n",
      "Validation Iteration #50 loss: 0.2461509257555008\n",
      "Epoch #707 Train loss: 0.07389643543252819\n",
      "Epoch #707 Val   loss: 0.21985877533546483\n",
      "Validation Iteration #50 loss: 0.24633385241031647\n",
      "Epoch #708 Train loss: 0.07381014182771507\n",
      "Epoch #708 Val   loss: 0.21986399631386686\n",
      "Validation Iteration #50 loss: 0.24356421828269958\n",
      "Epoch #709 Train loss: 0.07415402955130528\n",
      "Epoch #709 Val   loss: 0.21986868260902548\n",
      "Validation Iteration #50 loss: 0.24192938208580017\n",
      "Epoch #710 Train loss: 0.07260372411263616\n",
      "Epoch #710 Val   loss: 0.21987374796278483\n",
      "Validation Iteration #50 loss: 0.24591270089149475\n",
      "Epoch #711 Train loss: 0.07269380408290185\n",
      "Epoch #711 Val   loss: 0.21987718377059506\n",
      "Validation Iteration #50 loss: 0.2426242083311081\n",
      "Epoch #712 Train loss: 0.07305320352315903\n",
      "Epoch #712 Val   loss: 0.21988306972397637\n",
      "Validation Iteration #50 loss: 0.24953384697437286\n",
      "Epoch #713 Train loss: 0.07255975882473745\n",
      "Epoch #713 Val   loss: 0.21989030519569006\n",
      "Validation Iteration #50 loss: 0.2490694671869278\n",
      "Epoch #714 Train loss: 0.07341569093497176\n",
      "Epoch #714 Val   loss: 0.21989363414135718\n",
      "Validation Iteration #50 loss: 0.2522776126861572\n",
      "Epoch #715 Train loss: 0.07275226133826532\n",
      "Epoch #715 Val   loss: 0.21989917258368216\n",
      "Validation Iteration #50 loss: 0.24912260472774506\n",
      "Epoch #716 Train loss: 0.07351010645690717\n",
      "Epoch #716 Val   loss: 0.21990463031475257\n",
      "Validation Iteration #50 loss: 0.2496080845594406\n",
      "Epoch #717 Train loss: 0.07217073558192504\n",
      "Epoch #717 Val   loss: 0.21991068628447608\n",
      "Validation Iteration #50 loss: 0.25015515089035034\n",
      "Epoch #718 Train loss: 0.0714054395885844\n",
      "Epoch #718 Val   loss: 0.2199156816498384\n",
      "Validation Iteration #50 loss: 0.25112995505332947\n",
      "Epoch #719 Train loss: 0.07196675417454619\n",
      "Epoch #719 Val   loss: 0.21992025070019766\n",
      "Validation Iteration #50 loss: 0.2495787888765335\n",
      "Epoch #720 Train loss: 0.07188225880657372\n",
      "Epoch #720 Val   loss: 0.21992391559487026\n",
      "Validation Iteration #50 loss: 0.2518717050552368\n",
      "Epoch #721 Train loss: 0.07156892424743426\n",
      "Epoch #721 Val   loss: 0.2199289391058202\n",
      "Validation Iteration #50 loss: 0.2506556212902069\n",
      "Epoch #722 Train loss: 0.07154349414141555\n",
      "Epoch #722 Val   loss: 0.21993412020194023\n",
      "Validation Iteration #50 loss: 0.2508678138256073\n",
      "Epoch #723 Train loss: 0.07107765707922609\n",
      "Epoch #723 Val   loss: 0.21994118363336457\n",
      "Validation Iteration #50 loss: 0.25055551528930664\n",
      "Epoch #724 Train loss: 0.07096705566111364\n",
      "Epoch #724 Val   loss: 0.21994761312039524\n",
      "Validation Iteration #50 loss: 0.25204798579216003\n",
      "Epoch #725 Train loss: 0.07105026462752569\n",
      "Epoch #725 Val   loss: 0.21995406075412122\n",
      "Validation Iteration #50 loss: 0.25531676411628723\n",
      "Epoch #726 Train loss: 0.07007129902118131\n",
      "Epoch #726 Val   loss: 0.21995981825323865\n",
      "Validation Iteration #50 loss: 0.25304341316223145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #727 Train loss: 0.07091176205951917\n",
      "Epoch #727 Val   loss: 0.21996846839798173\n",
      "Validation Iteration #50 loss: 0.2518494427204132\n",
      "Epoch #728 Train loss: 0.06980173750535439\n",
      "Epoch #728 Val   loss: 0.21997672300229562\n",
      "Validation Iteration #50 loss: 0.24213995039463043\n",
      "Epoch #729 Train loss: 0.07013677276278797\n",
      "Epoch #729 Val   loss: 0.2199838005532329\n",
      "Validation Iteration #50 loss: 0.24882248044013977\n",
      "Epoch #730 Train loss: 0.06936330161988735\n",
      "Epoch #730 Val   loss: 0.21999458487875823\n",
      "Validation Iteration #50 loss: 0.24267204105854034\n",
      "Epoch #731 Train loss: 0.06978318281471729\n",
      "Epoch #731 Val   loss: 0.22000110472743128\n",
      "Validation Iteration #50 loss: 0.23959258198738098\n",
      "Epoch #732 Train loss: 0.06956664541442144\n",
      "Epoch #732 Val   loss: 0.22000945487619383\n",
      "Validation Iteration #50 loss: 0.24109964072704315\n",
      "Epoch #733 Train loss: 0.07005111784919311\n",
      "Epoch #733 Val   loss: 0.2200161963732481\n",
      "Validation Iteration #50 loss: 0.24240468442440033\n",
      "Epoch #734 Train loss: 0.06970864377523724\n",
      "Epoch #734 Val   loss: 0.22002444665572957\n",
      "Validation Iteration #50 loss: 0.24144455790519714\n",
      "Epoch #735 Train loss: 0.06903625514946486\n",
      "Epoch #735 Val   loss: 0.22003027242935422\n",
      "Validation Iteration #50 loss: 0.23974815011024475\n",
      "Epoch #736 Train loss: 0.06954589544942505\n",
      "Epoch #736 Val   loss: 0.22003804541196545\n",
      "Validation Iteration #50 loss: 0.241633340716362\n",
      "Epoch #737 Train loss: 0.0694511647483236\n",
      "Epoch #737 Val   loss: 0.220047204655261\n",
      "Validation Iteration #50 loss: 0.24097572267055511\n",
      "Epoch #738 Train loss: 0.06818026304244995\n",
      "Epoch #738 Val   loss: 0.22005663691021668\n",
      "Validation Iteration #50 loss: 0.24079343676567078\n",
      "Epoch #739 Train loss: 0.06801977292879631\n",
      "Epoch #739 Val   loss: 0.22006502225065677\n",
      "Validation Iteration #50 loss: 0.23994037508964539\n",
      "Epoch #740 Train loss: 0.06876714390359427\n",
      "Epoch #740 Val   loss: 0.22007357548092008\n",
      "Validation Iteration #50 loss: 0.23956817388534546\n",
      "Epoch #741 Train loss: 0.06880711754293818\n",
      "Epoch #741 Val   loss: 0.2200798137604313\n",
      "Validation Iteration #50 loss: 0.2364387959241867\n",
      "Epoch #742 Train loss: 0.06816022362756102\n",
      "Epoch #742 Val   loss: 0.2200886563788735\n",
      "Validation Iteration #50 loss: 0.24175819754600525\n",
      "Epoch #743 Train loss: 0.06825829728653557\n",
      "Epoch #743 Val   loss: 0.2201011064296744\n",
      "Validation Iteration #50 loss: 0.24343642592430115\n",
      "Epoch #744 Train loss: 0.06808558743643134\n",
      "Epoch #744 Val   loss: 0.22011370619332968\n",
      "Validation Iteration #50 loss: 0.24165485799312592\n",
      "Epoch #745 Train loss: 0.06773921337566878\n",
      "Epoch #745 Val   loss: 0.22012140259997923\n",
      "Validation Iteration #50 loss: 0.24554534256458282\n",
      "Epoch #746 Train loss: 0.06784700435635291\n",
      "Epoch #746 Val   loss: 0.22013458486210244\n",
      "Validation Iteration #50 loss: 0.23558999598026276\n",
      "Epoch #747 Train loss: 0.06801770353003551\n",
      "Epoch #747 Val   loss: 0.22014793501848892\n",
      "Validation Iteration #50 loss: 0.2410917729139328\n",
      "Epoch #748 Train loss: 0.06819331361667107\n",
      "Epoch #748 Val   loss: 0.22015882975250542\n",
      "Validation Iteration #50 loss: 0.23645032942295074\n",
      "Epoch #749 Train loss: 0.0670657388277744\n",
      "Epoch #749 Val   loss: 0.2201723439953266\n",
      "Validation Iteration #50 loss: 0.24074646830558777\n",
      "Epoch #750 Train loss: 0.06702768508540957\n",
      "Epoch #750 Val   loss: 0.22018217398051196\n",
      "Validation Iteration #50 loss: 0.24206939339637756\n",
      "Epoch #751 Train loss: 0.06669700165328227\n",
      "Epoch #751 Val   loss: 0.22019240947565255\n",
      "Validation Iteration #50 loss: 0.23728768527507782\n",
      "Epoch #752 Train loss: 0.06816276545195203\n",
      "Epoch #752 Val   loss: 0.22020094894920797\n",
      "Validation Iteration #50 loss: 0.23772302269935608\n",
      "Epoch #753 Train loss: 0.0669663087709954\n",
      "Epoch #753 Val   loss: 0.2202111342851543\n",
      "Validation Iteration #50 loss: 0.23729249835014343\n",
      "Epoch #754 Train loss: 0.06654785053902551\n",
      "Epoch #754 Val   loss: 0.22022078587003224\n",
      "Validation Iteration #50 loss: 0.24492761492729187\n",
      "Epoch #755 Train loss: 0.0669903646370298\n",
      "Epoch #755 Val   loss: 0.2202322033814264\n",
      "Validation Iteration #50 loss: 0.245444655418396\n",
      "Epoch #756 Train loss: 0.06639378782557814\n",
      "Epoch #756 Val   loss: 0.2202425157529602\n",
      "Validation Iteration #50 loss: 0.2389887273311615\n",
      "Epoch #757 Train loss: 0.06665205710420483\n",
      "Epoch #757 Val   loss: 0.2202546846250842\n",
      "Validation Iteration #50 loss: 0.23858579993247986\n",
      "Epoch #758 Train loss: 0.06621104400408895\n",
      "Epoch #758 Val   loss: 0.22026794199083574\n",
      "Validation Iteration #50 loss: 0.23404191434383392\n",
      "Epoch #759 Train loss: 0.06541714640824418\n",
      "Epoch #759 Val   loss: 0.22028021275966997\n",
      "Validation Iteration #50 loss: 0.23859825730323792\n",
      "Epoch #760 Train loss: 0.06634255243759406\n",
      "Epoch #760 Val   loss: 0.2202921826142001\n",
      "Validation Iteration #50 loss: 0.2404824197292328\n",
      "Epoch #761 Train loss: 0.06531583223687976\n",
      "Epoch #761 Val   loss: 0.2203036250413824\n",
      "Validation Iteration #50 loss: 0.24126289784908295\n",
      "Epoch #762 Train loss: 0.06561327568794552\n",
      "Epoch #762 Val   loss: 0.22031406424124644\n",
      "Validation Iteration #50 loss: 0.23942899703979492\n",
      "Epoch #763 Train loss: 0.06482008756383469\n",
      "Epoch #763 Val   loss: 0.2203266217470457\n",
      "Validation Iteration #50 loss: 0.23865476250648499\n",
      "Epoch #764 Train loss: 0.06608260119039762\n",
      "Epoch #764 Val   loss: 0.2203385435647854\n",
      "Validation Iteration #50 loss: 0.2369704395532608\n",
      "Epoch #765 Train loss: 0.06499445840324226\n",
      "Epoch #765 Val   loss: 0.22035089596211133\n",
      "Validation Iteration #50 loss: 0.23450465500354767\n",
      "Epoch #766 Train loss: 0.06514684218717248\n",
      "Epoch #766 Val   loss: 0.22036325057295808\n",
      "Validation Iteration #50 loss: 0.23538851737976074\n",
      "Epoch #767 Train loss: 0.0645321341917703\n",
      "Epoch #767 Val   loss: 0.2203763376813955\n",
      "Validation Iteration #50 loss: 0.24066495895385742\n",
      "Epoch #768 Train loss: 0.06415918116506777\n",
      "Epoch #768 Val   loss: 0.22038924751447966\n",
      "Validation Iteration #50 loss: 0.2443431317806244\n",
      "Epoch #769 Train loss: 0.06462686822602623\n",
      "Epoch #769 Val   loss: 0.2204030341856725\n",
      "Validation Iteration #50 loss: 0.23580795526504517\n",
      "Epoch #770 Train loss: 0.0638964120298624\n",
      "Epoch #770 Val   loss: 0.22041505506345996\n",
      "Validation Iteration #50 loss: 0.23954764008522034\n",
      "Epoch #771 Train loss: 0.06505021943073523\n",
      "Epoch #771 Val   loss: 0.22042981860212663\n",
      "Validation Iteration #50 loss: 0.23944289982318878\n",
      "Epoch #772 Train loss: 0.0648958571255207\n",
      "Epoch #772 Val   loss: 0.22044332820285623\n",
      "Validation Iteration #50 loss: 0.23912978172302246\n",
      "Epoch #773 Train loss: 0.06467408275133685\n",
      "Epoch #773 Val   loss: 0.22045522382365523\n",
      "Validation Iteration #50 loss: 0.23714543879032135\n",
      "Epoch #774 Train loss: 0.0635135991400794\n",
      "Epoch #774 Val   loss: 0.22046454504317148\n",
      "Validation Iteration #50 loss: 0.23676081001758575\n",
      "Epoch #775 Train loss: 0.06442719090141748\n",
      "Epoch #775 Val   loss: 0.22047779227560424\n",
      "Validation Iteration #50 loss: 0.24540027976036072\n",
      "Epoch #776 Train loss: 0.06425069762687934\n",
      "Epoch #776 Val   loss: 0.2204926546263631\n",
      "Validation Iteration #50 loss: 0.2424318641424179\n",
      "Epoch #777 Train loss: 0.06455043644497269\n",
      "Epoch #777 Val   loss: 0.22050745796580926\n",
      "Validation Iteration #50 loss: 0.2440442144870758\n",
      "Epoch #778 Train loss: 0.06381109533341307\n",
      "Epoch #778 Val   loss: 0.2205198558745055\n",
      "Validation Iteration #50 loss: 0.24384410679340363\n",
      "Epoch #779 Train loss: 0.06290421478058163\n",
      "Epoch #779 Val   loss: 0.22053342243799798\n",
      "Validation Iteration #50 loss: 0.24294127523899078\n",
      "Epoch #780 Train loss: 0.06392002733130205\n",
      "Epoch #780 Val   loss: 0.22054702332368992\n",
      "Validation Iteration #50 loss: 0.24444377422332764\n",
      "Epoch #781 Train loss: 0.06350388505349033\n",
      "Epoch #781 Val   loss: 0.2205609499326424\n",
      "Validation Iteration #50 loss: 0.2489563524723053\n",
      "Epoch #782 Train loss: 0.06284705755349837\n",
      "Epoch #782 Val   loss: 0.22057569878943858\n",
      "Validation Iteration #50 loss: 0.2512604296207428\n",
      "Epoch #783 Train loss: 0.06291161075626549\n",
      "Epoch #783 Val   loss: 0.22059423576169054\n",
      "Validation Iteration #50 loss: 0.24951530992984772\n",
      "Epoch #784 Train loss: 0.06359417040489222\n",
      "Epoch #784 Val   loss: 0.22061160000616986\n",
      "Validation Iteration #50 loss: 0.24853204190731049\n",
      "Epoch #785 Train loss: 0.06311670288835701\n",
      "Epoch #785 Val   loss: 0.22062796905832202\n",
      "Validation Iteration #50 loss: 0.24933601915836334\n",
      "Epoch #786 Train loss: 0.06272642608535917\n",
      "Epoch #786 Val   loss: 0.22064442195776748\n",
      "Validation Iteration #50 loss: 0.2498743087053299\n",
      "Epoch #787 Train loss: 0.06327524173416589\n",
      "Epoch #787 Val   loss: 0.22066214698854483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Iteration #50 loss: 0.2508854269981384\n",
      "Epoch #788 Train loss: 0.06246305629611015\n",
      "Epoch #788 Val   loss: 0.22068086109036322\n",
      "Validation Iteration #50 loss: 0.24883180856704712\n",
      "Epoch #789 Train loss: 0.06273302680959827\n",
      "Epoch #789 Val   loss: 0.22069744114721412\n",
      "Validation Iteration #50 loss: 0.2496098428964615\n",
      "Epoch #790 Train loss: 0.06235314170388799\n",
      "Epoch #790 Val   loss: 0.22071516253999685\n",
      "Validation Iteration #50 loss: 0.2503246068954468\n",
      "Epoch #791 Train loss: 0.06276040112501696\n",
      "Epoch #791 Val   loss: 0.22073263657896577\n",
      "Validation Iteration #50 loss: 0.2492440789937973\n",
      "Epoch #792 Train loss: 0.06254476455873564\n",
      "Epoch #792 Val   loss: 0.22075127973503558\n",
      "Validation Iteration #50 loss: 0.24950535595417023\n",
      "Epoch #793 Train loss: 0.06269331060742077\n",
      "Epoch #793 Val   loss: 0.22076983454643906\n",
      "Validation Iteration #50 loss: 0.24908271431922913\n",
      "Epoch #794 Train loss: 0.06275060292529433\n",
      "Epoch #794 Val   loss: 0.22078819214490283\n",
      "Validation Iteration #50 loss: 0.24941329658031464\n",
      "Epoch #795 Train loss: 0.061764834938864956\n",
      "Epoch #795 Val   loss: 0.22080739569689345\n",
      "Validation Iteration #50 loss: 0.25445064902305603\n",
      "Epoch #796 Train loss: 0.06227295532038337\n",
      "Epoch #796 Val   loss: 0.22082575567008422\n",
      "Validation Iteration #50 loss: 0.2512977421283722\n",
      "Epoch #797 Train loss: 0.061893976342521216\n",
      "Epoch #797 Val   loss: 0.22084225569849003\n",
      "Validation Iteration #50 loss: 0.24886387586593628\n",
      "Epoch #798 Train loss: 0.06287699163352188\n",
      "Epoch #798 Val   loss: 0.2208573505321136\n",
      "Validation Iteration #50 loss: 0.25205275416374207\n",
      "Epoch #799 Train loss: 0.06224483584887103\n",
      "Epoch #799 Val   loss: 0.22087481959078176\n",
      "Validation Iteration #50 loss: 0.2527589499950409\n",
      "Epoch #800 Train loss: 0.06153275709795324\n",
      "Epoch #800 Val   loss: 0.22089431288834666\n",
      "Validation Iteration #50 loss: 0.25182339549064636\n",
      "Epoch #801 Train loss: 0.06217696851021365\n",
      "Epoch #801 Val   loss: 0.22091071754007954\n",
      "Validation Iteration #50 loss: 0.2515159845352173\n",
      "Epoch #802 Train loss: 0.06259925673274618\n",
      "Epoch #802 Val   loss: 0.22092793299921376\n",
      "Validation Iteration #50 loss: 0.2515960931777954\n",
      "Epoch #803 Train loss: 0.06150310437538122\n",
      "Epoch #803 Val   loss: 0.22094507914762182\n",
      "Validation Iteration #50 loss: 0.2507499158382416\n",
      "Epoch #804 Train loss: 0.06157977947671162\n",
      "Epoch #804 Val   loss: 0.2209612450258503\n",
      "Validation Iteration #50 loss: 0.2541095018386841\n",
      "Epoch #805 Train loss: 0.06145206329069639\n",
      "Epoch #805 Val   loss: 0.220977134362651\n",
      "Validation Iteration #50 loss: 0.2510153353214264\n",
      "Epoch #806 Train loss: 0.06136588595415417\n",
      "Epoch #806 Val   loss: 0.2209949597464925\n",
      "Validation Iteration #50 loss: 0.25232890248298645\n",
      "Epoch #807 Train loss: 0.061411370278189055\n",
      "Epoch #807 Val   loss: 0.22101269241476323\n",
      "Validation Iteration #50 loss: 0.25261181592941284\n",
      "Epoch #808 Train loss: 0.06213134252711346\n",
      "Epoch #808 Val   loss: 0.2210339308861851\n",
      "Validation Iteration #50 loss: 0.25057077407836914\n",
      "Epoch #809 Train loss: 0.06167574138625672\n",
      "Epoch #809 Val   loss: 0.22105257788218438\n",
      "Validation Iteration #50 loss: 0.2599095106124878\n",
      "Epoch #810 Train loss: 0.06093395197469937\n",
      "Epoch #810 Val   loss: 0.22107184566730545\n",
      "Validation Iteration #50 loss: 0.25900349020957947\n",
      "Epoch #811 Train loss: 0.06060255720819298\n",
      "Epoch #811 Val   loss: 0.2210906605564789\n",
      "Validation Iteration #50 loss: 0.26483583450317383\n",
      "Epoch #812 Train loss: 0.062241219768398685\n",
      "Epoch #812 Val   loss: 0.22111173133004614\n",
      "Validation Iteration #50 loss: 0.260796457529068\n",
      "Epoch #813 Train loss: 0.060789242778953756\n",
      "Epoch #813 Val   loss: 0.22113154501150967\n",
      "Validation Iteration #50 loss: 0.26130014657974243\n",
      "Epoch #814 Train loss: 0.06062468866768636\n",
      "Epoch #814 Val   loss: 0.22115193362531937\n",
      "Validation Iteration #50 loss: 0.2610901892185211\n",
      "Epoch #815 Train loss: 0.060556412233333835\n",
      "Epoch #815 Val   loss: 0.22117249159280036\n",
      "Validation Iteration #50 loss: 0.2633158564567566\n",
      "Epoch #816 Train loss: 0.06034003276573984\n",
      "Epoch #816 Val   loss: 0.2211916478309195\n",
      "Validation Iteration #50 loss: 0.2620270848274231\n",
      "Epoch #817 Train loss: 0.06093031716974158\n",
      "Epoch #817 Val   loss: 0.22120942214764172\n",
      "Validation Iteration #50 loss: 0.2623094916343689\n",
      "Epoch #818 Train loss: 0.060955923834913654\n",
      "Epoch #818 Val   loss: 0.2212305458370504\n",
      "Validation Iteration #50 loss: 0.26339074969291687\n",
      "Epoch #819 Train loss: 0.06029118450456544\n",
      "Epoch #819 Val   loss: 0.22124968241012566\n",
      "Validation Iteration #50 loss: 0.2620902359485626\n",
      "Epoch #820 Train loss: 0.06072212863517435\n",
      "Epoch #820 Val   loss: 0.22126947037870764\n",
      "Validation Iteration #50 loss: 0.2623610496520996\n",
      "Epoch #821 Train loss: 0.05974435835684601\n",
      "Epoch #821 Val   loss: 0.2212906390269575\n",
      "Validation Iteration #50 loss: 0.2627164423465729\n",
      "Epoch #822 Train loss: 0.06022645376230541\n",
      "Epoch #822 Val   loss: 0.22131162171124721\n",
      "Validation Iteration #50 loss: 0.2642817795276642\n",
      "Epoch #823 Train loss: 0.06006959796343979\n",
      "Epoch #823 Val   loss: 0.22133335831440856\n",
      "Validation Iteration #50 loss: 0.26206669211387634\n",
      "Epoch #824 Train loss: 0.05993362849480227\n",
      "Epoch #824 Val   loss: 0.22135440499910386\n",
      "Validation Iteration #50 loss: 0.26367515325546265\n",
      "Epoch #825 Train loss: 0.05947720475102726\n",
      "Epoch #825 Val   loss: 0.22137480757993167\n",
      "Validation Iteration #50 loss: 0.26217716932296753\n",
      "Epoch #826 Train loss: 0.0594352086711871\n",
      "Epoch #826 Val   loss: 0.22139522725593744\n",
      "Validation Iteration #50 loss: 0.263386070728302\n",
      "Epoch #827 Train loss: 0.05978704136061041\n",
      "Epoch #827 Val   loss: 0.22141153509899925\n",
      "Validation Iteration #50 loss: 0.26433172821998596\n",
      "Epoch #828 Train loss: 0.059674347407723725\n",
      "Epoch #828 Val   loss: 0.22142880197368658\n",
      "Validation Iteration #50 loss: 0.26528942584991455\n",
      "Epoch #829 Train loss: 0.05994819643858232\n",
      "Epoch #829 Val   loss: 0.22144624114423245\n",
      "Validation Iteration #50 loss: 0.2620733976364136\n",
      "Epoch #830 Train loss: 0.05855050594790986\n",
      "Epoch #830 Val   loss: 0.22146306596651547\n",
      "Validation Iteration #50 loss: 0.2624053657054901\n",
      "Epoch #831 Train loss: 0.0588688568065041\n",
      "Epoch #831 Val   loss: 0.2214794189051984\n",
      "Validation Iteration #50 loss: 0.2637447714805603\n",
      "Epoch #832 Train loss: 0.05855042801091546\n",
      "Epoch #832 Val   loss: 0.22149689636332626\n",
      "Validation Iteration #50 loss: 0.2623150646686554\n",
      "Epoch #833 Train loss: 0.05758004851247135\n",
      "Epoch #833 Val   loss: 0.22151092813449733\n",
      "Validation Iteration #50 loss: 0.2650771141052246\n",
      "Epoch #834 Train loss: 0.057614133252125034\n",
      "Epoch #834 Val   loss: 0.22152591474261607\n",
      "Validation Iteration #50 loss: 0.2631913721561432\n",
      "Epoch #835 Train loss: 0.05744953865283414\n",
      "Epoch #835 Val   loss: 0.2215428693445566\n",
      "Validation Iteration #50 loss: 0.2649609446525574\n",
      "Epoch #836 Train loss: 0.057814535929968484\n",
      "Epoch #836 Val   loss: 0.2215603117312965\n",
      "Validation Iteration #50 loss: 0.26545411348342896\n",
      "Epoch #837 Train loss: 0.05813557889900709\n",
      "Epoch #837 Val   loss: 0.22157727328269486\n",
      "Validation Iteration #50 loss: 0.2622239589691162\n",
      "Epoch #838 Train loss: 0.05812653448236616\n",
      "Epoch #838 Val   loss: 0.22159458817341177\n",
      "Validation Iteration #50 loss: 0.26545649766921997\n",
      "Epoch #839 Train loss: 0.057992440011156235\n",
      "Epoch #839 Val   loss: 0.2216126848448873\n",
      "Validation Iteration #50 loss: 0.2647004723548889\n",
      "Epoch #840 Train loss: 0.05797596491481129\n",
      "Epoch #840 Val   loss: 0.22163000438581454\n",
      "Validation Iteration #50 loss: 0.2653278708457947\n",
      "Epoch #841 Train loss: 0.05775355900588788\n",
      "Epoch #841 Val   loss: 0.22164965417149646\n",
      "Validation Iteration #50 loss: 0.26337239146232605\n",
      "Epoch #842 Train loss: 0.05755654065624664\n",
      "Epoch #842 Val   loss: 0.22166719093805945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dcc861cc6fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mitr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mnew_images\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# [STAR] Training loop for DBT dataset\n",
    "#params       = [p for p in model.parameters() if p.requires_grad]\n",
    "#optimizer    = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "loss_hist     = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    model.train()\n",
    "    itr = 1\n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        new_images  = []\n",
    "        for img in images:\n",
    "            new_images.append(torch.Tensor(img).to(device))\n",
    "        \n",
    "        images    = new_images\n",
    "        targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses     = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in valid_data_loader:\n",
    "            new_images  = []\n",
    "            for img in images:\n",
    "                new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "            images    = new_images\n",
    "            targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            #print(loss_dict)\n",
    "\n",
    "            losses     = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "            val_loss_hist.send(loss_value)\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(f\"Validation Iteration #{itr} loss: {loss_value}\")\n",
    "            itr = itr+1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} Train loss: {loss_hist.value}\")\n",
    "    print(f\"Epoch #{epoch} Val   loss: {val_loss_hist.value}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'fasterrcnn_resnet50_dbt2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_target = []\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt2.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 87.7441, 272.9621, 167.5659, 358.1119],\n",
      "        [ 85.3471, 315.9027, 169.4834, 364.1429],\n",
      "        [ 93.2433, 281.2645, 154.7558, 335.7330]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.6662, 0.1156, 0.0504], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 62.8209, 157.9640,  91.7518, 180.2420],\n",
      "        [188.3838, 403.5847, 227.9947, 430.9800]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.1594, 0.1030], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[127.6940, 239.4918, 212.1160, 303.3631],\n",
      "        [100.5957, 239.2540, 233.9906, 450.2523],\n",
      "        [106.6572, 231.4821, 205.2414, 335.4365]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9776, 0.4190, 0.3068], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 54.6933, 342.8936, 107.2737, 389.0608]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.4411], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 45.9470, 292.7863,  95.4393, 335.4440],\n",
      "        [ 16.1607, 169.2287, 104.5972, 215.2625],\n",
      "        [ 47.3393, 293.1805,  85.5798, 319.8928],\n",
      "        [ 89.2450, 102.4065, 122.0412, 126.2210]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.9962, 0.0832, 0.0633, 0.0571], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[109.9681, 366.7819, 143.6940, 411.8315],\n",
      "        [119.6013, 374.9862, 142.7573, 401.3211],\n",
      "        [ 17.1889, 300.2581,  59.2231, 355.5367],\n",
      "        [ 91.8490, 360.6859, 144.6863, 428.6650],\n",
      "        [121.2340, 383.1045, 138.4797, 400.3714]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.3907, 0.1386, 0.0945, 0.0571, 0.0530], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 63.3263, 373.9135, 123.6651, 434.5003]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1264], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[  1.7093, 197.8749,  29.1680, 232.1727]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.0589], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 54.4230, 181.2646, 116.0316, 219.5648],\n",
      "        [  0.0000,   9.2048,  43.8576, 106.0882],\n",
      "        [ 53.9996, 397.3368,  77.9601, 418.3516],\n",
      "        [ 19.0042, 484.5821,  45.9066, 507.8534]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.6236, 0.0626, 0.0613, 0.0569], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 86.3047, 411.1221, 147.8296, 454.2407]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.8518], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[191.5669, 287.4332, 239.3960, 341.7889],\n",
      "        [163.4070, 282.8557, 245.6197, 347.0627],\n",
      "        [203.6080, 281.9589, 258.0730, 342.5842]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.2876, 0.1176, 0.0604], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[122.6346, 319.7882, 165.2002, 366.9745]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.0501], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[499.7853, 489.9831, 499.9906, 502.1169],\n",
      "        [499.7302, 295.1121, 499.9832, 307.2467],\n",
      "        [499.5078, 405.9683, 500.0000, 417.8947],\n",
      "        [499.5035, 352.7334, 499.9946, 363.5525],\n",
      "        [499.3902,  70.3790, 499.9878,  80.4302],\n",
      "        [499.1780,  61.6238, 500.0000,  73.6709]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.3548, 0.1761, 0.1385, 0.1130, 0.0910, 0.0620], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[419.4671, 163.7524, 481.0748, 214.7920],\n",
      "        [441.4935, 422.0967, 488.2498, 466.2947],\n",
      "        [407.0742, 328.0104, 438.9962, 360.7046],\n",
      "        [415.8179, 162.4938, 454.8242, 205.9276],\n",
      "        [499.9296, 369.3514, 499.9998, 378.4164],\n",
      "        [416.3678, 165.5183, 440.9952, 199.6491]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.9665, 0.7157, 0.5584, 0.5424, 0.4367, 0.1420], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[170.2075, 303.5156, 208.7308, 349.1207],\n",
      "        [207.4727, 335.2798, 259.0659, 416.5062]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.2614, 0.0733], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 37.3402,   6.5132,  78.2376,  53.4039],\n",
      "        [225.1718, 297.1352, 270.9807, 359.6861]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9775, 0.6154], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[367.2039,  54.3415, 446.9070, 144.0557]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.6522], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[311.9868,   4.8955, 394.5189,  46.5317],\n",
      "        [499.5726,   1.2060, 499.9944,  23.6127],\n",
      "        [499.3257,   0.0000, 500.0000,  16.9986]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9529, 0.1118, 0.0638], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 98.8875, 309.3100, 170.6292, 387.7149],\n",
      "        [105.6832, 294.7068, 203.3809, 380.9455]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9456, 0.3931], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[143.6679, 270.0651, 197.4323, 345.6205]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.0765], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[499.9681,  16.0281, 499.9999,  26.8850],\n",
      "        [499.6337,  28.9631, 499.9774,  38.3929],\n",
      "        [499.4336,  33.6375, 499.9908,  45.2189],\n",
      "        [499.5035,  19.8614, 500.0000,  32.7178]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.4367, 0.2635, 0.1473, 0.1272], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[499.4715,  82.7337, 499.9985,  91.4227]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1051], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 84.0292, 252.3157, 122.7671, 321.3441],\n",
      "        [ 78.0905, 242.2463, 128.7219, 354.2273],\n",
      "        [ 84.5651, 266.9648, 119.8848, 298.6945]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.6448, 0.1540, 0.0711], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 99.9987, 272.8256, 169.4839, 382.5668]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.1576], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[370.1624, 334.1904, 470.2678, 539.0542]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.0603], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[409.1983, 346.7355, 463.5965, 426.3463],\n",
      "        [407.2867, 362.7562, 457.1436, 398.5912],\n",
      "        [390.6879, 337.9027, 472.2061, 453.3321]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9905, 0.2093, 0.1611], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[348.9222, 321.5511, 394.1827, 373.1212]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.2836], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[359.7775, 253.6560, 416.8135, 350.0048],\n",
      "        [366.8633, 274.9408, 410.7755, 335.8074],\n",
      "        [353.1663, 339.8643, 416.7862, 463.3201]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.3717, 0.1787, 0.1041], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 34.0560, 420.7448,  68.2739, 463.4258],\n",
      "        [169.6226, 395.3075, 223.9077, 460.4456]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9644, 0.0771], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[  7.9197, 249.5517,  47.8030, 284.4421],\n",
      "        [212.5329, 367.5905, 271.5054, 427.7704],\n",
      "        [122.1639, 248.6662, 167.3399, 273.3680]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9230, 0.2939, 0.1535], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[450.5877, 336.3201, 488.7928, 375.8241],\n",
      "        [308.9315, 194.8580, 385.1514, 265.5412],\n",
      "        [265.5277, 176.3790, 415.1353, 276.9954],\n",
      "        [499.4893, 417.6945, 499.9884, 426.8476]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.8796, 0.5947, 0.1330, 0.1117], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[333.2836, 227.8199, 380.6322, 255.6241],\n",
      "        [278.6511, 219.5878, 378.2039, 266.1948]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.4081, 0.1473], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 46.1107, 405.5239,  71.6780, 437.1997],\n",
      "        [ 42.7793, 347.9014,  56.2818, 360.6665],\n",
      "        [ 41.6673, 393.6028,  73.9918, 449.7811],\n",
      "        [ 48.1536, 408.9196,  68.3946, 428.1262],\n",
      "        [ 31.6906, 403.1239,  97.7401, 438.2408],\n",
      "        [138.2087, 287.4263, 219.9564, 413.7140],\n",
      "        [ 36.5702, 399.3036,  67.1983, 433.8997],\n",
      "        [167.6327, 301.2122, 225.8681, 385.3062]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.9581, 0.4841, 0.3068, 0.2908, 0.1898, 0.0761, 0.0657, 0.0507],\n",
      "       device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[366.4285, 176.6773, 461.4644, 291.6107]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.0559], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[318.7711, 204.2470, 427.5560, 271.7654],\n",
      "        [293.7112, 205.0566, 414.1891, 304.1883]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.4121, 0.0620], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[499.3061, 427.4390, 499.9987, 452.7164],\n",
      "        [499.3102, 441.5074, 499.9987, 468.4199]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.0527, 0.0513], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[325.4286, 331.5887, 385.3473, 376.0620],\n",
      "        [320.4232, 320.5614, 396.3699, 392.9236]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9907, 0.6132], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[302.0854, 200.1139, 337.1468, 241.2957],\n",
      "        [280.7161, 192.7445, 338.6138, 246.1929],\n",
      "        [304.1088, 378.5365, 343.6106, 426.7848],\n",
      "        [273.8993, 196.1818, 314.9573, 241.8597]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.9893, 0.7380, 0.4361, 0.0683], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[499.9626, 474.5816, 499.9999, 484.5222],\n",
      "        [499.6474,  40.3720, 499.9775,  50.2673],\n",
      "        [499.6779,  35.1923, 499.9807,  46.3782],\n",
      "        [390.7622, 144.4956, 446.8911, 188.2757],\n",
      "        [499.3083, 484.8903, 499.9964, 494.9060]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.4367, 0.2729, 0.2618, 0.2557, 0.0678], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[338.1658, 419.0842, 493.8680, 541.5319],\n",
      "        [369.0457, 443.6576, 462.7893, 535.2563],\n",
      "        [409.2600, 442.1702, 498.8105, 541.6466],\n",
      "        [347.9068, 353.7234, 384.2653, 399.3178]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5175, 0.1110, 0.1070, 0.0557], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 58.1999, 285.0853,  96.0590, 305.8445],\n",
      "        [ 65.3804, 287.7560,  88.2731, 302.6144],\n",
      "        [ 13.7995, 519.1786,  34.3385, 536.3815],\n",
      "        [ 33.5499, 228.8126,  85.1862, 267.4377],\n",
      "        [ 64.0786, 287.7161,  80.9402, 298.9505],\n",
      "        [ 51.2011, 279.9479, 102.1107, 311.3332]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.8250, 0.2039, 0.1625, 0.0660, 0.0553, 0.0501], device='cuda:0',\n",
      "       grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[304.2423, 354.8976, 344.9725, 401.4660],\n",
      "        [308.1842, 365.1329, 344.1197, 390.1929],\n",
      "        [291.8588, 356.9646, 362.2301, 407.5594]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1, 1], device='cuda:0'), 'scores': tensor([0.9129, 0.5096, 0.1068], device='cuda:0', grad_fn=<IndexBackward>)}]\n",
      "[{'boxes': tensor([[ 53.3140, 226.2638, 137.9574, 298.9110],\n",
      "        [ 16.9899, 384.4053,  40.9124, 411.5141]], device='cuda:0',\n",
      "       grad_fn=<StackBackward>), 'labels': tensor([1, 1], device='cuda:0'), 'scores': tensor([0.9940, 0.0540], device='cuda:0', grad_fn=<IndexBackward>)}]\n"
     ]
    }
   ],
   "source": [
    "#print(loss_dict[0]['boxes'].shape)\n",
    "\n",
    "all_target = []\n",
    "all_scores  = []\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "model.load_state_dict(torch.load('fasterrcnn_resnet50_dbt2.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "for images, targets, image_ids in valid_data_loader:\n",
    "    new_images  = []\n",
    "    for img in images:\n",
    "        new_images.append(torch.Tensor(img).to(device))\n",
    "\n",
    "    images    = new_images\n",
    "    targets   = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    loss_dict = model(images)\n",
    "    print(loss_dict)\n",
    "    \n",
    "    all_scores.append(loss_dict[0]['scores'].data.cpu().numpy())\n",
    "    all_target.append(loss_dict[0]['boxes'].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[419.4671  163.75243 481.07483 214.79196]\n",
      " [441.4935  422.09668 488.24985 466.29468]\n",
      " [407.07422 328.0104  438.99622 360.7046 ]\n",
      " [415.8179  162.4938  454.8242  205.9276 ]\n",
      " [499.92963 369.35138 499.99982 378.4164 ]\n",
      " [416.3678  165.51833 440.99518 199.64906]]\n",
      "[0.9665258  0.71571624 0.5583865  0.5424448  0.43674445 0.14202209]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0, len(all_target)-1)\n",
    "print(all_target[index])\n",
    "print(all_scores[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [STAR] For doing the inference on the test images\n",
    "\n",
    "model.eval()\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "outputs = model(images)\n",
    "outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 750, 500)\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c  = valid_dataset[index]\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(b['boxes'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 750, 500)\n",
      "[[499.4715   82.73369 499.99854  91.42273]]\n",
      "[0.10514107]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(outputs)\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "index        = random.randint(0, len(valid_dataset)-1)\n",
    "images, b, c = valid_dataset[index]\n",
    "\n",
    "print(images.shape)\n",
    "plt.imshow(images[0])\n",
    "ax   = plt.gca()\n",
    "\n",
    "print(all_target[index])\n",
    "print(all_scores[index])\n",
    "\n",
    "temp  = all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "temp  = b['boxes'].data.cpu().numpy()#all_target[index]\n",
    "index = 0\n",
    "rect  = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='red', fill = False)\n",
    "ax.add_patch(rect)\n",
    "\n",
    "\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "images, targets, image_ids = next(iter(train_data_loader))\n",
    "images  = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "boxes  = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "sample = images[2].permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# [STAR] For plotting the images\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "for box in boxes:\n",
    "    cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2], box[3]),\n",
    "                  (220, 0, 0), 3)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 1.7607202701618698\n",
      "Iteration #100 loss: 1.7268396514227657\n",
      "Iteration #150 loss: 1.7064545852167243\n",
      "Iteration #200 loss: 2.0347349650472\n",
      "Iteration #250 loss: 1.712778722073257\n",
      "Iteration #300 loss: 1.5943888343150856\n",
      "Iteration #350 loss: 1.5178248758969637\n",
      "Iteration #400 loss: 1.5444363160208001\n",
      "Iteration #450 loss: 1.5647550482854071\n",
      "Iteration #500 loss: 1.5485437903163515\n",
      "Iteration #550 loss: 1.9103887034181755\n",
      "Iteration #600 loss: 1.491785500489904\n",
      "Iteration #650 loss: 1.466433896319538\n",
      "Epoch #0 loss: 1.7162335509320108\n",
      "Iteration #700 loss: 1.7310519640009574\n",
      "Iteration #750 loss: 1.4555127588148413\n",
      "Iteration #800 loss: 1.4856642979590085\n",
      "Iteration #850 loss: 2.133777053700693\n",
      "Iteration #900 loss: 1.6277719320275719\n",
      "Iteration #950 loss: 1.44726940291462\n",
      "Iteration #1000 loss: 1.3971989617046594\n",
      "Iteration #1050 loss: 1.3552668006526838\n",
      "Iteration #1100 loss: 1.375852235221211\n",
      "Iteration #1150 loss: 1.3284461516759858\n",
      "Iteration #1200 loss: 1.3081550448389287\n",
      "Iteration #1250 loss: 1.1751181678118\n",
      "Iteration #1300 loss: 1.27619185567906\n",
      "Iteration #1350 loss: 1.3163148808307834\n",
      "Epoch #1 loss: 1.4463744356515336\n",
      "Iteration #1400 loss: 1.346408020827714\n",
      "Iteration #1450 loss: 1.4509955625944122\n",
      "Iteration #1500 loss: 1.364827779535403\n",
      "Iteration #1550 loss: 1.4334169862670842\n",
      "Iteration #1600 loss: 1.407514695126605\n",
      "Iteration #1650 loss: 1.3416174707230355\n",
      "Iteration #1700 loss: 1.088765164209676\n",
      "Iteration #1750 loss: 1.3059198483364811\n",
      "Iteration #1800 loss: 1.2234584833610427\n",
      "Iteration #1850 loss: 1.2163395890296245\n",
      "Iteration #1900 loss: 1.4437247944097054\n",
      "Iteration #1950 loss: 1.1362663042711096\n",
      "Iteration #2000 loss: 1.257143647906567\n",
      "Epoch #2 loss: 1.3227414647873994\n",
      "Iteration #2050 loss: 1.4417955994306881\n",
      "Iteration #2100 loss: 1.2887957222151805\n",
      "Iteration #2150 loss: 1.1400735086186962\n",
      "Iteration #2200 loss: 1.3628313886434062\n",
      "Iteration #2250 loss: 1.4560974294531748\n",
      "Iteration #2300 loss: 1.2282406376511603\n",
      "Iteration #2350 loss: 1.28671306956072\n",
      "Iteration #2400 loss: 1.2797563239531533\n",
      "Iteration #2450 loss: 1.1688346587596594\n",
      "Iteration #2500 loss: 1.260821003606902\n",
      "Iteration #2550 loss: 1.2040374501180904\n",
      "Iteration #2600 loss: 1.1972478812620455\n",
      "Iteration #2650 loss: 1.1712962493828663\n",
      "Iteration #2700 loss: 1.2207586776395578\n",
      "Epoch #3 loss: 1.2301452783142202\n",
      "Iteration #2750 loss: 1.131861098379026\n",
      "Iteration #2800 loss: 1.2891437930785192\n",
      "Iteration #2850 loss: 1.14637454007873\n",
      "Iteration #2900 loss: 1.328613510451238\n",
      "Iteration #2950 loss: 1.2091155384214054\n",
      "Iteration #3000 loss: 1.2318513916896916\n",
      "Iteration #3050 loss: 1.1055627710892748\n",
      "Iteration #3100 loss: 1.1594369515750969\n",
      "Iteration #3150 loss: 0.9557497630701202\n",
      "Iteration #3200 loss: 1.0094715023434597\n",
      "Iteration #3250 loss: 1.0686707177495565\n",
      "Iteration #3300 loss: 0.8352206339954255\n",
      "Iteration #3350 loss: 1.1205806873400617\n",
      "Epoch #4 loss: 1.1621026393237242\n",
      "Iteration #3400 loss: 1.1955752304016987\n",
      "Iteration #3450 loss: 1.3170233018400381\n",
      "Iteration #3500 loss: 1.050823029625402\n",
      "Iteration #3550 loss: 1.0902784829836047\n",
      "Iteration #3600 loss: 1.187195728553164\n",
      "Iteration #3650 loss: 1.2105885212317862\n",
      "Iteration #3700 loss: 1.0959537532657933\n",
      "Iteration #3750 loss: 1.349466143398455\n",
      "Iteration #3800 loss: 0.9913880280214239\n",
      "Iteration #3850 loss: 1.0350717634867168\n",
      "Iteration #3900 loss: 0.9277540556027634\n",
      "Iteration #3950 loss: 1.1566059944469866\n",
      "Iteration #4000 loss: 1.0937833658236307\n",
      "Iteration #4050 loss: 1.0216458043195995\n",
      "Epoch #5 loss: 1.120662040947033\n",
      "Iteration #4100 loss: 1.100794490607796\n",
      "Iteration #4150 loss: 1.1819245332664325\n",
      "Iteration #4200 loss: 1.1410733082678435\n",
      "Iteration #4250 loss: 1.0875705666268152\n",
      "Iteration #4300 loss: 1.2606450852544995\n",
      "Iteration #4350 loss: 1.058986403148981\n",
      "Iteration #4400 loss: 0.9109938163043974\n",
      "Iteration #4450 loss: 1.1557238204325426\n",
      "Iteration #4500 loss: 0.9370555477833052\n",
      "Iteration #4550 loss: 1.173954231055069\n",
      "Iteration #4600 loss: 0.9850710144688257\n",
      "Iteration #4650 loss: 1.1487813825086015\n",
      "Iteration #4700 loss: 1.005689021379677\n",
      "Epoch #6 loss: 1.0917526986408996\n",
      "Iteration #4750 loss: 1.0475928359390307\n",
      "Iteration #4800 loss: 1.1650594750433194\n",
      "Iteration #4850 loss: 1.075659432778928\n",
      "Iteration #4900 loss: 1.2033784126943747\n",
      "Iteration #4950 loss: 1.0421480256584839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2d11f927d179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(len(images), len(targets), images[0].shape, len(targets[0]['boxes']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_roi_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0mbox_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mclass_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mtracing_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mper_level_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0midx_in_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mrois_per_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_in_level\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        #print(len(images), len(targets), images[0].shape, len(targets[0]['boxes']))\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [STAR] DBT Training Code\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "\n",
    "from torchvision import  transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "index = random.randint(0, len(td1)-1)\n",
    "a, b, c = td1[index]\n",
    "temp  = b['boxes'].data.cpu().numpy()\n",
    "index = random.randint(0, len(temp)-1)\n",
    "\n",
    "#print(temp)\n",
    "\n",
    "im    = a.data.cpu().numpy()\n",
    "plt.imshow(im[0].astype('float32'), cmap='gray')\n",
    "\n",
    "ax   = plt.gca()\n",
    "\n",
    "\n",
    "index = 0\n",
    "rect = patches.Rectangle((temp[index][0], temp[index][1]), temp[index][2]-temp[index][0], temp[index][3]-temp[index][1], linewidth=1, edgecolor='cyan', fill = False)\n",
    "#rect = patches.Rectangle((temp[index][1], temp[index][0]), temp[index][3]-temp[index][1], temp[index][2]-temp[index][0], linewidth=1, edgecolor='cyan', fill = False)\n",
    "ax.add_patch(rect)\n",
    "#rect = patches.Rectangle((0, 0), 500, 100, linewidth=2, edgecolor='cyan', fill = False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = DBTDataset()\n",
    "#valid_dataset = DBTDataset()\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# valid_data_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=collate_fn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.00001, momentum=0.9, weight_decay=0.5)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #50 loss: nan\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #100 loss: nan\n",
      "{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.7703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>), 'loss_rpn_box_reg': tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "Iteration #150 loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-402f63afabfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "#train_dataset = td1\n",
    "#model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for k in range(len(train_dataset)-1):\n",
    "        images, targets, image_ids = train_dataset[k]\n",
    "        #for images, targets, image_ids in train_data_loader:\n",
    "        #print(images, targets)\n",
    "        \n",
    "        images  = [images]\n",
    "        targets = [targets]\n",
    "        #images  = list(image.to(device) for image in images)\n",
    "        #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        #losses = loss_dict['loss_rpn_box_reg'] + loss_dict['loss_objectness']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(loss_dict)\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[  76., 1665.,  254., 1791.]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'image_id': tensor([156]), 'area': tensor([22428.], device='cuda:0'), 'iscrowd': tensor([0], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "images, targets, image_ids = train_dataset[k]\n",
    "images  = [images]\n",
    "targets = [targets]\n",
    "\n",
    "loss_dict = model(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-80a206e9139a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#cpu_device = torch.device(\"cpu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch2/lib/python3.7/site-packages/torchvision-0.8.2-py3.7-linux-x86_64.egg/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moriginal_image_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "images  = [images]\n",
    "#cpu_device = torch.device(\"cpu\")\n",
    "outputs = model(images)\n",
    "#outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_dict[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3000, 2000])\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4f50cab3d0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAAD8CAYAAAAmJnXEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9W6xt2Xae9bXe+7jNOddt36rqVJ27j6M4Jo5JiI14CUIhVl4CD0GJhIxERBAi4oUHnCcQUaQ8QJACIiJIUcKDHaJIERYKhDhCCkIE27EisE/iY3N87Lru+7rNyxij9954aH2MuarOqeOqVa4qV+3ZpKW99lxzrjX32m300drf/vb/oqoc4hAvYrhP+w0c4hCfVhyS/xAvbByS/xAvbByS/xAvbByS/xAvbByS/xAvbHziyS8iPyEivyoivy4iP/VJ//xDHGIK+SRxfhHxwLeAPwq8AfwC8KdV9Zuf2Js4xCFKfNIn/x8Gfl1Vv62qA/C3gD/xCb+HQxwCgPAJ/7xXgddv/P0N4Mfe+yQR+bPAnwXw+D+44PiTeXef5xBB2oZcO9QJEhUXMwA5OEQV9YKM2Z4bE2QFEXu9E3LtiQtBHUgCdfYhGRD7fP/zbn6u5Qnl72oPzTWHK8/PN14n5WcAvku0YURQvCiVJLxkHrQ/OP+Ib/2Tb7/vP/2K509U9f57H/+kk1++x2PfVXep6l8D/hrAsdzRH5N/7eN+Xy9E+NVd+t//FVLjcEmpznukJHgOjtx6/HrE9ZG0agiPryBn8B5tAttXj4gLx8VXPTjIAYYTRbIlamoVyYIKaFByZxeXjGIfWi6YSpEoSAI3yPza3CoaytdGQaJQXwi7+5mXfvAxd7sNrR/5QneBE+Wv/OjPzP+2P+r+5Pv+u39O/85vfq/HP+nkfwP44o2/vwa89Qm/hxcyXNuimy3NO1f0Lx+RWkdc1VRXA+TMcKcFBb+NyJhwfUTbCtkOyHqLbITquEW0wkVP7EC9nfqpsfPLklvnO4AkQUXt0G8URkGivU4rhSzz9wgbYWgt8RHKn0pqoXvH8U51F/2KEFzGiXIUev69X/xJnvZLtv92d7vfye/UL/cDxi8A3xCRr4pIDfwp4Gc/4ffwYkZV2Sn+1kNS4yxpW8fufst40lJdR9yQGU4b4qklU1o26LKFKqC7nurp2r7VWud7uERL+txlcqvkWi2xAXJJ4gwyCBoUDXbau52QW7szTOVTuHK4rUMGIdflLuIh19C9HTi/7lDg0eaI31qfsY4NQw5Wnt0iPtGTX1WjiPw54O8DHvjrqvorn+R7eFFDhwF/5wxdb1j8wrdZ//jX8UNGBYaTQLVOqBNi58ihIWwSLmV0EPJRhwN0jKRW8L19zxzARSHHKcmthNFyJ3C9oAFUSl8QBTfsa/9w6e2C8PZcydgFoaC1kkRwo90J/E4Y31xy7jOnyy3rocZLJqmj1nyr38knXfagqn8P+Huf9M990UPHCM4hd07RiyuWv/qE3Vfv4PtMWEfGo8o+VyXXjlwJokKuAz4p6d4xbt3jBp2bXDudwY3AxpG6jFbsG1cVwJ6TA+AVjeWkL+WOJPtabjNk0C7ZG44OjiNpcHARoFGaJ44+HfH41UDdRM5dx2ao+KJe3Op38okn/yE+nXB1hW62cP8Oslqizy+oVx1pVROuelzM5ODwO50vhByEeFpT1Q7XJ7Ty1M8HtveCnfACeDvIXYScxZrZ0uDm2mCdqbm9mfRMza9X/NaBCKlRZOORKLgopN4hTpFkF4lkq/+H3YLNawN1iDQh7RGpDxmH5H8BQkLAnZ6gqsgYyXePcQ8j/p2n8IW7aB3wF1tc5dHK47pA7Dxhl8gNDMcVYePwTnDbSPc0kqpArmSu1wG8AE7mxPZbAZH56+qxK0WxPsCB6x2iEFtF24TsPKKW7H4r5MaQI0VInaJOqC+F8SowHAdWbX/r38sh+V+AcKsldC14B9cbpGvQoyUyjLjrnrxqUO/BOXIT8NuI+orsbR6Ag9Q4YtdQnwthk1g+FOq1Y3PfERdWuvi+XAxB3435w3x6ayh3hmwQKALjcYIuwejmu0Ke4NAMuYLcZKroSK2CE8K1Y3PdUPl069/LIflfhPAerQL5qMWJQEwQvN0J+gGpAvG0taf2CXImAFo5krO6RgFJynhsJVF1Hcl1hRsVvxPoQDPW4GaBbMgOeV/u4MCNNgdwUciVkpcJaTKaBbf2+N4eV2elFFFwZQAXFwVl2hjKVL3ecHFe8cr47Fa/lkPyvwgxRvJRi3rH+PIJ1dM1Ghy6XCL9iMRMuOrJTUC9gNgUmAxuKLX/UZjLFkmKA/w245JDI9BDaiBPEGgW3M5qenWQW71xISixzQa0B4XLQNjZqa/YheJ2YtPdnZBae42U16fG7gqLh0L30EN/u9LnkPwvQOS+Zzhrad++Jq5q4klHeHKFVh6tA+oVt+lxqqRlQ14E3JBInbcJMBA7IVWW2bkSqnUuF0I51bOdxtSWqLkuDTGlDAqKVhkqxdXJBlw7j2y94f4DpM4GX26wabDf2Tdwgw3M4lKpLg0+negQ1bWi6TMCdR7ikw8dIy4puavwu0hcBvxVjdsMaB2sKW0rQ00E4/k4QbL9iROa80Suhdg4XFRyEPwu43tlXJUkTQZ7arZkzhXERRl8dQkJGVTIo4Odx/XOJsIehnsJt3XglNRm/NrhBrELKUz0IEWDIUcIjEdWRskB7TnE+0ZO1G9dsvviCbmyTnR4sKS6GozMFjMkRXJGon2od0iyZjI3nlx5xs7hR8UlxY1Wfy8eR4ajCnVloOWMp5PKtBeHnfgKnNdzQgPgrDxCwO2cYf6tQlDSUdqXSrUiSSAK8TgjgxA2wrgqj7tD8h/i+8Wjp9RHLXFVI6ps79X4PuF2I1p5cIrikJSRMZEXNSqCixlJmbAecWNDahypMTjT72xAtnjsufyyJy4gLpW4ymhQS+heYGuJPc29RK3pzVWhN7QZGRxkIVw5YgY5G8jbgL/ypFrRkC3J64wGIeJBIA2A97f6lRyS/wUJHQZkTISrntRV5CDs7tUsfmtE0sTFyfa5CCqCBrdvYIdEuBrxW0dcBtxozx1OK3Z3HP2Zkisre7q3PK4MwXIoRDYPudEbmL8W4huQ7G7gkkMihI0jUiO+3AmcIqPDrx2psW+gtSK92J3jlnFI/hclVNHgcLuI9IlVVtavtmy+vKR92Nvj5TkSM25MqCpkNeJa5W0KLJBaYf1SzXgkSFK6p0r1z6A/EXKQmfSWaiAUenPh+Giwia2L+6TV6OwicEpqZSbHSS6kuNIU+619pE5JWB/gBj4bxLZDfHqhw0BuAu66RwDJNd2TkavXGrJv6Z4M+PUISSFm3HqHNjXa2PBLUiYuK7YPaq5ec8QlNE/h6I1EfT6SGse4qEm1sTC18P2ZMH5jKNsg7OYbEwwf9aUx9tnoDVu3J8gVzr/cAHWkDMJid/s13EPyvyChMRpr8+4SN2ZSG0i14+j13pCTRWA4rqifD/h+z1fItaVIf7fh+lVPbIXuieLfUuqrhCQYj+w51dpOeD8YNAp2AUgqw63C9RcMCVIHuP1KlySZLwQEq/ODorGUN7qnSPid2IXWYJPrW8Qh+V+gqJ5vGe4vUe9sqFV3pMbh+4wbM26rxGVgOK1xo9GaU+fZPDAi29FvRdRLgTyFYeVxsaA/g1JfZ1LtGZeCK+xNr+DSxPeZ6A+29AJ7XtC0+OISaBIbgJWLwfVGmLMBXCG6qS3AuChwSxGGQ/K/SPH62/DgB0CVeNzgxsy4CqTOMtBvM9WVTXzjUc1wWjEuHdVa6R71aLB1R92BngQjoGVLRrBpsKgvn1tDm+tS/hi72ejLtZYlFZ33gWcORWEwpLLoQpkDzCxRypRY310G3SYOyf8CRbq6QrISVxUSM36XUA+pTGzDOiIxzzSH5tlAWHtbagm23O6jkhpP2OV5EGZljaE/3ePIsKpmxmdsbXKr3kqdtDCcftpp973V87myrLftLUV6ZyzPOpNOI27tDRG6sQxDFhhu//s4KLa9SKFK/fpz4sKhwTEeGTGtvkyEjfHi46omdZ762Q63G43mULnCyxEbkk1ATTmpZ/jSCdV1pLnIpBYbXk31vhTu/trhoi2nu53M9AgtkGgOZR84gfQOOa+QnSM3GV2WwVdRipDIfNe5TRxO/hcs9Mkz2kenXH+xxSWoLyL1xYCKkBaB/iywfGNrig5NQIMDJ8RlsOmvYrU3xvFJtbO7SWeKEOEa/KCkBrYv5/mkduOeljC/l1CW3501sFqS2iXI3uBQ9YpW4DeO5NTuCqMzQtyEKN0yDif/CxZ5u8OvR5ZvD6iD5lmP2474dc/urt0JUhtMo6fz9Gc2EEutDb0sQW/g+Td0doaVYzwKVJtM+1RtmQUIW+Pl641BrDoMw083trwK+hM7Y3AyrUoeD6TjBElwG2e0iDg10Qeo8xAfMHQcCOfXuOstfrMkLiu8wPaVjurayp/UelLryUEI20yuhOwF2rKdpYrflfIkO1TEltlHGJeCOsficWZceVJrj+eqrDpOzS/2uBvFYE8/Nbp2umt0SDKkJ60rpIuoloX3DK5Mdo3heeD2HOIDhj59jpydEN5+Dq/eYfegY3Pfc+dXNlbidLZK6IdCd1BL6NgZRydsrNBWL/htRr2pLPgh2UUihvwcfwe2dx25NgkSFSEuYTi23V51BgGpt8UXiSZxIuxFrmxOILYgI7b2qEFNTC4X2vMB6jzEB43c97jNDgkeGRNXr3kbTB1VuDFTbWJJzAJfFiZobPdVsiQ1pCcb83JKQKmttpFsjbQo7E6dne5iF4FbwHin7Otma4TxzMoPEoV4nGbGJxnceTVDm1MDnSudZVRuE4fkfwFDhwHdbIh/4Bu8/S8v2LyWuftPxeRKouDGNOt4AvMaYXMejU8Ts6k5TPz/ifcP+F1Ey8Q17BIuKrGpGL2d+qmG6kpwQ7Xn/ARbgcyraEhOFqTKaBTcVZhPeIlFzKr0CCT5SA3vIflfxFCFlBhXgXu/PFL9nwPxyLB5kwrMJlgL5EXFuLKFFzfkebNLnSkzkHUvaOsEGdN8MbhtwgNt64hdwEWoNob9TwksWpZfEnAZyIvC/d8FJEtJemZFBylNcq50Hogdav5DfKiQtqF7/RKGEZzDxQWbL3Skk1BqeUvi1BiFQZINtVJrq40SnJU+MaPeW2+QTVlBvbNpsFg97vpM2JZd3lpwFYW7Y9ehS1hpo4Ks3Z7q7IooVkl2SVYS5Wa/z+t7gcMa4yE+TGjKyBiR3YC2ttJYXxhr00SpAs1Foroy1eZce1Nz8GW5vSr4fXT4bSpiVEoOVvOrE3LjcEPC7xL12jMu/Yzlx07LJhezAJYWdQdyYUP4UtaoFkRI5wV4spC7TNjebpEFDsn/woYOA3J+hZ4dIzEVpYTI4q3M7n7DuDTdTomekBU3JDQrwdlgS/3UjNpeLSrIYCiQRNvVlTIM832ieTIQm5ZxWZQdRimfl9XHle3uTs2spCJs65Vcy7tOfDfahZecMpzmT4fVKSLfAa6ABERV/UMicgf4H4GvAN8B/i1VfV6e/+eBP1Oe/x+p6t//KD//ELcPHSO6XuOaGkSQMRI2O/LJkuUuMh43pNamuxqcyW8m6wVCVlJlUiOSFSlIj3pjnNmFYSc43shwfjuyeOTY3alwoyNvC6qzoDTN4HfOCG0e8mLEVZmmiQx9II0OBofsPH5rr6nOvSk+yKdHaf5XVfXJjb//FPAPVfUvFcO5nwL+ExH5IUyS/PcBXwB+TkR+UFU/AjvjELeOnMi7Hrm8grpCQoAYcbsemhq37RjuL4kLD53H9yC93QEomj4axGjKMk18jejmN9F6BO/M9aU0pG6XqNbeaBGt4AclN8LYWfM6n/hOYXToJjAMjV0claKLCNGRWrtwjOZ8+wnvx0Fv+BPA3yyf/03g37jx+N9S1V5VfwP4dcyj6xCfUogT8mZDPr9A+wGcg7qyL/YD9cMr2oebIlzlyJU3isPE5ox26htpbc/3yY23j7rs23pBy+duyDMrM1dW0097vhNl2e2EcB5wW5vw+o0QLh3+SY2/MmJcrpV0GuF0RD6lZRYF/jcRUeC/K3ZCL6nq2wCq+raIPCjPfRX4xzde+0Z57BCfUkjXISEY7t/3aIxIUyNNYxBiP+K3PdIntKtIi4A6h4wZtzOUKC4rcuMMdowZFUdceNP4hJnzn8ThRPC7SH3tiG3AFYqyZMP5w2Y/1BK1oRgO/CBkr/gidZ4aJXcJosC6+tREq/4VVX2rJPg/EJF//n2e+73A2O95z7ppSNey+Ihv8RDvGymhOeOOj9BxBLA7wDCaHIgTpK6LSUWDbCvwMgtdgfH4q4tsdwQvJo6FQZrqBF9Oelf4++odYZ2oW0euHW60ej/sbqRHBpwpNyMmfKXe7jB5kjk0DRS0u/1Gy0dKflV9q/z5SET+LlbGPBSRV8qp/wrwqDz9A/txvdeQ7qO8x0O8f+gwgDjy9Rp3cgzBI0crGEZ00r9UhXGEXY9rG6gCedFCsFJDBFxWZEykRW0XwcLPa4w5CGnhCq/fllpM49PqfJcM/enPMtqYQQWUpZYwyZvofHTOPQHgFhE9r+22cYu4dfKLyBJwqnpVPv/Xgf8c89j6d4C/VP78n8pLfhb4aRH5y1jD+w3g52/78w/xOxDe4+/dhaZGm8pgy7ZCdiMsO0v8YUTryqxJxwgpIf0AY1l93JTpbs6E3Yh2NZIqUmepZYrMN+xPR6M9IMbm3N43dmfYmHanG2yoFVs1xeZCZ8iTBEoFVBm3iAXxcZ8Kse0l4O+WTj4AP62q/6uI/ALwt0XkzwC/BfxJAFX9FRH528A3gQj8hwek59MLaRrz6Fq0aGMqblpbIuWjFtlFJGe0rqAf7M+6mNqVZJdhNLlz5wxrdw5tAm5r65AaHKkLhgh5SLWpvanz1FeZ1dsJN5i3r8kPMm9pucjs0qjeePvz1HdwZFcurur2v4NbJ7+qfhv4ke/x+FPgexrnqupfBP7ibX/mIX7nwt+7iy470+mv/D6pSy2vlQc1rr6ImG7nJA5VFSnz4GEssGZdkVcNw1lrKI8YCzQ11tAaRQLaZzbxnbfCpGHjzQMs16bubDSGvcVpLgmuDrQu498kyCBlE+wgV3iIDxrOo8emza+quGlIVTR6ZIj7pyYtRLi8vzDEml5tqmIaUVnzOthSfHVtzXMVzGtLvcyyJTJVKF7QpFRXkbCqbZJ7JYzHSi6LLbamuDfAk4QtrYvx/v1mcnI5cHsO8QHDH6/QOiBXG+gaS2pVq/UBrUJRWvOkosA8sTkn5bZcOXyf5glvdTXiNiMyTjomIN5DsEHXhAapE3JtPYDvE2GbaC4TqfH4naFEUhX15QTii26/QFqWJM/Ygk2lBnfeMg7J/wKG3DmDmPdlT3BW5oAtrGcll4bV9QnXm3KzDBGy4i+kLLhXpGVFdb5DRnNrnPQ+EbHTfeIAqaLO4VIm40zDU+xCMO1OCFub+Jowls57vbkpyNC2DM0qLSUTpop1yzgk/wsWEoLV6cOILsyHayp3iBk3DJDBr4sgTs5W4gSHurosj2crhbzgtyOyHezOkbL1AUXh2Rbe932EZLUeQhXmpXVHfTWSGmE4smHZRHWYnF3CtStmduU1uXy9VmTrPlVuzyE+QyFNg+56ZNkhMZHrCtmOdmJ7MeQmZ0tab4skJEVEybUnd8Gmt6q4PlqZE7z1BKr7D8qqI9P3FaTf7/4SHFq7IpMYaZ47kICLQn/qSLV5cYHdFfpTNZ3+SmdHR79xRaX5dqDhIflfpBCb2EpdoV1jnlyVtxo6T/W0/SkpGfc2ZSTnsvBSEVc1joyMOqNDWgd7XZnMamVOLiZtIrPY1dS4UrhAky+ABkd1NdjzTwOpVluEV1N8Ts2e668Kqck27U2K6w81/yE+QLimQdoGXXbklR2rOTjcEIsrS9qXNLCHPquAVp60aoqCQsJf9TM8OvcL1X6RxcSu9j9b1GqZXImxQQsDFAGpTSxXfdkjLuuKvi8cHwdQ+Py9AAXazMWc7pZxSP4XJKRpkC+/RjruSItA9m5O5FwH3GB8hNz4guljFAa1Ez4tayRlqvMdbhf3dwpVSDrr+E/sTlRJld+L2eaycyuG+Ru7U4ttO+aseDmQg+AWVsePS5sCp8aUGiYIVKLJmigQ1nIwpzjE9w//8gOGl4+MYrCNSOVBFb8pnlwz8pLJTQ2wL0vEuPvhvJ8bYJKgje3u3mxuNexpzDJx/v2+NJGk5vOVy8S2vF6ckoNdkMPSMS5s0SW1JmSVJ4SnqLWpm3Q9D2XPIb5fiKBtY84r5SSXPhbagVEVcG5+Lhj7Mld7qNJvo5nU+dK4uvJ9/f41TBh+wfPdkM3RHfb9gMhMd562vGaTuspIb/V1RrKR4XIQpCu05jD1CGZC7eI04T2oNxzifcI1DTJGsxxyzKd0bryhLUMyrg5Y6VKSOdeWmC5pQX/2dwOgrCyWU7/av06ioUMumcWpPWhJL+xVHiaH95vENN+nQo2wQZoGkNFgzknsdja00GJ8ccs4JP/nPZzH3b1j6E7ji6qCxw2ZcD1Ygzs1rs74B26a0mad1xFxgmINqUGche5Q9HtkzLhcLob5jrJ/G1bzTwjRe96jWBPshjyXSLGz19fnVkJNiI+Wie97L4TbxCH5P+fhj1c2yS2hlTO05rIvK4gy3w3m8qWcxG4w9TUXc1lVdLgx2QVR2Ykv5QKRmA3eLCFqdxmyzF83JbepdBFbgi+HvgrkZSB7wY2Z7lkmtmZ/lGvj96emVE/FyNoNZQ5xaHgP8V0hAsEw+OlEdX2BM71h5uq9IS4FtaFIh8/LKmNGy06ulTCOXBuKwyRTqAXmLDQGFGSwO4gWcatp0JUrZ3V8YXpOModaOesfssGc1WVEJRRNziJnkiFsQdaQaruDuJEDse0Q3x1SG2qDNzTHbccZjQH264hFl1Oy1ehaeVNqGxK6qOzkHvYJNiXzVL2on6bB+7vAHBOqA/MdJddGY3DjfqMrXI83GmeQXS7v1S6WHIBkWp8T1SGHgw/vIb5XOI87PkZWC1R1TkgtCgw4w/BliIWXs5+2Isa4zIXkNtEUcnFpmbU5pdT4ZVpryyZ2UUgyOgRqjxmxzXg9YZvmC8XsRcu0V3Tm/7ghEq6F1NQ2K1AlOxtqxdpkzmdtfn/g8x/iRvjVEk5WaF3NxLQ5GQHB9m5lTKhzc9JPU1q5odQ8MTxnuQEBN9rFkRqzI8rVJE2SimxhQYCKro+bFBaK0BWwF7t12AVVFBty40mLQOw848KRirHFdAGlBupLu0ukgw/vId4VziMnx+RFQ//yinHp8bty8nujFYRNwpWhkpQF9KkEElVDgOqADBG/UXIbTLfH74dWNvxyaM6l8S3T4DbMdT0AArmI16ZJ5kSLhWlBgPKRnequV/Y0aAjFASYHIZ5a86veav3dPUW+tEGq2+0yHpL/cxhuuQAnpGWDJKV93FvpMpU/E/ZeDkx3tTOG56JF20AO3kocsAskqyE5jrnBlSlpy/Q3O1tuyaWxBVt8AfZ3m0Fx477UmWYIYDye2NpdJ2xTuZBs71fUMSwdfrCf77d77o//f5bodnur39Mh+T9vIWJCVIsWv+5nxEW9Q3a2njid7DLsdTW1a4hnXcHUDYKUqKTW4/tcdm4VkVygTk+unCVqgUwlKaEMrWRuonX++cB+bjBaaUUpi7R2sw2STMiTCM4pbDOucUg0p0dJEJcmdts+009NtOoQv8vCLRZw/w5gaM6MvZcBlKQ883NyKIprQUitww1KfTnid3FugHMwnH/m6E87uaE0qc4uEj/EAkfu+fyI2C7ADZEO0/FPBpe6sttbpsOut9Jrmh2oiO0XqzeLo+zKUrzV+mFtpdekBfqhf1e3/B0f4ndpSF0blSHb6Tmd/qKKjLnwchy5CaQ2IEmJC8+4tJN3Qne0YPZT6SJjsoZTmGkMGkpyj2nG8SmGFfOHavHZejcEanBp0fpPOtMspDi9WDNcnpvt676IXg3HMu8G6C15PXA4+T9XISEgTW1JWgUrRxrT2pE+2fZUGWi5zYjzQq49bgw0fcZvolGbsdNU4sTXd3BzQaWUREhhaCpooS1L2dUFmxuoezevP1du5vGjiiB7R5fSX0yyhjgbsqUgjEfBfH4XRnP2O/A7E8HSgxvjIaRpoG3QrpmXx1O7/y+WZCuDOMhtwO2irRA+Y1+jF7EpRGbqA2AwqchcjpAVH7OhQynNWLsWqjSqKG5/wU21fyl1JrgT3df4c6lV+2J4wQxjuqiEXkk1xIXdFVInpQ85sDpf+JDVkrxaoJ0xInNtRtK5cfhtproe5uf6zTCXMjjAufnv83ILzFTnd01ty+L6zdXFOckpE1hnLNB5olzWGafXUxrbPHGKyuezro8yN9++z+ioeJ+p1464EFJXDK3zHl36sHFI/s9JSAhGZxgjeVWWUWLG4QjbEbfbL6lLEaLKTWX4faE2+yGild/bkE6YfjbKw0RdEL1BQy68fmJGcHsqA2V+MKR9OePMk0vGPJPloCy8TPsEY54vFEmKU8pgy1zgJSnVtRYhXKv9D/SGFzlEcEdHaFujTTCG5bIyCZLNuJ+ozuWMoiHM2L1tcFmC5yYYp6cgQyoyK4Pk4KzUmbR5iu0QYNBkLv5YWtYW31uOZJ0Rnlymv6KgpeSRgkilzhenF3uZ32V8bxBnf+qKFzCm1qyHmv+FDgmVJV1ZNM9NQIa8HzKVmBbLJ3rxXFYMGddH6w+8kPF7aLRQCuwkzjc4+WKi+1IYl6pWOinvLnHKa+eLItnSzH6B3RrqcRlwYy5+u2o3lKh7WgSYxs91Zlw4st9bG902fluoU0T+uog8EpFfvvHYHRH5ByLya+XPsxtf+/Mi8usi8qsi8sduPP4HReT/LV/7K3LbLuUQ3xVu2cFL94qECPskm8qB6WRVZk8tDcVTV7H1RoxWTNrTjBHIlTdINJfd30m5bZItKWS2iSgHTL4RM5Qp0WBMNxSB2mnCrPZe3ZCpr8bi66szVWIyurN/E8TOkapCzS56/278CL+3D/CcvwH8xHsem0znvowjvhUAACAASURBVAH8w/J33mM69xPAfysiE+Xur2JuK98oH+/9noe4TRTOvgZnSVlOdlEMP++joTFg1OUbq4ZuyNTPe9PfPKpnotnErdFykdzcltLKo021R39uLKNMKM/ctM68obxXeyjy5i7mmdUpKePXo02Ss5rV6XQ0ThezKr5XqnWeP+rr/eDtNvHbJr+q/iPg2Xse/lCmc8Wh5VhV/y+1Au1/uPGaQ3zEkGC8/NxNe6+muibJWJvmnGgsyylUIGzGmcvv+oTfRlsnHLMhRbWdWy7acCstK5Mgr9yMzhjR7ebVsYcwXRmqvUsPaKJPl+S3IVhRg6vcPD32fS5qEFYWSVSq62h3C6HArqbvqWk/Qf4wcdua/8Oazo3l8/c+/j3j4Mn1wUPqGqqqYO+lrIFSk5tKmjW1Vr74id+TdE5MsuynqyJzje9uLLFocAwnFb64qqA6IzaS96814lyah2l4M6MwJxZvEoXbd1uVTvMDv4vkxhiobkz7rbL531AujsIj0kp+V014v9c70e/z+PeMgyfXBw+pa6u5b64iQpEYF0TFvLIqU0h220juDMefxak8s7uiwyRNXG+QpGhBgYDmWT8PwWQacE0JWkJvliBuf0dIy73U+bwQL2VeUJ6bb3B81E8KE0X0VigNsZU/qbXvHfrbp8dtk//Dms69UT5/7+OH+CghYrqbVZinp25IM3sTb8sp6mU+bW3d0JpOE5/SeaA16eZPS+szPSFnwuXAJDM+a/ZMiy+5eGzV/t1N9miriVqH2ZPL7+IMlcokfVIYogh2gRY0ypLdTCygUCpcMbyGvUz5LeO2xLbJdA6+23TuT4lIIyJfpZjOlRLpSkR+vKA8P3njNYe4ZbjVCjk5RhcNqQ2kLpCWFem4IZ20Rl7rqn1CZjuBJWVSV5G6yuRMbrin5CaUU9mgRcl5L28yyQtOymwzwa2sMhaYE5jvGLNF6S69a6o8lTy5C8b3SYV4N06G1/umV7LiJ58vVyjXyQZdfrjBIv2Q8due/CLyM8AfAe6JyBvAf4o5LX5Y07n/AEOOOuB/KR+H+AjhlgtDXhY1qXGMKz8vh0iC5mlvZYYTCI7xtNmzKIeE20XGs5b+tY76MlE/2xn/f0puJ8jOmlM7vd3stOJuDLv0hhaPG6bkfncj7HZxhi5TZ36+MmZb8c35u2r3aVlGSx8xcf4lZvzE0pBs0iW3jN82+VX1T7/Plz6U6Zyq/iLwwx/q3R3i/WMaNInJB9YXA6L1Hlcv1GKcGUPLmIiLQNhMJY4nnTb0J1YWbe4HoKV5tCEta2Ln8btkpcE0xQ1u1tOcNr32dAn2Q7Dy/qayZF6eFyF3YdYOmqjK011ngkgnUVtgnldoUXgTNXw/V9Odiu+eJH/AOEx4P6NhfldhT0zLSljHeQIrYxkqFbug3AZ8nwlXvU2AUzYtHhGGpeASbO8GJHVIVprHGzvtK3+Dg1PoEWFPafB92qs2aGF1Foryfpi1L5ckK26X5g2yaQYxDbUmfaBpjRFlRp9wJm8YF57YlR5n/BjLnkP8LgwRkyBsi3l0QVXcZtyrsE0CtGMsLE8jps0sywzN21fkcEyqLA3GhW10db95tT/tS+Qu2KZVzkxTL9dbHW5S5vu7gkBRY9CZrnBzxgDslSGq/YqjqpIqhxOTJZkWcFBsnXK01/j+hjPk7VRLgEPyfyZDQoUsOquDC39eFEvYqJaMOZPbCmmqoreDdWEZhDJcSpn2nTWiS3yfilBVRrvKTtNYZAmn/dspYZ1Y8mbK6a1ozri8T8qJsjw10xSVOEnsNYAEO+mhlDlluBXKKuNgF2susoZTr2HSiIFc2fNvG4fk/wyGO16hwZsX1iQvngtZrSyvuCEznrSk1uF3BR8fU0F71BpQZ+VR+8YluakYX15AEHb3W9pHW0tesNM8KvGoJnshrKNthnlB3ztcvakMMX0tg6Rkw65CgIP9CqJBmMzit/56D4cOK7NFdWNZofT+XRKGest6f3qLh/gshQhMSVSmuqgSjxvSoppV1qaTtz/xxIWJVU26mROur0XGcDqt3ZhJrSdskiVx3O/V5trWD8M6zjOAefvrhgGdfSNsulyw/MnTd9rltedOu8FF7WGcRK32NXxcVKW5/m4atO8zLlrje2h4X5AILz0gv3wX2Q5Wv3uhv9sa76Xs0E7yfq5o3fvBqANuO84nr1GfvVEcVHH9SPMokdvKeofJgQWsmb42g2kNzjQ//URrBs3FXlTV9D6LV9Z8MZS8dkO09xwzUi7EmaeT93KGqQ2kRWBceTOii7ofbN2cWRS8/9DwviCRXrvP5rUF1VUyng1QXUf81W5eLpGYYRhxY033JFKdG3NTnZnO7VcGrT/QaBz7/sGS+umuePQ270J6pOD/8ymbuXEBqPUasL+TlDuSBmceXjGZsztlAJYz05aMG9Je9sQJcRnY3Q3s7jiqa2NytlcFoq1cMa1TmmejqcLl29X9h+T/DIWEwO4l09qPnSc1jrBJVOc7+7oqisAwIpsd8bVTqssBvx5scWXVAGGGQmeZwqQMD1Y2E9gNaFfbZFhvkt9KrqpCTMXFBduhhXfh+9N7gX2ZQ4EqtQ7W/LpJqdkuxmk4lxbGARo7Yfl2ons0zGXXuLJ0ra7jzPvP9e0r90Pyf4bCnRwzLo0/48tgaTwKIC3hskfGASabntqSyD/fGPfH257uFJPO5nySC7RvXhXfrWJCMRTxqmx6PtqEstYo1nh6X1TXrKaXm16+eX8BgOH/6cjsTyc2qBsKZaHcYbTxaOWIS8fJbwxUF7vZ0zd7ob4crZSb/w1lSnwoe16AyJOEt9XTbswkb1r6etoSvDN2pirp3rFNXcciQT6Uer+uyHWAYjgRu0B1NeLXY1FyDvjtiPTjzOGRaDpAM0rjvVmP5jLUKmoMs15PEcmy50rhFDWkxlNd9rPqs+ttYV6dkNvAeFQRl47mWSQUl8hcGKT1drQmvMC4E7Eu154g8v4U4e8TB7TnMxR5s6F7p5+VyiYEZhJ/TYuiVuwcl19fkishH3WW+M8vkc0OoolXpS6weVCj3pIVIM+lzn430Da3AnlV2/cPztQhpkRPRWJwmjTPM4W0lzbxYqS7zjHcacm1x29G0qImtYHhTsv61ZawS3QPS5lT+T27dNojLks0s+9XsUW6bRxO/s9SpET9+lOGs1csaYu1jxvyrJkpYyIftVy/6kAD9UWFbIJp9Tc141ln+LwIi3cGO329ubBP9bvRoP2ey9MYJdlN7M6Ctbs8yRL2MG1lwR6Gdd5O50XFeGylCy2o8+zuVoRdnuVHVm9Y35IaP5tW2x0E4rKyn1OkDAFSZ/r9YXO7LS44JP9nLvTymsXr1/T3OuPA5/2AaEqO8cRq680Dx8XXViweLmkuMjkI3ZOR+ul2ngynrrLVxV0krWyyOzEGcrCLQitvyTfm+S4hSQ1uratic1oejwkNHu1qtq8eMa4cw5HDRRgXprZWXynd04zrM2GbrJ+oi4JDVMLluPf6hRmytRPfVh4nMasJAr1NHJL/MxRS10hTI5cb6sqze9CaDn/lLGFH081JnSEp25eUV/7w2/TJc9XX8L+f0T2B8bS1ie8mEhceP2TbtApC/WhjfKC2MjaoKlD2gCubKxg/KMGiKWYX2abNMZkgw7Jl88Ujtvc8118UUq2ErdA8U85+LRLWaXZ0Sa2fiXF+Z/OIyTBv2veVlI0N6mRufsMukeqi5nZoeF+M0KMlVAH3/Bp/bDx+9Y7xKFBfRHxtnJfhBIb7ERFlVQ/80NlD/o9XTnlCQ7VW2vNM+1TwQyac9+RFRXi6gd5qbjbZVBGqQDrp7GReBBAbLqmD5jzO6It6R3+vZf1SYPtA6M9s2BbW0D4Vjl5PNOdxVnNwKc9mFNV1nO8obkgzD2jaDtPgyN4skMAm0blyuFHxm/Gg2Pa5DxEToi04uy5bwtWA6z1xVVlzWxVxWYX+fgQV3vilLxBPEtuvVKZ9s4DmHJpnkVRbAqVlZazJtUOXrZU7m8HuALXJGaa2OLOIFFsgZTzycOSBmhyE9QPP1dcz6XSE0REuPGEL7VPFRdu71dIcT3KE1ToaQlX+jYBNl0MwKcUCx86GGVlnolvobWIsB63Oz3mIQ5YLUwPoB3TRWAKc74xV6eoZEXFRkexwW+Hsm7B5OfDszZdp+iJZslOTBCks0Isf6Fi9Nc7JlmuP3u1mCUG3S4xHge1dT6qhe5btommsnt/dFVIN21cyucn45wHfC91D4fTXRsI2kVpvCmtbe23YjHsJk2kjzAnaBmR6D06K6oSbRbfIar10VMajiupyOJz8n/dwdYExm3p2T3SX5kUVnkWjBXTG2Q+bRPdmRa5sIby+hO6Jyfx1TxP11cjuTs2wsjLCdDCVuKzm0iLXtijePIvEo4rYOZrLzO7EmZEc1lDv7phJ3O4rPQyO6mmgfSKcfjvSvbNDRYhH1Zz4Yb13fbF/GGRfEB6FFBzibX9Xp6X2IRdUK9ldrnaIZuMbxdtRG+CQ/J+dqCq0NzKbTP9tKc2rg9WbzwhdA1lxbcXyLSONdU8S3WOluhzx2xF3fm2Q5/KMeN8S/ejNRFiPRS4EUufNCC7rrOXZPRqKFk/g4qsexOMGK2n6O0L1Ts3ibWHxKLP6zQ1gfUBcWg/SPB9nDaBZtcGDJJlP7ml4ZwsxQvJSjOeMUNffaUCguir7wN7WIv2h7HkBIidkOxDvrazRgz09YbuDZ+fIcgF3ltTromxcdHbSYvLSXRKPikvj84xkqM+tBPEXa0JhfO5eXtKfenItpofpheHE3FH6O4obhcU7SrUxvczj75i5dPu4n1XY1HkztVt5JHqaTSRXjtQacjMR88B2iuMiUF0OsxQiQFx6WHg0QNiYe8w0p3iX7uct4pD8n6UQQYaR8aiy5HcOsk1SNRaO/p0T1l808ltqYHdak1qo1orvA7Ht8OW6UQft0/L9ekeIBnlef6kl1UJzkRmXjouvBsYV9Hdz4fQL6pXtA2FcC90TpXs8Ul30phnkTW8/HtkifPfOrqAyGZEKGvMAMJ2g4ugi4MfC86mcSZgUSZNqbajSvLiSzBUmB/ddStQfJg7J/1mJSTdHDOO28gdm/62uQ++ccv4vnLE7FZpLNffyRogLcCMm691AfyrEJdz5ZiR1jnHh2H414PsGMMM3v1PGlWc4gu1riXB3S97UdN+uWb6pjCtbSfS90lwUhmjMxNPWFmBSxu8S9dMt5Gzcni7s+fneFflBoCynpMohN/YAqk00mZWS6LBfdJ9CK/895QA/SByS/7MQIshiAdk48dVbF+TjDqoA/QBVIN854sm/eMpwalKDxmqz0z0uLKHdqLjB4MLqypa/ZYD6KlNtsjm195n+NNCfCf3dTK4VrTLjecvyO4Hlm4oflfo601yY1o8MEZwjHjU2bS41vL8yykI860iNJy4crldcsovFFxWH1LpZ/FaizuzTWDnywnoPPxTz6pKxrk9Fx+dQ9ny+Qxz5tQf4d56iOSO7ASdCPu5wsUG7ms0XV1x/2U7jZrSkT7Wd8KnV0nSCH5U0CGe/fEnuzI4UAbc2qsD51wP9HSUHcFFIy4zsHN3bnuPvZJZv9oTnW2TXl/dmg7DceVNkS8ru5eW73NUnCkJ9HvcTXLG1xNzYiuW8hK8wrqwniEuDbavLPH8fF+2iTguzUfVDPECdn/fYvbxgsekt6USQ9Ra9s2T71TPq857hyBzKqytwgxJbs+yUZCVParAkH60+T8c12Tviws9eXdu7nu3LaoS5EVKnEJTFdwJ3vxlZ/Mbl/PPxnrxqTN4QbDFliNYonxh02T6NM3RaXY5Fk78s0IipyNldKM7ui+qFuPCkWkyH/zoSO09/6qm2GTcYLBs2qSi5cRhyfd4jLhzDF45pvvPUCF+j4p+tia90VFeOsFXqC6vBXbRkr9bKcCSEjTEnd2fWiIatm5fCNRi2vnvFsXnZ+DTNU2H5dmZYCfWVZ/l2T/vtxxAT+fTIaMsYFdplGM9aENg9MAW4apPxO5vojiuPG5U6afH0irZOqaYgkUJtEonR1hfHo0BshXElNBe2qRWXDj/a6R42JsZ1Uyf0UPN/nkPNaWU4DjQTg7LYAHUPd7P0d9gpZKg2SrUp2lIZ4tLw8rBW6qtsciZbY1SuT2vWrziGE4gLpXkmvPzzW/PrLSEpo8uOtKzp77ZWzhSaQW6s1FIPY+doLhPVVSK1xsWpLxNhHcmtn2nRIgplA2zyCxjPioOkWqPuRkOAcuMI6xtiVWA2SbMFkuA/rmWW9/Hk+s9E5E0R+afl44/f+NrBk+t3OlSNtLbLECO63dqAazcQzreE8x3VdaI5z1RbKwsm98LueaY5V5rnmebS7HzCxhJ/d68itiaxliulvhDOvpXwG+PMTGK1ua3oX16xe6mjP/WsX6p4+vsarr4U2J6ZdGBsHS6BRBhOAuPSU19Emqe72VBigitTYxtYk1LzeFSbqfU24fpJBsXudm7IuNHKH5fy7PWrIvSnJm3ycbI6/wbw32BWQjfjv1LV/+LmA+/x5PoC8HMi8oNFqXny5PrHwN/DPLkOSs0fMOqnW/oHC1h06OUVVDUSFB4/x7UNzTBSXS4YTmrGo1DkPmBYOuqrjB9y0bbMpMZx/vWG/q7QPFXqC8Vvxe4cAtdfWpCD1dxhm+jPLE12J45xJYxHUK0xiHK0/kKlQKSD4Huor01T0yWzRg19sg0xZ+S2SRodDB3yfZ6b3rooNtQX46waPU2FfUxoEIajiuZ83Ov93CI+iErzPxKRr3zA7zd7cgG/ISKTJ9d3KJ5cACIyeXIdkv8Dhju/Jn5lxfDqGbUq7HqDOi+vyLsdrrqLe35N0zeEbVvUHaw88LtMfTFw+bWOx78/EI90boTDRlg8TNSXCT9khuPA+dcDLkJzLtRXhrD0R57dfUty30P3OOMilvgOxqX1Dxrt1K6uy1wiFUOKIj6r2CYYWXFqpZMbkqE7qwo3ZhaP0qziPIUWxYncBWIXaJ71aHGZv+0O70ep+f+ciPwk8IvAf6yqzzl4cn1soddrUu24+FpL/sEvsHyUqK4iza4nP3mGPnuOnJ0Wv6qM32HQZmdU5Oe/Z8GTfylBSPgrj7bgduC3UG1s2lo/76kVzr4F1ToiYy7cHEd/NqExUF0rw0oYTqXIj8C4sj9dLzRX0eTN+/SugdQc02TW7T2Bc+Nn4zsb3jEb1WkwSfO4qokLT301opUjFQO728Ztk/+vAn+hvMW/APyXwL8LB0+ujyt0a9TlceUYV7B9EFi84zl2r9L80pa83eHuOcazjv6sInZSIEsjp53/XghXHg1KbhTXC9WVCU4NRx7JsP7Sgvo80jztcTEznNT0Z8ES/dhO9FzD5dcgfPmao0VPHz3Xv3VM+9DTPVGW7yTaxzbcckVxQRtTeEhHzdycT6e/KPP6Yg5SDDFSwfKrGduPR7azUF+M86RbFNwnvcmlqg+nz0Xkvwf+5/LXgyfXxxTa91TXifa5gDjCRumeJcL1wPgjXyM1ttf6/BuB7QMlniTqJ57ukSt4fxkONYoGpXpsyepGK11SA8Oxo33qkFwzLsp55WD9mpKaTPPMkRolP+jpmpGnT1f4txuWT4TFI2X5zogbjIoQrodZtHaSKB9OalLraM5Hg1ll7/EbOz8vquTKFm/8Lpsm55iAQLguRnXBzRfKR/HkulXyT2Z05a//JjAhQT8L/LSI/GWs4Z08uZKIXInIjwP/N+bJ9V/f/m2/eKEpUV1HhmPTr9zdEcDz+EePkB++ZBxMUTnHyEv3L2h84slLS66WRzTPHX4nRkF+5PE9rN5KbO86Ln4A4tJ6AES5/nrGbRxhY/3A9pWEdgmiY/u1EXae9tst+bLlOEHYKGGXqa+MFrG9I9z9ZzuTN5zkDDGZwe29QH/m2DwwMlt9VZwVG2F71/aO2+fG3a82JpLrtyZxEopLfCqDOcQujo+V1fk+nlx/RET+AFa6fAf494GDJ9fHGar4656wqzn99ZH+1FvT2gvjt46IL40c3V3jRXl2uWS4aCALIQrjUqmuhOa5sno7IQl2dzybV4ThXoQMi9eDUZfF4Xe27dWfQf3M46InLhW/DbRPob5Qqm0mtoIfIGxzoVNA97TIkRQXyPGkZTz29Eee9at2h/E7obo2lOf6Vc/uDoxHyvJNc4hZvDOYRamqiWjtIkRbYh9XwZbmS5nkPkJRLLetlz6pOJY7+mPyPe2/XrgIX/0y22/cp/3NcwjeRKZ20USpgme8uyR1nnA1mkhU43j2QxWbVzJh7WjOAYXmXLn6kjAeZ1wUqkuxBvl84vjboKk/tcfjUnCDEnaW6H5UwrUJ5WpwXHy1YTwSJEJ7nkmVUF9nciXsTq02T43dAapry7dxJfR3oD/LtE8cx7+ROXq9N3RoaxtauasM3rzaEU86+jtV8Rowno8N7iL+57+J9v37/t5+Tv/OP1HVP/Rdv8+P57/pEB9H6GaLL2oJ4x1DwarLLTx9DkB9uSbdP8X1I9Xbps7mh5f5rfs1qVXWryp+ZwldXYMbjVKcGghlIux3mWoTcTGwvVfRv2LPra+VVAnjwpFHZXfqGY6F1ELsDOmRBMOJY3c/ow2079hdBIXFQ0v64cQ4RwDqlJNvOVZvJ5a/eW0PTrSF2vaJ/XZkvLsgtp7qOs0GeL4I1abGEbz/xKHOQ3zCoZut6ed0NdWjK6QfoB/sPz4rutuBF17/4/e4/sYIQSEJMmTCtSM7gyp394WwhrCF4QS7GNZKc2HKb9t7NeuXPP1d6O8ndscDl+c1kiCsBTeUPd+gs3LccKI0Tx2S4eg7BZF6JaF1xq8i11cV1bmnuoDusVJfK6s3YfFwN1MpJO53FrTIkGtlVklV3CsyT3Bof6di7ITG386Y65D8n6HQrS2sX/7gMcf//Bx94200RtzpCdI2aF3hn1zypZ95xvDVB1x9uWV73+F7U3QWNXmT2Nlpv/lihDbjn9qCeexMpnB3TxleGvg9X3ubVxaXBMm8uTnh9fNTrh+ukN6hywjRUZ1bAy7JJE2ac7uQJAmSTGHZjRXqYfmWUq1toFVtMs3T3lYWnSDDXuI8N4HcBFwfzaQ6uNmIIuwS2Tv6OwFJ0D5LMI7f57f2/nFI/s9QaDQu/PWrjv74jLut1cSqSlxUuJSp3jxH33lEePqMs19ynJVFmN2PfpnHP1LTnxnOnxvFHY3kXUCDMtzNxG/0HC13/N7T52SE1o/84ttf5PrREupsFUkGrTOMDklCXGYkCu0jR/tMCVtArfFtzoueaMLsTkc71RcPjfefG4/rE/6qtxNflXTSEZcVLmbiqp6V3erng+mJOofUnsXbqcCjHy+35xC/iyI8viS2S4YjYVwdsXor0z0eyLWjerxD1luyKjoMe8OI9Zr2jTPkh+8hX1/z5bvnXPUNl+uWfl0RNkK4dqRnHeevBa7XLXdPr1n3NdePl/grj+SAZBiPE+FkID1v6N4s5ca0WtsI3dOCJp06ws5Qof7Im0bnWukeDbPqm0s67/YiwvDSEeNRmJEcRKg2EX89zBo/2oAbbJlF/cQS/eTpDYf4FEIfPqF5/jLXX4IBwf9GxpeaefPakvyVFavfvIf//94kX6/ROOIWCxhGzr4VeeZXvLVYMZza0OrkwpiYkhWeQ1wE4l3l4cMT3HkFXSY/GHAhE6rEsh5Zb2ukn9QVjA0a1hTKhRHPFk+s1Nrc94xHwurNRPewx/fJhKhWwZbUa///t3emsZJm513/Pee8S+136dvdt6e7Z3U7jmXDxDOOjQwRQgI7/mKCFGQ+JJawFD4kwiAhMSZfLETEImIgIJAcEcmJIgwKoMwHJGIbUIQIOONoFns8S8/SPb3M7eWudavq3c7hw3Oqut3pmem+vd6u85NKVfe9tTz36nlPnfec5/n/ZyJVqjsUhLJqrQmyQ13FkdrhgtPktKQhmTSMVzLSNIXJ5Kb/lzH59xluOGT5lYLxaguXeXaOJrTXIHt3Bztus/WhDlsneiyND2G3d7UkeakPwzG958/SfaNDcWSgkiDeUfYN44OG8QGhfKTgodUNRmXKxrsDTKkXnmynNImnyRsmPkfGFt9xFAcg3dYVndaGrvubOohaeS1JVp0dT2tdzS+asJrTtA3lwNJaVw3+Jtf+YTvW2n+7W2nTCoT6HzW5UCcYA42n7iY0ubDX6viY/PsN78leeIvBwx9h5xFhdMRz8ckuybiLLbWhpc6F0aMDdo4tIw4W3ihpra3jyxLxnrxuSBc6FAdaJBNdwy8ONhxd3eCh3hant5fAeJq2w+eOtFdSVxZfWLU9aqD7ToItNLE7FxvaaxNcbtk5pjKKxUAvrtNdT/9spZKHg1QvaBPRfYIwbWlybXRJxpVeB4T9gzpPMFVD0060rn9S0QxyqkGmZtlB1Mo3e9Poj8m/D2k2Nlh+foO6taTVlqJSIqO+JkO+rlOYfDPU2ifanuh3R/hJgWkabFmRe0/9cI9yQTj+xEU+uXKKFzePsr7VBcC3HZI3VKOU9ILW20gD+Ya2OdrSkw4b7c8dlri0TbFkqFt6kWvHejK6RDTx0VHcOsE0HmeEyYFM1+0nTpvSg1a/VI5kp2CyqrFoY3sbnwjJbkPdsQwfSuidu+INcLPE5N+vnDxN/3gf8QmtdUe+WVP1LWXXUA6EqqsthOluMHA+vIxJU3ye0iz1tNZmtcWFpwwnPvMWAP/jzIfZOr1AftGShCZ2W6Sku9oYg4dyUc0l0pEKzooH17LsPLrI8LihWNKNtM55T/dCQ7ZZkwxLigMtqr6WZLgEmiwhmXiSkdNOssxgi5pkp9D1/sZTH2iTbl1Z5UnCPkTTyRgeSeidrcm2yrjaM2+40YjuC+coe8fxVrDjmmxtl047ZbzaZveQZetxy+TPqm5m/tIS7UuL2Al01yrGKwmbJwzVsQmnNxfZ8+vvrgAAFDlJREFUfXOB7lnD4XMOUzdU3dAUP2y0fRJoMgOnUHHZRKh6lmIgjA4L9ceHVNs5gx+lVzW6GJqDKX41Y3fVUC7qTvLgrYaFN0Zaoj3IdNVmrBWqZlJTD1q4XH27zKQKdqlqgOG6OaPVjN75Wl1dRKJ0yTxSnz3HwisDLnxqEWfbLLzeYC9t07uwSQ9oDi5Qfj8PtfIFk5WU8bJheDxjcsghlaf9aotkPefABC1Jrhx124LoqKzSISo2W3VUF7/OBZfAaFVmG2XtH/ZYfbGmc26HJrdsPdFm9yGhXPKkW0I6gpUXG9rnJ2CEYimnXLC0L1Zk6xNoPK6dUBxQS6Vso1An+LKa+YDVSx22nmiTb+kJWXcs2Ub05JpPvMe//Aa9409y+WMJpulhj3VUm+fMEHvmIvnLOyCCWV6ifdKwsNBl97E+xRnV2k8mjsmywQx1dPcCybghXy+wwwJvLU1qKFZSxgcMdRfKgcdlnnxDWH7e0toQOu+OwaDTm55KoS++ru+B0Y4rb1W/s2kZqq4l32xItwrMqKReaDM60sKWnvaZXczUETJNwDkmxxcYPpTSfbcm2yzUirVJaDoJ1hr8HjZ5Y/Lvc3xV0v2jk4wP/ATFQCgHqoeZry7SXevTXhtjTq/pplfTIFlKa61AmpydY5bN4wZEVZebLCHfcrQuVyQXtnFLPTY+0mN0WEuRTaMnR+e80D/jaF8Yz3yBp8prybAiv6zfQEwK/KBHcWxBqzC96oXWLXWESXZrvBGGJxbZXdXXL72u6/o+tfgsQaqG4lCf0aGEwamCZLuY2ZuacY0xEuf880xzeZ2D3znF5b/4MC6Bnce1YK13PpQMPHJYG79blslywmjVzCox7QSqgbYmYjxSGUyT02SrNH0VnM0uGzrntQGmvTbBru+CNTPXdZ8YfJZgRuXMudEnlvqxw5SLqsejdqnawphv1io1LjA51KZYsHQuONprE6R2NL1MRWyFmSz6whtjrfE3Qdw2dIKpPWpM/rmmPnuO5T9uc+EvHKJ1Sdj92ITTCymtCwNe+Yd/98ee++jv/GN6L7ZId7SLymVCvgumEsolKB6faJvSbkL7jGX5lYb+y5dhcxuKAqxF2m1oZaGX1mKGE6hqdWYUwa0sUCznmCq4sXiCQlxYfWqpq2K+UZJtyUzn0ycGUzWzb5RW7em860JnWNASCrKG07ZHE+UKI81rb3AoTRieWMDZFuXHRxz6yOafel7aqtn5UE37TEJr3dO6pBWZ6Y5n4VRD8WZOk0LvXE3njTVkdxy0NRO9Zalq7Yd5uc/1Z+oa321THV1kvJJRt0UNKlxCvunI1wvE6QnQpMFG1Ig2rxQNvh32Ahqvzu5OlR3E+SuaoITyBqM+wd7IByuvvQcx+R8wmpdfo1c+hikPsDnscGGhw9O/+3Xa69oDXPUSBge7TJY14euO0L7gZ43kduJZOrUNqCdWvdLDdnJ1Qx9NrtTbt3KkVCsjrMW1cqpHVxgezSj76uaSTHQ/oH2xxG6X+q2AzuetyKxGR2qHz60mc5A1EefxSTL72RuZuUEi6jaPZdbcsxdi8j9oeE9z8i26jaNzKocL6+reAuA8mRF6gz7NYk+dGycVZmOIH+5qT0C/q9OWnhpVeGuoexm2aJDU6pp7UeLzhHq5GzaoLFU/YfuRBBd0fdKRp3euIL2kptYkNjSpqPO7T4yeUEGu3NsgfuvCyWVVwlwxMxXnKxWkKl9OcJ3fCzH5H0S8p37zbezBg0ivo35d024n52BSYM6NYTzB1zVOBEmT2WtJrNbt5yaMwMLoobbKg4cVnWmCIrr5VbcEW3jSEpLC07pUkVweq2dwluJbyUyTv2klqswQ/MJm/ruNw6PFcASJQhoVrjKV1vI4q5Ilqvmjri17JSb/A0xz8SLWH0AGfZ02pAnU4VvAGFjozy5QkaCjkyc6OpvpXNtQt1SJoVgypLsOWxjsbqVlyMEAO5l4ui+PVZSqDlo7Vmh6Hb3QtTp6121LultjgxQJhpmQldQOnwZ15kxPwGqQ4jLtCiOYUNuJU5MLp1r/e5VgiMn/gNNcuowpSmT1oCZ8qonuOlpW4IJBxFQvc7okaoIevmmmwlEev5RQdQ3SJNRtS9XVUTsZO7KNUjfFwrSm6WTQTfWkI6zv9yx24rWOv52EQjmnt1CqLEUFmc71644qQDe5doElE48pNfFNUesJG5Sk90JM/jnA7ewgkwJ7aAU/6OLa6UwmvO5qgnkBW6p9UTJ2eKvmElpjr6UELhGKgTBZTKiDNmc69NgCsEK11Namk9xS9SxV5yrDuQrS3YZkrBtbcpUVkZnUqtGTWyT4DohXhxjxYGq9TYf4qVXpVK1tb+3rMfnnBl+V1OfOY8sVzMElXJ6EMgNDk2kiNZnqYjaZfjuoVInF9DW9nFWF5ib3JEOhve7IdhptXGknYV6uHVl1y+BSfd90rBtb6r6um1LTRpVp7y7eY0YFrpNrYofR3BaOdKgnZdm3lH29JrClnxlf7JWY/POE93odUJVYc4Q0NcFkWnSzK5QgNJlqaCYTHX2bTEgmOvVJxg11S+UCs60qaP432pCe6ZQn2SnJU6urNQJmorKDUmmTChCqMZ2u9qA6PaDLq66d4MImVrpT4zJD1UsxlacYGFw2vU5Q0aq42hO5YZrNLWQ8IRsexhSLFEs5yRi1EkoF46CxmvSg2vumEbLtGjuuSbYLnao4lRmfrtiIlZn2jilqrFebJECNqcM6fhM2s2zR4NOwI5zZWR+vqRym0WuDJtepWTJyOoWC2TfKreh0Qkz+ucUXBfWpd7CbW3QOH8S3M5puRtPSEdubcDJkQmtTrYykVncUH1aHCIbQXkSTWwSf2dlaPd7PShYAXB4sRjuWZNTQtBJ8osufdcdQ51oynYwF2dUSB4MjX28wk0pPJKN7D9KoW/vUnHov3IhQ7XHUkmgV/fhveO//lYgsA/8ReBQVq/3rwaACEfkq8GWgAf629/6/h+NPcUWs9r8BX/H3u1jog4z3NJtbsD3ELgxIjhzEdHOVO2+pJk62pTZGddvgspRkpB5Z04SeLl9WXYMNwlPpsMGOa32e+NnIXg5SkkkwqMssLtXELweGOleR2qnAVhP2GEwVVoJQszrZGiNVjc9SXR3q5Hv+82/ktKlR55WfBD4N/HLw3noG+K73/gTw3fDztb5cnwP+rYhML8invlwnwu1ze448cvtwDc3GBu71t7Hn10m2J2SbBemWFpuZUtskTempelaXIDuWciGlHKggrq2Cf2+pyetyS9XPKJdbej9Idd/ACi63M9f10UFL2ZOZR3AyUS8uRDX7vWg5RNNOdVMtS/G9zpWL5KK6cw3sQYf/fHi8IyI/Qi2FvoBKlwN8E/hfwN8n+nLtW3xVUp85i6xl2JVlOHYQ17LhItiEE0EVnMueLmPakpDUOvKnYxecFmWW7N6EpUpUDNclQt0WigWZSZtIo6JXo8MJu3/e0jst5JueckHtitIdvaj21mCKGilAai21wO9tl/em5vzBmO6nUIOJw1ODCu/9eRE5FJ52y75c0ZPr3uKrkvr8u8ildfLVQ5SPH5z56jaZoWqHdXqnzejehJG7Vi/ecqBK0E1LC+emOj7prqdua2OMeHBp0Aw9Ak3Lkz+0y7HlTTbHbS53lrBDQzIydM/qSVT1LZ3zBbI+DNcU091pA/7mR/8bTn4R6QH/Gfg73vvt9xEKumVfrujJdX/gq5L6nTOYs+fpLAzg0AGqQ30mKxll31D21YHRpSpT7lJVgSZoemoVpp+5whRLQjKGfMOT7egUxxZeFRpqB7VDqi6HNnc4JCN8niKTEgC32KdebOETYfLoMslYL4LN1ggxsqfB/4aSX0RSNPF/13v/X8Lhtak9kYgcAS6E49GX60EjXBOwsYF5Teh1OpiVZZrlAc0go1hMqVu6QjQd4sQFI4vCUw4sdS7kOw3JUC94Te0wuwVc3FBp9arClRUYoRGDpAmSpdDv4wddmkGOqR2jQ212jlkGp2r6L6xrxegeuZHVHgH+PfAj7/3Xr/rVs8CXgH8S7n//quPRl+tBxXvc7i5udxdOgTGWbpYiWQZZirRaukPbaWnbYlHRtgaptIQBE9ZYQtJ6QPIMrMFmmdbp5DmSppCluF6LarnD8FhGnav9aTLSyUCzMsC+u3FHFds+A/wC8JKIPB+O/QM06f+TiHwZOA38vP5voi/XXOEa3KT5caFYY8PIrUmN83pdYK0ey7WvF+91dHfuSpdYYsPGmZ2pTCdbBX3v2fhQi9ERBz40219O+IPT/xKAv2x+/qZDv5HVnv/N9efrANc1y/Le/xrwa9c5/hzwsZsJMLIPcQ2+aP60T5axmFaOtFuQJDq6p8msG4wkrBKFXWIpKmQ4UdWJ19Y5fHKBlRcP8OZf67HzUwXty61bCjPu8EbuHq7BjUYwGv34cWMRa5HpSRBuHnRFx3ncaKRTrbPneLj7Cda+MuF7v/3MLYUTkz9y73EN3jU3LDyVfu8V/P97UneUboG9F0ZEIvcINxpx/F+/cMvvE5M/si9x106d9kBM/si+xLTbt/4etyGOSOTuc+KRW36LmPyRfcmlpxZv+T1i8kf2HZJmXPrklV3dzy78zT29T1zqjOw73Cd/ko9/9DQf/kdf59Fnd2BSfPCLrkNM/si+wrRavPILOfLWUT7yrcvQOPaq2RanPZF9RfXpj/KpP3OS3ou5NsMvdaJoVeTBR5KEM38p5+3/8xM88nxBtdyhblva01KImySO/JF9gz2ySjXwPPztStshM3NLGRyTP7Jv2PnEQ2QbhmRYzfy9ZjIqeyBOeyL7AxEmS5ajfziZKUN7I9QtudIgc5PEkT+yL7D9Pl4gWxtqY4yHuiXUbcP79JO/LzH5I/sC6XYYnC6pljuhB1h1gvDs2Yo0Jn9kX1A/coimpXKGdlhiSkfZN9Qd4sgfebCpuynZRkmyMYLE0AQDDWf3lvgQkz+yT0h2SuxuiYwmuFS1/5tMdYPiJlfkgcZuavOKz1KadsJ42dLcWv96TP7IPmF9ExoHK0vg1fHRpaoHulfitCeyL3A7Q3xRIKMJpnEkE0c68rgkaHXugZj8kfsfYzF5jg8qby5Rwdw6F6oecc4fecBptzBG8L3ObKrjrTpC7pU48kfuf7xDRJDFBVy/hdT+ih/X3lc648gf2Qd4j28cdNs0rQRpHKb2eKv6/nstbPvAkV9EjovI/xSRH4nID0XkK+H410TkrIg8H26fv+o1XxWRkyLyqoh89qrjT4nIS+F3vyF73ZqLzB/hYjddH5Fc3KFzbszCWxVLrzb48eSDX38dbmTkn3py/YmI9IHvi8i3w+/+hff+n1/95Gs8uR4CviMiHw5KzVNPrv+LGtJ9jqjUHLkBmu1t2N7+sWNpuN2xNkbv/Xnv/Z+ExzvA1JPrvZh5cnnv3wKmnlxHCJ5cwYFx6skVidwTbuqC9xpPLoBfEZEXReS3RGQpHDsKvHPVy6beW0e5QU+uSORucMPJf60nFzqFeQJ4EnVr/PXpU6/z8pvy5BKRXxKR50TkuYq9yVJEIh/EDSX/9Ty5vPdr3vvGe++A3wR+Ojz9lj25vPff8N4/7b1/OmXvJsORyPtxI6s91/XkCnP4KT8H/CA8fhb4oojkIvIYVzy5zgM7IvLp8J6/yBUfr0jkrnMrnlx/Q0SeRKcubwN/C6InV2T/IHttAbtbDGTZf0qua/0VidwQ3/G/933v/dPXHo/lDZG5JSZ/ZG6JyR+ZW2LyR+aWmPyRuSUmf2RuickfmVti8kfmlpj8kbklJn9kbonJH5lbYvJH5paY/JG5JSZ/ZG6JyR+ZW2LyR+aWmPyRuSUmf2RuickfmVti8kfmlpj8kbklJn9kbonJH5lbYvJH5paY/JG5JSZ/ZG6JyR+ZW2LyR+aWmPyRuSUmf2RuickfmVvue31+EdkBXr3XcVzFCnDpXgdxFTGeD+YR7/3Baw/uBwf2V69nLHCvEJHnYjzvzf0Wz/sRpz2RuSUmf2Ru2Q/J/417HcA1xHjen/stnvfkvr/gjUTuFPth5I9E7ggx+SNzy32b/CLyORF5VUROisgzd/Fz3xaRl0TkeRF5LhxbFpFvi8jr4X7pqud/NcT4qoh89jbF8FsickFEfnDVsZuOQUSeCn/LSRH5DRGR2xjP10TkbPg/PS8in79b8dw2vPf33Q2wwBvA40AGvAB89C599tvAyjXH/hnwTHj8DPBPw+OPhthy4LEQs70NMfwM8AngB7cSA/A94M8Bgrrd/+xtjOdrwN+7znPveDy363a/jvw/DZz03r/pvS+BbwFfuIfxfAH4Znj8TeCvXnX8W977wnv/FnASjf2W8N7/IbB+KzGIyBFg4L3/I6+Z99tXveZ2xPNe3PF4bhf3a/IfBd656ucz4djdwAN/ICLfF5FfCscOe+/PA4T7Q/cgzpuN4Wh4fCdj+xUReTFMi6bTsHsZz01xvyb/9eaCd2tN9jPe+08APwv8soj8zPs8917G+UEx3OnY/h3wBPAkcB749Xscz01zvyb/GeD4VT8fA87djQ/23p8L9xeA/4pOY9bC1zbh/sI9iPNmYzgTHt+R2Lz3a977xnvvgN/kynTvnsSzF+7X5P9j4ISIPCYiGfBF4Nk7/aEi0hWR/vQx8FeAH4TP/lJ42peA3w+PnwW+KCK5iDwGnEAv6u4ENxVDmBrtiMinw6rKL171mltmeiIGfg79P92zePbEvbza/oAVhs8Dr6GrBb96lz7zcXSl4gXgh9PPBQ4A3wVeD/fLV73mV0OMr3KbVi+A/4BOJSp0xPzyXmIAnkaT8g3g3xB29G9TPL8DvAS8iCb8kbsVz+26xfKGyNxyv057IpE7Tkz+yNwSkz8yt8Tkj8wtMfkjc0tM/sjcEpM/Mrf8f2C75mbFGdRDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
